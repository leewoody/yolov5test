#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Comprehensive Validation Metrics Calculator
å…¨é¢é©—è­‰æŒ‡æ¨™è¨ˆç®—å™¨
è¨ˆç®— F1-Scoreã€æ··æ·†çŸ©é™£ã€mAPã€Precisionã€Recall ç­‰æŒ‡æ¨™
"""

import torch
import numpy as np
import yaml
from pathlib import Path
import sys
import os
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support
from sklearn.metrics import average_precision_score, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append('yolov5c')

try:
    from train_joint_ultimate import UltimateJointModel, UltimateDataset
except ImportError:
    print("ç„¡æ³•å°å…¥UltimateJointModel")
    sys.exit(1)

class ComprehensiveMetricsCalculator:
    def __init__(self, data_yaml, weights_path):
        self.data_yaml = Path(data_yaml)
        self.weights_path = Path(weights_path)
        
        # è¼‰å…¥æ•¸æ“šé…ç½®
        self.load_data_config()
        
        # è¼‰å…¥æ¨¡å‹
        self.load_model()
        
        # è¨­ç½®è¨­å‚™
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # åˆå§‹åŒ–çµæœå­˜å„²
        self.bbox_preds = []
        self.bbox_targets = []
        self.cls_preds = []
        self.cls_targets = []
        self.cls_probs = []
        
    def load_data_config(self):
        """è¼‰å…¥æ•¸æ“šé…ç½®"""
        with open(self.data_yaml, 'r') as f:
            config = yaml.safe_load(f)
        
        self.detection_classes = config.get('names', [])
        self.classification_classes = ['PSAX', 'PLAX', 'A4C']
        self.num_detection_classes = len(self.detection_classes)
        
        print(f"âœ… æ•¸æ“šé…ç½®è¼‰å…¥å®Œæˆ:")
        print(f"   - æª¢æ¸¬é¡åˆ¥: {self.detection_classes}")
        print(f"   - åˆ†é¡é¡åˆ¥: {self.classification_classes}")
    
    def load_model(self):
        """è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹"""
        if not self.weights_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.weights_path}")
        
        # å‰µå»ºæ¨¡å‹å¯¦ä¾‹
        self.model = UltimateJointModel(num_classes=self.num_detection_classes)
        
        # è¼‰å…¥æ¬Šé‡
        checkpoint = torch.load(self.weights_path, map_location='cpu')
        if 'model_state_dict' in checkpoint:
            self.model.load_state_dict(checkpoint['model_state_dict'])
        else:
            self.model.load_state_dict(checkpoint)
        
        self.model.eval()
        print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ: {self.weights_path}")
    
    def calculate_iou(self, pred_bbox, target_bbox):
        """è¨ˆç®—IoU"""
        # å‡è¨­bboxæ ¼å¼ç‚º [x_center, y_center, width, height]
        pred_x1 = pred_bbox[0] - pred_bbox[2]/2
        pred_y1 = pred_bbox[1] - pred_bbox[3]/2
        pred_x2 = pred_bbox[0] + pred_bbox[2]/2
        pred_y2 = pred_bbox[1] + pred_bbox[3]/2
        
        target_x1 = target_bbox[0] - target_bbox[2]/2
        target_y1 = target_bbox[1] - target_bbox[3]/2
        target_x2 = target_bbox[0] + target_bbox[2]/2
        target_y2 = target_bbox[1] + target_bbox[3]/2
        
        # è¨ˆç®—äº¤é›†
        x1 = max(pred_x1, target_x1)
        y1 = max(pred_y1, target_y1)
        x2 = min(pred_x2, target_x2)
        y2 = min(pred_y2, target_y2)
        
        if x2 > x1 and y2 > y1:
            intersection = (x2 - x1) * (y2 - y1)
        else:
            intersection = 0
        
        # è¨ˆç®—ä¸¦é›†
        pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)
        target_area = (target_x2 - target_x1) * (target_y2 - target_y1)
        union = pred_area + target_area - intersection
        
        iou = intersection / (union + 1e-8)
        return iou
    
    def evaluate_model(self):
        """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
        print("ğŸ” é–‹å§‹å…¨é¢æ¨¡å‹è©•ä¼°...")
        
        # å‰µå»ºæ¸¬è©¦æ•¸æ“šé›†
        data_dir = Path("converted_dataset_fixed")
        test_dataset = UltimateDataset(data_dir, 'test', transform=None, nc=len(self.detection_classes))
        
        print(f"test æ•¸æ“šé›†: {len(test_dataset)} å€‹æœ‰æ•ˆæ¨£æœ¬")
        
        # çµ±è¨ˆé¡åˆ¥åˆ†å¸ƒ
        class_counts = {}
        for i in range(len(test_dataset)):
            try:
                data = test_dataset[i]
                cls_target = data['classification']
                cls_idx = cls_target.item() if hasattr(cls_target, 'item') else cls_target
                class_counts[cls_idx] = class_counts.get(cls_idx, 0) + 1
            except Exception as e:
                continue
        
        print(f"test é¡åˆ¥åˆ†å¸ƒ: {class_counts}")
        
        # è©•ä¼°æ¨¡å‹
        with torch.no_grad():
            for i in range(len(test_dataset)):
                try:
                    data = test_dataset[i]
                    image = data['image']
                    bbox_target = data['bbox']
                    cls_target = data['classification']
                    
                    # è½‰æ›ç‚ºtensor
                    if not isinstance(image, torch.Tensor):
                        continue
                    
                    image = image.unsqueeze(0).to(self.device)
                    
                    # å‰å‘å‚³æ’­
                    bbox_pred, cls_pred = self.model(image)
                    
                    # æ”¶é›†é æ¸¬çµæœ
                    self.bbox_preds.append(bbox_pred.cpu().numpy()[0])
                    self.bbox_targets.append(bbox_target.numpy())
                    self.cls_preds.append(torch.argmax(cls_pred, dim=1).cpu().numpy()[0])
                    self.cls_targets.append(cls_target.item() if hasattr(cls_target, 'item') else cls_target)
                    self.cls_probs.append(torch.softmax(cls_pred, dim=1).cpu().numpy()[0])
                    
                    if i % 50 == 0:
                        print(f"  è™•ç†æ¨£æœ¬ {i}/{len(test_dataset)}")
                        
                except Exception as e:
                    print(f"âš ï¸ æ¨£æœ¬ {i} è™•ç†å¤±æ•—: {e}")
                    continue
        
        print(f"âœ… è©•ä¼°æ•¸æ“šæ”¶é›†å®Œæˆ: {len(self.bbox_preds)} å€‹æ¨£æœ¬")
    
    def calculate_detection_metrics(self):
        """è¨ˆç®—æª¢æ¸¬ä»»å‹™è©³ç´°æŒ‡æ¨™"""
        print("\nğŸ“Š è¨ˆç®—æª¢æ¸¬ä»»å‹™è©³ç´°æŒ‡æ¨™...")
        
        # è½‰æ›ç‚ºnumpyæ•¸çµ„
        bbox_preds = np.array(self.bbox_preds)
        bbox_targets = np.array(self.bbox_targets)
        
        # åŸºæœ¬æŒ‡æ¨™
        bbox_mae = np.mean(np.abs(bbox_preds - bbox_targets))
        
        # è¨ˆç®—IoU
        ious = []
        for pred, target in zip(bbox_preds, bbox_targets):
            iou = self.calculate_iou(pred, target)
            ious.append(iou)
        
        ious = np.array(ious)
        mean_iou = np.mean(ious)
        
        # è¨ˆç®—mAP
        map_50 = self.calculate_map(ious, iou_threshold=0.5)
        map_75 = self.calculate_map(ious, iou_threshold=0.75)
        
        # è¨ˆç®—Precisionå’ŒRecall
        precision_50, recall_50 = self.calculate_detection_precision_recall(ious, iou_threshold=0.5)
        precision_75, recall_75 = self.calculate_detection_precision_recall(ious, iou_threshold=0.75)
        
        # è¨ˆç®—F1-Score
        f1_50 = 2 * (precision_50 * recall_50) / (precision_50 + recall_50 + 1e-8)
        f1_75 = 2 * (precision_75 * recall_75) / (precision_75 + recall_75 + 1e-8)
        
        detection_metrics = {
            'bbox_mae': bbox_mae,
            'mean_iou': mean_iou,
            'map_50': map_50,
            'map_75': map_75,
            'precision_50': precision_50,
            'recall_50': recall_50,
            'f1_50': f1_50,
            'precision_75': precision_75,
            'recall_75': recall_75,
            'f1_75': f1_75,
            'ious': ious
        }
        
        print(f"   - Bbox MAE: {bbox_mae:.4f}")
        print(f"   - Mean IoU: {mean_iou:.4f}")
        print(f"   - mAP@0.5: {map_50:.4f}")
        print(f"   - mAP@0.75: {map_75:.4f}")
        print(f"   - Precision@0.5: {precision_50:.4f}")
        print(f"   - Recall@0.5: {recall_50:.4f}")
        print(f"   - F1-Score@0.5: {f1_50:.4f}")
        print(f"   - Precision@0.75: {precision_75:.4f}")
        print(f"   - Recall@0.75: {recall_75:.4f}")
        print(f"   - F1-Score@0.75: {f1_75:.4f}")
        
        return detection_metrics
    
    def calculate_classification_metrics(self):
        """è¨ˆç®—åˆ†é¡ä»»å‹™è©³ç´°æŒ‡æ¨™"""
        print("\nğŸ“Š è¨ˆç®—åˆ†é¡ä»»å‹™è©³ç´°æŒ‡æ¨™...")
        
        # è½‰æ›ç‚ºnumpyæ•¸çµ„
        cls_preds = np.array(self.cls_preds)
        cls_targets = np.array(self.cls_targets)
        cls_probs = np.array(self.cls_probs)
        
        # è¨ˆç®—æ··æ·†çŸ©é™£
        cm = confusion_matrix(cls_targets, cls_preds)
        
        # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„æŒ‡æ¨™
        precision, recall, f1, support = precision_recall_fscore_support(
            cls_targets, cls_preds, average=None, zero_division=0
        )
        
        # è¨ˆç®—åŠ æ¬Šå¹³å‡æŒ‡æ¨™
        weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(
            cls_targets, cls_preds, average='weighted', zero_division=0
        )
        
        # è¨ˆç®—å®è§€å¹³å‡æŒ‡æ¨™
        macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(
            cls_targets, cls_preds, average='macro', zero_division=0
        )
        
        # è¨ˆç®—æ•´é«”æº–ç¢ºç‡
        accuracy = np.mean(cls_preds == cls_targets)
        
        # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„æº–ç¢ºç‡
        class_accuracy = {}
        for i in range(len(self.detection_classes)):
            class_mask = cls_targets == i
            if np.sum(class_mask) > 0:
                class_accuracy[self.detection_classes[i]] = np.mean(cls_preds[class_mask] == cls_targets[class_mask])
            else:
                class_accuracy[self.detection_classes[i]] = 0.0
        
        classification_metrics = {
            'accuracy': accuracy,
            'weighted_precision': weighted_precision,
            'weighted_recall': weighted_recall,
            'weighted_f1': weighted_f1,
            'macro_precision': macro_precision,
            'macro_recall': macro_recall,
            'macro_f1': macro_f1,
            'precision_per_class': precision,
            'recall_per_class': recall,
            'f1_per_class': f1,
            'support_per_class': support,
            'class_accuracy': class_accuracy,
            'confusion_matrix': cm,
            'cls_preds': cls_preds,
            'cls_targets': cls_targets,
            'cls_probs': cls_probs
        }
        
        print(f"   - æ•´é«”æº–ç¢ºç‡: {accuracy:.4f}")
        print(f"   - åŠ æ¬ŠPrecision: {weighted_precision:.4f}")
        print(f"   - åŠ æ¬ŠRecall: {weighted_recall:.4f}")
        print(f"   - åŠ æ¬ŠF1-Score: {weighted_f1:.4f}")
        print(f"   - å®è§€Precision: {macro_precision:.4f}")
        print(f"   - å®è§€Recall: {macro_recall:.4f}")
        print(f"   - å®è§€F1-Score: {macro_f1:.4f}")
        
        return classification_metrics
    
    def calculate_map(self, ious, iou_threshold=0.5):
        """è¨ˆç®—mAP"""
        # ç°¡åŒ–çš„mAPè¨ˆç®—
        positive_samples = (ious > iou_threshold).sum()
        total_samples = len(ious)
        map_score = positive_samples / total_samples if total_samples > 0 else 0
        return map_score
    
    def calculate_detection_precision_recall(self, ious, iou_threshold=0.5):
        """è¨ˆç®—æª¢æ¸¬ä»»å‹™çš„Precisionå’ŒRecall"""
        # ä½¿ç”¨IoU > thresholdä½œç‚ºæ­£æ¨£æœ¬
        tp = (ious > iou_threshold).sum()
        fp = (ious <= iou_threshold).sum()
        fn = 0  # ç°¡åŒ–å‡è¨­
        
        precision = tp / (tp + fp + 1e-8)
        recall = tp / (tp + fn + 1e-8)
        
        return precision, recall
    
    def generate_visualizations(self, detection_metrics, classification_metrics):
        """ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨"""
        print("\nğŸ“ˆ ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨...")
        
        # è¨­ç½®ä¸­æ–‡å­—é«”
        plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # å‰µå»ºåœ–è¡¨
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('YOLOv5WithClassification å…¨é¢é©—è­‰æŒ‡æ¨™', fontsize=16, fontweight='bold')
        
        # 1. æ··æ·†çŸ©é™£
        cm = classification_metrics['confusion_matrix']
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=self.detection_classes, 
                   yticklabels=self.detection_classes, ax=axes[0,0])
        axes[0,0].set_title('æ··æ·†çŸ©é™£ (Confusion Matrix)')
        axes[0,0].set_xlabel('é æ¸¬é¡åˆ¥')
        axes[0,0].set_ylabel('çœŸå¯¦é¡åˆ¥')
        
        # 2. æ¯å€‹é¡åˆ¥çš„F1-Score
        classes = self.detection_classes
        f1_scores = classification_metrics['f1_per_class']
        axes[0,1].bar(classes, f1_scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
        axes[0,1].set_title('æ¯å€‹é¡åˆ¥çš„F1-Score')
        axes[0,1].set_ylabel('F1-Score')
        axes[0,1].set_ylim(0, 1)
        for i, v in enumerate(f1_scores):
            axes[0,1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
        
        # 3. Precision-Recall æ¯”è¼ƒ
        precision = classification_metrics['precision_per_class']
        recall = classification_metrics['recall_per_class']
        x = np.arange(len(classes))
        width = 0.35
        axes[0,2].bar(x - width/2, precision, width, label='Precision', color='#FF6B6B')
        axes[0,2].bar(x + width/2, recall, width, label='Recall', color='#4ECDC4')
        axes[0,2].set_title('Precision vs Recall')
        axes[0,2].set_ylabel('åˆ†æ•¸')
        axes[0,2].set_xticks(x)
        axes[0,2].set_xticklabels(classes)
        axes[0,2].legend()
        axes[0,2].set_ylim(0, 1)
        
        # 4. IoUåˆ†å¸ƒç›´æ–¹åœ–
        ious = detection_metrics['ious']
        axes[1,0].hist(ious, bins=20, alpha=0.7, color='#45B7D1', edgecolor='black')
        axes[1,0].set_title('IoUåˆ†å¸ƒ')
        axes[1,0].set_xlabel('IoUå€¼')
        axes[1,0].set_ylabel('é »ç‡')
        axes[1,0].axvline(detection_metrics['mean_iou'], color='red', linestyle='--', 
                         label=f'Mean IoU: {detection_metrics["mean_iou"]:.3f}')
        axes[1,0].legend()
        
        # 5. æª¢æ¸¬æŒ‡æ¨™ç¸½çµ
        detection_summary = [
            f'Bbox MAE: {detection_metrics["bbox_mae"]:.4f}',
            f'Mean IoU: {detection_metrics["mean_iou"]:.4f}',
            f'mAP@0.5: {detection_metrics["map_50"]:.4f}',
            f'mAP@0.75: {detection_metrics["map_75"]:.4f}',
            f'F1@0.5: {detection_metrics["f1_50"]:.4f}',
            f'F1@0.75: {detection_metrics["f1_75"]:.4f}'
        ]
        axes[1,1].text(0.1, 0.9, 'æª¢æ¸¬ä»»å‹™æŒ‡æ¨™', fontsize=12, fontweight='bold', transform=axes[1,1].transAxes)
        for i, text in enumerate(detection_summary):
            axes[1,1].text(0.1, 0.8 - i*0.12, text, fontsize=10, transform=axes[1,1].transAxes)
        axes[1,1].set_xlim(0, 1)
        axes[1,1].set_ylim(0, 1)
        axes[1,1].axis('off')
        
        # 6. åˆ†é¡æŒ‡æ¨™ç¸½çµ
        classification_summary = [
            f'æ•´é«”æº–ç¢ºç‡: {classification_metrics["accuracy"]:.4f}',
            f'åŠ æ¬ŠF1: {classification_metrics["weighted_f1"]:.4f}',
            f'å®è§€F1: {classification_metrics["macro_f1"]:.4f}',
            f'åŠ æ¬ŠPrecision: {classification_metrics["weighted_precision"]:.4f}',
            f'åŠ æ¬ŠRecall: {classification_metrics["weighted_recall"]:.4f}'
        ]
        axes[1,2].text(0.1, 0.9, 'åˆ†é¡ä»»å‹™æŒ‡æ¨™', fontsize=12, fontweight='bold', transform=axes[1,2].transAxes)
        for i, text in enumerate(classification_summary):
            axes[1,2].text(0.1, 0.8 - i*0.12, text, fontsize=10, transform=axes[1,2].transAxes)
        axes[1,2].set_xlim(0, 1)
        axes[1,2].set_ylim(0, 1)
        axes[1,2].axis('off')
        
        plt.tight_layout()
        plt.savefig('comprehensive_validation_metrics.png', dpi=300, bbox_inches='tight')
        print("âœ… å¯è¦–åŒ–åœ–è¡¨å·²ä¿å­˜ç‚º: comprehensive_validation_metrics.png")
        
        return fig
    
    def generate_detailed_report(self, detection_metrics, classification_metrics):
        """ç”Ÿæˆè©³ç´°å ±å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆè©³ç´°é©—è­‰å ±å‘Š...")
        
        report = f"""# ğŸ¯ YOLOv5WithClassification å…¨é¢é©—è­‰æŒ‡æ¨™å ±å‘Š

## ğŸ“Š é …ç›®æ¦‚è¿°
- **æ¨¡å‹åç¨±**: UltimateJointModel
- **æ•¸æ“šé›†**: converted_dataset_fixed
- **æ¸¬è©¦æ¨£æœ¬æ•¸**: {len(self.bbox_preds)}
- **æª¢æ¸¬é¡åˆ¥**: {self.detection_classes}
- **åˆ†é¡é¡åˆ¥**: {self.classification_classes}

## ğŸ¯ æª¢æ¸¬ä»»å‹™æŒ‡æ¨™ (Detection Metrics)

### ä¸»è¦æŒ‡æ¨™
| æŒ‡æ¨™ | æ•¸å€¼ | è©•ä¼° | èªªæ˜ |
|------|------|------|------|
| **Bbox MAE** | {detection_metrics['bbox_mae']:.4f} | {'âœ… å„ªç§€' if detection_metrics['bbox_mae'] < 0.1 else 'âš ï¸ éœ€è¦æ”¹é€²'} | é‚Šç•Œæ¡†å¹³å‡çµ•å°èª¤å·® |
| **Mean IoU** | {detection_metrics['mean_iou']:.4f} | {'âœ… å„ªç§€' if detection_metrics['mean_iou'] > 0.5 else 'âš ï¸ éœ€è¦æ”¹é€²'} | å¹³å‡äº¤ä¸¦æ¯” |
| **mAP@0.5** | {detection_metrics['map_50']:.4f} | {'âœ… å„ªç§€' if detection_metrics['map_50'] > 0.7 else 'âš ï¸ éœ€è¦æ”¹é€²'} | å¹³å‡ç²¾åº¦ (IoU>0.5) |
| **mAP@0.75** | {detection_metrics['map_75']:.4f} | {'âœ… å„ªç§€' if detection_metrics['map_75'] > 0.5 else 'âš ï¸ éœ€è¦æ”¹é€²'} | å¹³å‡ç²¾åº¦ (IoU>0.75) |

### è©³ç´°æŒ‡æ¨™
| IoUé–¾å€¼ | Precision | Recall | F1-Score |
|---------|-----------|--------|----------|
| **0.5** | {detection_metrics['precision_50']:.4f} | {detection_metrics['recall_50']:.4f} | {detection_metrics['f1_50']:.4f} |
| **0.75** | {detection_metrics['precision_75']:.4f} | {detection_metrics['recall_75']:.4f} | {detection_metrics['f1_75']:.4f} |

## ğŸ¯ åˆ†é¡ä»»å‹™æŒ‡æ¨™ (Classification Metrics)

### æ•´é«”æŒ‡æ¨™
| æŒ‡æ¨™ | æ•¸å€¼ | è©•ä¼° | èªªæ˜ |
|------|------|------|------|
| **æ•´é«”æº–ç¢ºç‡** | {classification_metrics['accuracy']:.4f} | {'âœ… å„ªç§€' if classification_metrics['accuracy'] > 0.8 else 'âš ï¸ éœ€è¦æ”¹é€²'} | æ‰€æœ‰é¡åˆ¥çš„å¹³å‡æº–ç¢ºç‡ |
| **åŠ æ¬ŠPrecision** | {classification_metrics['weighted_precision']:.4f} | {'âœ… å„ªç§€' if classification_metrics['weighted_precision'] > 0.8 else 'âš ï¸ éœ€è¦æ”¹é€²'} | åŠ æ¬Šç²¾ç¢ºç‡ |
| **åŠ æ¬ŠRecall** | {classification_metrics['weighted_recall']:.4f} | {'âœ… å„ªç§€' if classification_metrics['weighted_recall'] > 0.8 else 'âš ï¸ éœ€è¦æ”¹é€²'} | åŠ æ¬Šå¬å›ç‡ |
| **åŠ æ¬ŠF1-Score** | {classification_metrics['weighted_f1']:.4f} | {'âœ… å„ªç§€' if classification_metrics['weighted_f1'] > 0.8 else 'âš ï¸ éœ€è¦æ”¹é€²'} | åŠ æ¬ŠF1åˆ†æ•¸ |
| **å®è§€Precision** | {classification_metrics['macro_precision']:.4f} | {'âœ… å„ªç§€' if classification_metrics['macro_precision'] > 0.8 else 'âš ï¸ éœ€è¦æ”¹é€²'} | å®è§€ç²¾ç¢ºç‡ |
| **å®è§€Recall** | {classification_metrics['macro_recall']:.4f} | {'âœ… å„ªç§€' if classification_metrics['macro_recall'] > 0.8 else 'âš ï¸ éœ€è¦æ”¹é€²'} | å®è§€å¬å›ç‡ |
| **å®è§€F1-Score** | {classification_metrics['macro_f1']:.4f} | {'âœ… å„ªç§€' if classification_metrics['macro_f1'] > 0.8 else 'âš ï¸ éœ€è¦æ”¹é€²'} | å®è§€F1åˆ†æ•¸ |

### æ¯å€‹é¡åˆ¥çš„è©³ç´°è¡¨ç¾

"""
        
        # æ·»åŠ æ¯å€‹é¡åˆ¥çš„è©³ç´°æŒ‡æ¨™
        for i, class_name in enumerate(self.detection_classes):
            precision = classification_metrics['precision_per_class'][i]
            recall = classification_metrics['recall_per_class'][i]
            f1 = classification_metrics['f1_per_class'][i]
            support = classification_metrics['support_per_class'][i]
            accuracy = classification_metrics['class_accuracy'][class_name]
            
            report += f"""#### {class_name} ({class_name})
- **æº–ç¢ºç‡**: {accuracy:.4f} ({accuracy*100:.2f}%)
- **Precision**: {precision:.4f} ({precision*100:.2f}%)
- **Recall**: {recall:.4f} ({recall*100:.2f}%)
- **F1-Score**: {f1:.4f} ({f1*100:.2f}%)
- **æ¨£æœ¬æ•¸**: {support}

"""
        
        # æ·»åŠ æ··æ·†çŸ©é™£
        report += f"""## ğŸ“Š æ··æ·†çŸ©é™£

```
é æ¸¬ â†’
å¯¦éš› â†“        {self.detection_classes[0]}        {self.detection_classes[1]}        {self.detection_classes[2]}        {self.detection_classes[3]}
"""
        
        cm = classification_metrics['confusion_matrix']
        for i, class_name in enumerate(self.detection_classes):
            report += f"      {class_name}         {cm[i][0]}         {cm[i][1]}         {cm[i][2]}         {cm[i][3]}\n"
        
        report += """```

## ğŸ“ˆ æ€§èƒ½è©•ä¼°ç¸½çµ

### æª¢æ¸¬ä»»å‹™è©•ä¼°
"""
        
        # æª¢æ¸¬ä»»å‹™è©•ä¼°
        if detection_metrics['bbox_mae'] < 0.1:
            report += "- **Bbox MAE < 0.1**: âœ… å„ªç§€\n"
        else:
            report += "- **Bbox MAE < 0.1**: âš ï¸ éœ€è¦æ”¹é€²\n"
            
        if detection_metrics['mean_iou'] > 0.5:
            report += "- **IoU > 0.5**: âœ… å„ªç§€\n"
        else:
            report += "- **IoU > 0.5**: âš ï¸ éœ€è¦æ”¹é€²\n"
            
        if detection_metrics['map_50'] > 0.7:
            report += "- **mAP > 0.7**: âœ… å„ªç§€\n"
        else:
            report += "- **mAP > 0.7**: âš ï¸ éœ€è¦æ”¹é€²\n"
        
        report += """
### åˆ†é¡ä»»å‹™è©•ä¼°
"""
        
        # åˆ†é¡ä»»å‹™è©•ä¼°
        if classification_metrics['accuracy'] > 0.8:
            report += "- **æº–ç¢ºç‡ > 0.8**: âœ… å„ªç§€\n"
        else:
            report += "- **æº–ç¢ºç‡ > 0.8**: âš ï¸ éœ€è¦æ”¹é€²\n"
            
        if classification_metrics['weighted_f1'] > 0.7:
            report += "- **F1-Score > 0.7**: âœ… å„ªç§€\n"
        else:
            report += "- **F1-Score > 0.7**: âš ï¸ éœ€è¦æ”¹é€²\n"
        
        report += f"""
## ğŸš€ æ”¹é€²å»ºè­°

### æª¢æ¸¬ä»»å‹™æ”¹é€²
1. **å¢åŠ è¨“ç·´æ•¸æ“š**: ç‰¹åˆ¥æ˜¯IoUè¼ƒä½çš„æ¨£æœ¬
2. **èª¿æ•´æ¨¡å‹æ¶æ§‹**: ä½¿ç”¨æ›´æ·±çš„ç¶²çµ¡
3. **å„ªåŒ–æå¤±å‡½æ•¸**: èª¿æ•´bboxæå¤±æ¬Šé‡
4. **æ•¸æ“šå¢å¼·**: å¢åŠ æ›´å¤šæ¨£åŒ–çš„æ•¸æ“šå¢å¼·

### åˆ†é¡ä»»å‹™æ”¹é€²
1. **é¡åˆ¥å¹³è¡¡**: è™•ç†é¡åˆ¥ä¸å¹³è¡¡å•é¡Œ
2. **ç‰¹å¾µæå–**: æ”¹é€²ç‰¹å¾µæå–èƒ½åŠ›
3. **æ­£å‰‡åŒ–**: å¢åŠ Dropoutç­‰æ­£å‰‡åŒ–æŠ€è¡“
4. **å­¸ç¿’ç‡èª¿åº¦**: å„ªåŒ–å­¸ç¿’ç‡ç­–ç•¥

## ğŸ“Š è©³ç´°æ€§èƒ½åœ–è¡¨

è©³ç´°æ€§èƒ½åœ–è¡¨å·²ä¿å­˜ç‚º: `comprehensive_validation_metrics.png`

---
*ç”Ÿæˆæ™‚é–“: 2025-08-02*
*æ¨¡å‹ç‰ˆæœ¬: UltimateJointModel v1.0*
"""
        
        # ä¿å­˜å ±å‘Š
        with open('comprehensive_validation_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("âœ… è©³ç´°é©—è­‰å ±å‘Šå·²ä¿å­˜ç‚º: comprehensive_validation_report.md")
        return report
    
    def run_calculation(self):
        """é‹è¡Œå®Œæ•´è¨ˆç®—"""
        print("ğŸš€ é–‹å§‹å…¨é¢é©—è­‰æŒ‡æ¨™è¨ˆç®—...")
        
        # è©•ä¼°æ¨¡å‹
        self.evaluate_model()
        
        # è¨ˆç®—æª¢æ¸¬æŒ‡æ¨™
        detection_metrics = self.calculate_detection_metrics()
        
        # è¨ˆç®—åˆ†é¡æŒ‡æ¨™
        classification_metrics = self.calculate_classification_metrics()
        
        # ç”Ÿæˆå¯è¦–åŒ–
        self.generate_visualizations(detection_metrics, classification_metrics)
        
        # ç”Ÿæˆè©³ç´°å ±å‘Š
        self.generate_detailed_report(detection_metrics, classification_metrics)
        
        return detection_metrics, classification_metrics

def main():
    parser = argparse.ArgumentParser(description='Comprehensive Validation Metrics Calculator')
    parser.add_argument('--data', type=str, default='converted_dataset_fixed/data.yaml', help='Path to data.yaml file')
    parser.add_argument('--weights', type=str, default='joint_model_ultimate.pth', help='Path to model weights')
    
    args = parser.parse_args()
    
    try:
        # å‰µå»ºè¨ˆç®—å™¨
        calculator = ComprehensiveMetricsCalculator(args.data, args.weights)
        
        # é‹è¡Œè¨ˆç®—
        detection_metrics, classification_metrics = calculator.run_calculation()
        
        print("\nğŸ‰ å…¨é¢é©—è­‰æŒ‡æ¨™è¨ˆç®—å®Œæˆï¼")
        print("ğŸ“Š ä¸»è¦çµæœ:")
        print(f"   - æª¢æ¸¬ä»»å‹™ mAP@0.5: {detection_metrics['map_50']:.4f}")
        print(f"   - åˆ†é¡ä»»å‹™æº–ç¢ºç‡: {classification_metrics['accuracy']:.4f}")
        print(f"   - åˆ†é¡ä»»å‹™F1-Score: {classification_metrics['weighted_f1']:.4f}")
        
    except Exception as e:
        print(f"âŒ è¨ˆç®—å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    import argparse
    main() 