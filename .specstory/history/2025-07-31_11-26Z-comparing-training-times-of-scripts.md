<!-- Generated by SpecStory -->

# Comparing training times of scripts (2025-07-31 11:26Z)

_**User**_

@train_joint_ultimate.py @train_joint_advanced.py explain and compre, why ultimate take long time then advanced?

---

_**Assistant**_

根據你提供的 `train_joint_ultimate.py` 和 `train_joint_advanced.py` 兩個檔案，以下是詳細的比較與說明，為什麼 ultimate 版訓練時間會比 advanced 版長：

---

## 1. 網路結構（Model Architecture）

### UltimateJointModel（ultimate）
- **Backbone 更深更大**：有 5 個 block，每個 block 都有多層卷積、BatchNorm 和 ReLU，最後還有 AdaptiveAvgPool2d。
- **Detection Head**：3 層全連接（Linear）+ Dropout。
- **Classification Head**：4 層全連接（Linear）+ Dropout。
- **Attention 機制**：額外有一個 attention 分支，對分類特徵進行加權。
- **參數量更多**，計算量更大。

### AdvancedJointModel（advanced）
- **Backbone 較淺**：只有 4 個 block，且每個 block 層數較少。
- **Detection Head**：3 層全連接（Linear）+ Dropout。
- **Classification Head**：4 層全連接（Linear）+ Dropout。
- **沒有 attention 機制**。
- **參數量較少**，計算量較小。

---

## 2. 資料增強與輸入尺寸（Data Augmentation & Input Size）

- **Ultimate**：輸入尺寸為 (224, 224)，資料增強較「平衡」。
- **Advanced**：輸入尺寸為 (256, 256)，資料增強更「強烈」。
- 但這部分對訓練速度影響不大，主要還是網路結構。

---

## 3. 損失函數與優化器（Loss & Optimizer）

- **Ultimate**：
  - bbox: SmoothL1Loss
  - cls: FocalLoss（自定義，支援 one-hot）
  - optimizer: AdamW，**分層學習率**（backbone 0.0001, 其他 0.001）
  - scheduler: OneCycleLR（max_lr 為 [0.001, 0.01, 0.01, 0.01]，多組 learning rate）
  - **梯度裁剪 max_norm=0.5**（更嚴格）

- **Advanced**：
  - bbox: SmoothL1Loss
  - cls: FocalLoss（BCEWithLogitsLoss 版本，支援 one-hot）
  - optimizer: AdamW，單一學習率 0.001
  - scheduler: CosineAnnealingLR
  - **梯度裁剪 max_norm=1.0**（較寬鬆）

---

## 4. 資料集過採樣（Oversampling）

- 兩者都對訓練集做過採樣，但 ultimate 版的 oversample 策略更複雜（針對缺失類別有特殊處理），但這對單次 epoch 時間影響有限。

---

## 5. 訓練細節

- **Ultimate**：
  - attention 機制會多一次前向計算與乘法運算。
  - backbone 更深，參數更多，單張圖像前向/反向計算時間更久。
  - 梯度裁剪更嚴格，可能導致收斂慢。
  - OneCycleLR 調度器計算更複雜。
- **Advanced**：
  - 結構較簡單，forward/backward 計算快。
  - 沒有 attention，無需額外計算。
  - 梯度裁剪較寬鬆，訓練步伐較大。

---

## 6. 實際訓練時間差異的根本原因

**Ultimate 版訓練時間較長的主要原因：**
1. **網路更深更大，參數量多，forward/backward 計算量大。**
2. **多一個 attention 分支，增加額外計算。**
3. **分層學習率與 OneCycleLR 調度器，優化器步驟更複雜。**
4. **梯度裁剪更嚴格，可能導致每步更新幅度小，收斂慢。**

**Advanced 版訓練時間較短的原因：**
1. **網路較淺，參數少，計算快。**
2. **無 attention，forward/backward 較快。**
3. **優化器與 scheduler 較簡單。**

---

## 總結

- **Ultimate 版更注重模型表現與泛化，結構複雜，訓練時間自然更長。**
- **Advanced 版則偏向速度與實用，結構簡單，訓練時間較短。**

如果你追求更高準確率與泛化能力，ultimate 版值得等待；如果你需要快速實驗或資源有限，advanced 版會更合適。

如需更詳細的參數量或 FLOPs（計算量）對比，可以進一步用 torchsummary 或 ptflops 工具分析。

---

