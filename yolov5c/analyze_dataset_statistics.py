#!/usr/bin/env python3
"""
Dataset Statistics Analyzer
åˆ†ææ•¸æ“šé›†ä¸­çš„æª¢æ¸¬é¡åˆ¥å’Œåˆ†é¡æ¨™ç±¤æ•¸é‡çµ±è¨ˆ
"""

import os
import yaml
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter
import argparse
import logging
from pathlib import Path

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DatasetStatisticsAnalyzer:
    def __init__(self, data_yaml_path):
        """åˆå§‹åŒ–æ•¸æ“šé›†çµ±è¨ˆåˆ†æå™¨"""
        self.data_yaml_path = data_yaml_path
        self.data_config = self.load_data_config()
        self.detection_classes = self.data_config.get('names', [])
        self.classification_classes = ['PSAX', 'PLAX', 'A4C']  # æ ¹æ“šæ¨™ç±¤æ ¼å¼
        
        # çµ±è¨ˆæ•¸æ“š
        self.detection_stats = defaultdict(lambda: defaultdict(int))
        self.classification_stats = defaultdict(lambda: defaultdict(int))
        self.combined_stats = defaultdict(lambda: defaultdict(int))
        
    def load_data_config(self):
        """è¼‰å…¥æ•¸æ“šé…ç½®æ–‡ä»¶"""
        try:
            with open(self.data_yaml_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"âœ… æˆåŠŸè¼‰å…¥æ•¸æ“šé…ç½®: {self.data_yaml_path}")
            return config
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥æ•¸æ“šé…ç½®å¤±æ•—: {e}")
            return {}
    
    def parse_label_file(self, label_path):
        """è§£ææ¨™ç±¤æ–‡ä»¶"""
        try:
            with open(label_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            if len(lines) < 2:
                return None, None
            
            # ç¬¬ä¸€è¡Œï¼šæª¢æ¸¬æ¨™ç±¤ (YOLOæ ¼å¼)
            detection_line = lines[0].strip()
            detection_labels = []
            
            if detection_line:
                parts = detection_line.split()
                if len(parts) >= 5:  # è‡³å°‘åŒ…å« class_id center_x center_y width height
                    class_id = int(parts[0])
                    detection_labels.append(class_id)
            
            # ç¬¬äºŒè¡Œï¼šåˆ†é¡æ¨™ç±¤ (one-hotç·¨ç¢¼)
            classification_line = lines[1].strip()
            classification_label = None
            
            if classification_line:
                parts = classification_line.split()
                if len(parts) == 3:  # PSAX PLAX A4C
                    # æ‰¾åˆ°å€¼ç‚º1çš„ç´¢å¼•
                    for i, val in enumerate(parts):
                        if val == '1':
                            classification_label = i
                            break
            
            return detection_labels, classification_label
            
        except Exception as e:
            logger.warning(f"âš ï¸ è§£ææ¨™ç±¤æ–‡ä»¶å¤±æ•— {label_path}: {e}")
            return None, None
    
    def analyze_dataset_split(self, split_name, labels_dir):
        """åˆ†ææ•¸æ“šé›†åˆ†å‰²"""
        logger.info(f"ğŸ“Š åˆ†æ {split_name} åˆ†å‰²...")
        
        if not os.path.exists(labels_dir):
            logger.warning(f"âš ï¸ æ¨™ç±¤ç›®éŒ„ä¸å­˜åœ¨: {labels_dir}")
            return
        
        label_files = [f for f in os.listdir(labels_dir) if f.endswith('.txt')]
        logger.info(f"ğŸ“ æ‰¾åˆ° {len(label_files)} å€‹æ¨™ç±¤æ–‡ä»¶")
        
        for label_file in label_files:
            label_path = os.path.join(labels_dir, label_file)
            detection_labels, classification_label = self.parse_label_file(label_path)
            
            if detection_labels is not None:
                # çµ±è¨ˆæª¢æ¸¬é¡åˆ¥
                for class_id in detection_labels:
                    if 0 <= class_id < len(self.detection_classes):
                        class_name = self.detection_classes[class_id]
                        self.detection_stats[split_name][class_name] += 1
                        self.detection_stats['total'][class_name] += 1
            
            if classification_label is not None:
                # çµ±è¨ˆåˆ†é¡é¡åˆ¥
                if 0 <= classification_label < len(self.classification_classes):
                    class_name = self.classification_classes[classification_label]
                    self.classification_stats[split_name][class_name] += 1
                    self.classification_stats['total'][class_name] += 1
                    
                    # çµ±è¨ˆçµ„åˆæƒ…æ³
                    if detection_labels:
                        for det_class_id in detection_labels:
                            if 0 <= det_class_id < len(self.detection_classes):
                                det_class_name = self.detection_classes[det_class_id]
                                key = f"{det_class_name}_{class_name}"
                                self.combined_stats[split_name][key] += 1
                                self.combined_stats['total'][key] += 1
    
    def analyze_full_dataset(self):
        """åˆ†æå®Œæ•´æ•¸æ“šé›†"""
        logger.info("ğŸš€ é–‹å§‹åˆ†æå®Œæ•´æ•¸æ“šé›†...")
        
        # åˆ†æå„å€‹åˆ†å‰²
        for split_name in ['train', 'val', 'test']:
            labels_dir = os.path.join(os.path.dirname(self.data_yaml_path), split_name, 'labels')
            self.analyze_dataset_split(split_name, labels_dir)
        
        logger.info("âœ… æ•¸æ“šé›†åˆ†æå®Œæˆ")
    
    def generate_statistics_report(self):
        """ç”Ÿæˆçµ±è¨ˆå ±å‘Š"""
        report = []
        report.append("# æ•¸æ“šé›†çµ±è¨ˆåˆ†æå ±å‘Š")
        report.append("=" * 50)
        report.append("")
        
        # æª¢æ¸¬é¡åˆ¥çµ±è¨ˆ
        report.append("## 1. æª¢æ¸¬é¡åˆ¥çµ±è¨ˆ")
        report.append("-" * 30)
        for split_name in ['train', 'val', 'test', 'total']:
            report.append(f"\n### {split_name.upper()} åˆ†å‰²")
            total_detections = sum(self.detection_stats[split_name].values())
            for class_name, count in self.detection_stats[split_name].items():
                percentage = (count / total_detections * 100) if total_detections > 0 else 0
                report.append(f"  - {class_name}: {count} ({percentage:.1f}%)")
        
        # åˆ†é¡é¡åˆ¥çµ±è¨ˆ
        report.append("\n\n## 2. åˆ†é¡é¡åˆ¥çµ±è¨ˆ")
        report.append("-" * 30)
        for split_name in ['train', 'val', 'test', 'total']:
            report.append(f"\n### {split_name.upper()} åˆ†å‰²")
            total_classifications = sum(self.classification_stats[split_name].values())
            for class_name, count in self.classification_stats[split_name].items():
                percentage = (count / total_classifications * 100) if total_classifications > 0 else 0
                report.append(f"  - {class_name}: {count} ({percentage:.1f}%)")
        
        # çµ„åˆçµ±è¨ˆ
        report.append("\n\n## 3. æª¢æ¸¬+åˆ†é¡çµ„åˆçµ±è¨ˆ")
        report.append("-" * 30)
        for split_name in ['train', 'val', 'test', 'total']:
            report.append(f"\n### {split_name.upper()} åˆ†å‰²")
            total_combined = sum(self.combined_stats[split_name].values())
            for combo_name, count in self.combined_stats[split_name].items():
                percentage = (count / total_combined * 100) if total_combined > 0 else 0
                report.append(f"  - {combo_name}: {count} ({percentage:.1f}%)")
        
        # æ•¸æ“šé›†å¹³è¡¡æ€§åˆ†æ
        report.append("\n\n## 4. æ•¸æ“šé›†å¹³è¡¡æ€§åˆ†æ")
        report.append("-" * 30)
        
        # æª¢æ¸¬é¡åˆ¥å¹³è¡¡æ€§
        detection_counts = list(self.detection_stats['total'].values())
        if detection_counts:
            detection_std = np.std(detection_counts)
            detection_mean = np.mean(detection_counts)
            detection_cv = detection_std / detection_mean if detection_mean > 0 else 0
            report.append(f"\n### æª¢æ¸¬é¡åˆ¥å¹³è¡¡æ€§")
            report.append(f"  - æ¨™æº–å·®: {detection_std:.2f}")
            report.append(f"  - è®Šç•°ä¿‚æ•¸: {detection_cv:.3f}")
            report.append(f"  - å¹³è¡¡æ€§è©•ä¼°: {'è‰¯å¥½' if detection_cv < 0.5 else 'éœ€è¦æ”¹é€²'}")
        
        # åˆ†é¡é¡åˆ¥å¹³è¡¡æ€§
        classification_counts = list(self.classification_stats['total'].values())
        if classification_counts:
            classification_std = np.std(classification_counts)
            classification_mean = np.mean(classification_counts)
            classification_cv = classification_std / classification_mean if classification_mean > 0 else 0
            report.append(f"\n### åˆ†é¡é¡åˆ¥å¹³è¡¡æ€§")
            report.append(f"  - æ¨™æº–å·®: {classification_std:.2f}")
            report.append(f"  - è®Šç•°ä¿‚æ•¸: {classification_cv:.3f}")
            report.append(f"  - å¹³è¡¡æ€§è©•ä¼°: {'è‰¯å¥½' if classification_cv < 0.5 else 'éœ€è¦æ”¹é€²'}")
        
        return "\n".join(report)
    
    def create_visualizations(self, output_dir):
        """å‰µå»ºå¯è¦–åŒ–åœ–è¡¨"""
        os.makedirs(output_dir, exist_ok=True)
        
        # è¨­ç½®ä¸­æ–‡å­—é«”
        plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. æª¢æ¸¬é¡åˆ¥åˆ†ä½ˆåœ–
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('æ•¸æ“šé›†çµ±è¨ˆåˆ†æ', fontsize=16, fontweight='bold')
        
        # æª¢æ¸¬é¡åˆ¥ - ç¸½é«”åˆ†ä½ˆ
        detection_data = self.detection_stats['total']
        if detection_data:
            axes[0, 0].pie(detection_data.values(), labels=detection_data.keys(), autopct='%1.1f%%')
            axes[0, 0].set_title('æª¢æ¸¬é¡åˆ¥ç¸½é«”åˆ†ä½ˆ')
        
        # åˆ†é¡é¡åˆ¥ - ç¸½é«”åˆ†ä½ˆ
        classification_data = self.classification_stats['total']
        if classification_data:
            axes[0, 1].pie(classification_data.values(), labels=classification_data.keys(), autopct='%1.1f%%')
            axes[0, 1].set_title('åˆ†é¡é¡åˆ¥ç¸½é«”åˆ†ä½ˆ')
        
        # æª¢æ¸¬é¡åˆ¥ - å„åˆ†å‰²åˆ†ä½ˆ
        splits = ['train', 'val', 'test']
        detection_splits = {split: self.detection_stats[split] for split in splits}
        if detection_splits:
            x = np.arange(len(self.detection_classes))
            width = 0.25
            
            for i, (split, data) in enumerate(detection_splits.items()):
                values = [data.get(cls, 0) for cls in self.detection_classes]
                axes[1, 0].bar(x + i*width, values, width, label=split)
            
            axes[1, 0].set_xlabel('æª¢æ¸¬é¡åˆ¥')
            axes[1, 0].set_ylabel('æ•¸é‡')
            axes[1, 0].set_title('æª¢æ¸¬é¡åˆ¥å„åˆ†å‰²åˆ†ä½ˆ')
            axes[1, 0].set_xticks(x + width)
            axes[1, 0].set_xticklabels(self.detection_classes)
            axes[1, 0].legend()
        
        # åˆ†é¡é¡åˆ¥ - å„åˆ†å‰²åˆ†ä½ˆ
        classification_splits = {split: self.classification_stats[split] for split in splits}
        if classification_splits:
            x = np.arange(len(self.classification_classes))
            width = 0.25
            
            for i, (split, data) in enumerate(classification_splits.items()):
                values = [data.get(cls, 0) for cls in self.classification_classes]
                axes[1, 1].bar(x + i*width, values, width, label=split)
            
            axes[1, 1].set_xlabel('åˆ†é¡é¡åˆ¥')
            axes[1, 1].set_ylabel('æ•¸é‡')
            axes[1, 1].set_title('åˆ†é¡é¡åˆ¥å„åˆ†å‰²åˆ†ä½ˆ')
            axes[1, 1].set_xticks(x + width)
            axes[1, 1].set_xticklabels(self.classification_classes)
            axes[1, 1].legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'dataset_statistics.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. çµ„åˆçµ±è¨ˆç†±åŠ›åœ–
        if self.combined_stats['total']:
            fig, ax = plt.subplots(figsize=(12, 8))
            
            # å‰µå»ºçµ„åˆçŸ©é™£
            combo_matrix = np.zeros((len(self.detection_classes), len(self.classification_classes)))
            for combo_name, count in self.combined_stats['total'].items():
                det_class, cls_class = combo_name.split('_', 1)
                det_idx = self.detection_classes.index(det_class)
                cls_idx = self.classification_classes.index(cls_class)
                combo_matrix[det_idx, cls_idx] = count
            
            sns.heatmap(combo_matrix, annot=True, fmt='.0f', cmap='YlOrRd',
                       xticklabels=self.classification_classes,
                       yticklabels=self.detection_classes,
                       ax=ax)
            ax.set_title('æª¢æ¸¬+åˆ†é¡çµ„åˆçµ±è¨ˆç†±åŠ›åœ–')
            ax.set_xlabel('åˆ†é¡é¡åˆ¥')
            ax.set_ylabel('æª¢æ¸¬é¡åˆ¥')
            
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'combined_statistics_heatmap.png'), dpi=300, bbox_inches='tight')
            plt.close()
        
        logger.info(f"ğŸ“Š å¯è¦–åŒ–åœ–è¡¨å·²ä¿å­˜åˆ°: {output_dir}")
    
    def save_statistics(self, output_dir):
        """ä¿å­˜çµ±è¨ˆçµæœ"""
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜å ±å‘Š
        report = self.generate_statistics_report()
        report_path = os.path.join(output_dir, 'dataset_statistics_report.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # ä¿å­˜åŸå§‹æ•¸æ“š
        import json
        stats_data = {
            'detection_stats': dict(self.detection_stats),
            'classification_stats': dict(self.classification_stats),
            'combined_stats': dict(self.combined_stats),
            'detection_classes': self.detection_classes,
            'classification_classes': self.classification_classes
        }
        
        stats_path = os.path.join(output_dir, 'dataset_statistics.json')
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“„ çµ±è¨ˆå ±å‘Šå·²ä¿å­˜åˆ°: {output_dir}")
        return report_path

def main():
    parser = argparse.ArgumentParser(description='åˆ†ææ•¸æ“šé›†çµ±è¨ˆä¿¡æ¯')
    parser.add_argument('--data', type=str, default='converted_dataset_fixed/data.yaml',
                       help='æ•¸æ“šé…ç½®æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--output', type=str, default='dataset_analysis',
                       help='è¼¸å‡ºç›®éŒ„')
    
    args = parser.parse_args()
    
    # æª¢æŸ¥æ•¸æ“šé…ç½®æ–‡ä»¶
    if not os.path.exists(args.data):
        logger.error(f"âŒ æ•¸æ“šé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.data}")
        return
    
    # å‰µå»ºåˆ†æå™¨
    analyzer = DatasetStatisticsAnalyzer(args.data)
    
    # åˆ†ææ•¸æ“šé›†
    analyzer.analyze_full_dataset()
    
    # ç”Ÿæˆå ±å‘Šå’Œå¯è¦–åŒ–
    output_dir = args.output
    report_path = analyzer.save_statistics(output_dir)
    analyzer.create_visualizations(output_dir)
    
    # é¡¯ç¤ºå ±å‘Šæ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š æ•¸æ“šé›†çµ±è¨ˆåˆ†æå®Œæˆ")
    print("="*60)
    
    # é¡¯ç¤ºæª¢æ¸¬é¡åˆ¥çµ±è¨ˆ
    print("\nğŸ” æª¢æ¸¬é¡åˆ¥çµ±è¨ˆ (ç¸½è¨ˆ):")
    total_detections = sum(analyzer.detection_stats['total'].values())
    for class_name, count in analyzer.detection_stats['total'].items():
        percentage = (count / total_detections * 100) if total_detections > 0 else 0
        print(f"  {class_name}: {count} ({percentage:.1f}%)")
    
    # é¡¯ç¤ºåˆ†é¡é¡åˆ¥çµ±è¨ˆ
    print("\nğŸ·ï¸ åˆ†é¡é¡åˆ¥çµ±è¨ˆ (ç¸½è¨ˆ):")
    total_classifications = sum(analyzer.classification_stats['total'].values())
    for class_name, count in analyzer.classification_stats['total'].items():
        percentage = (count / total_classifications * 100) if total_classifications > 0 else 0
        print(f"  {class_name}: {count} ({percentage:.1f}%)")
    
    print(f"\nğŸ“„ è©³ç´°å ±å‘Š: {report_path}")
    print(f"ğŸ“Š å¯è¦–åŒ–åœ–è¡¨: {output_dir}/")

if __name__ == "__main__":
    main() 