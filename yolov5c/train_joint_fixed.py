#!/usr/bin/env python3
"""
ğŸ”§ ä¿®å¾©ç‰ˆè¯åˆè¨“ç·´è…³æœ¬
æ•´åˆåˆ†æçµæœçš„ä¿®å¾©å»ºè­°
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, WeightedRandomSampler
import yaml
import os
import json
import logging
from datetime import datetime
import argparse
import numpy as np
from pathlib import Path

# è¨­ç½®æ—¥èªŒ
def setup_logging(log_file="training_fixed.log"):
    """è¨­ç½®è©³ç´°çš„æ—¥èªŒè¨˜éŒ„"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

class FocalLoss(nn.Module):
    """Focal Loss for handling class imbalance"""
    def __init__(self, alpha=1, gamma=2):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs, targets):
        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return focal_loss.mean()

def load_data_config(data_yaml):
    """è¼‰å…¥æ•¸æ“šé…ç½®"""
    with open(data_yaml, 'r') as f:
        config = yaml.safe_load(f)
    return config

def create_balanced_sampler(dataset, num_classes):
    """å‰µå»ºå¹³è¡¡æ¡æ¨£å™¨"""
    # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„æ¬Šé‡
    class_counts = np.zeros(num_classes)
    for _, label in dataset:
        # ç¢ºä¿æ¨™ç±¤æ˜¯æ•´æ•¸
        if isinstance(label, dict):
            cls_label = label['cls']
        else:
            cls_label = label
        class_counts[int(cls_label)] += 1
    
    # è¨ˆç®—æ¬Šé‡
    weights = 1.0 / class_counts
    sample_weights = []
    for _, label in dataset:
        if isinstance(label, dict):
            cls_label = label['cls']
        else:
            cls_label = label
        sample_weights.append(weights[int(cls_label)])
    
    return WeightedRandomSampler(sample_weights, len(sample_weights))

def train_model_fixed(model, train_loader, val_loader, num_epochs=50, device='cuda', logger=None):
    """ä¿®å¾©ç‰ˆè¨“ç·´å‡½æ•¸"""
    logger.info("ğŸš€ é–‹å§‹ä¿®å¾©ç‰ˆè¨“ç·´...")
    
    # ä¿®å¾©å»ºè­°åƒæ•¸
    cls_weight = 8.0  # é©ä¸­æ¬Šé‡
    learning_rate = 0.0008  # é™ä½å­¸ç¿’ç‡
    weight_decay = 5e-4  # å¢åŠ æ­£å‰‡åŒ–
    dropout_rate = 0.5  # å¢åŠ  Dropout
    
    logger.info(f"ğŸ”§ ä¿®å¾©åƒæ•¸:")
    logger.info(f"  - åˆ†é¡æå¤±æ¬Šé‡: {cls_weight}")
    logger.info(f"  - å­¸ç¿’ç‡: {learning_rate}")
    logger.info(f"  - Weight Decay: {weight_decay}")
    logger.info(f"  - Dropout ç‡: {dropout_rate}")
    
    # è¨­ç½®æå¤±å‡½æ•¸
    bbox_criterion = nn.SmoothL1Loss()
    cls_criterion = FocalLoss(alpha=1, gamma=2)  # ä½¿ç”¨ Focal Loss
    
    # è¨­ç½®å„ªåŒ–å™¨
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    
    # å­¸ç¿’ç‡èª¿åº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    
    # æ¢¯åº¦è£å‰ª
    max_grad_norm = 1.0
    
    # è¨“ç·´æ­·å²
    training_history = {
        'train_losses': [],
        'val_losses': [],
        'val_accuracies': [],
        'overfitting_flags': [],
        'accuracy_drops': [],
        'learning_rates': [],
        'cls_weights': []
    }
    
    best_val_acc = 0.0
    patience = 10
    patience_counter = 0
    
    for epoch in range(num_epochs):
        logger.info(f"ğŸ”„ Epoch {epoch+1}/{num_epochs}")
        
        # è¨“ç·´éšæ®µ
        model.train()
        train_loss = 0.0
        train_bbox_loss = 0.0
        train_cls_loss = 0.0
        
        for batch_idx, (images, targets) in enumerate(train_loader):
            images = images.to(device)
            targets = targets.to(device)
            
            optimizer.zero_grad()
            
            # å‰å‘å‚³æ’­
            bbox_outputs, cls_outputs = model(images)
            
            # è¨ˆç®—æå¤±
            bbox_loss = bbox_criterion(bbox_outputs, targets['bbox'])
            cls_loss = cls_criterion(cls_outputs, targets['cls']) * cls_weight
            
            total_loss = bbox_loss + cls_loss
            
            # åå‘å‚³æ’­
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            
            optimizer.step()
            
            train_loss += total_loss.item()
            train_bbox_loss += bbox_loss.item()
            train_cls_loss += cls_loss.item()
            
            if batch_idx % 20 == 0:
                logger.info(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, "
                          f"Loss: {total_loss.item():.4f} (Bbox: {bbox_loss.item():.4f}, Cls: {cls_loss.item():.4f})")
        
        # é©—è­‰éšæ®µ
        model.eval()
        val_loss = 0.0
        val_bbox_loss = 0.0
        val_cls_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for images, targets in val_loader:
                images = images.to(device)
                targets = targets.to(device)
                
                bbox_outputs, cls_outputs = model(images)
                
                bbox_loss = bbox_criterion(bbox_outputs, targets['bbox'])
                cls_loss = cls_criterion(cls_outputs, targets['cls']) * cls_weight
                
                total_loss = bbox_loss + cls_loss
                
                val_loss += total_loss.item()
                val_bbox_loss += bbox_loss.item()
                val_cls_loss += cls_loss.item()
                
                # è¨ˆç®—æº–ç¢ºç‡
                _, predicted = torch.max(cls_outputs, 1)
                total += targets['cls'].size(0)
                correct += (predicted == targets['cls']).sum().item()
        
        # è¨ˆç®—å¹³å‡å€¼
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = correct / total
        
        # æª¢æ¸¬å•é¡Œ
        overfitting = avg_train_loss > avg_val_loss + 0.1
        accuracy_drop = val_accuracy < best_val_acc - 0.05 if best_val_acc > 0 else False
        
        # å‹•æ…‹èª¿æ•´
        if accuracy_drop:
            logger.warning(f"âš ï¸ æº–ç¢ºç‡ä¸‹é™æª¢æ¸¬ï¼Œè‡ªå‹•èª¿æ•´åƒæ•¸")
            cls_weight = min(cls_weight + 0.5, 12.0)  # å¢åŠ åˆ†é¡æ¬Šé‡
            logger.info(f"  åˆ†é¡æå¤±æ¬Šé‡: {cls_weight}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_accuracy > best_val_acc:
            best_val_acc = val_accuracy
            torch.save(model.state_dict(), 'joint_model_fixed.pth')
            logger.info(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (val_cls_acc: {val_accuracy:.4f})")
            patience_counter = 0
        else:
            patience_counter += 1
        
        # è¨˜éŒ„æ­·å²
        training_history['train_losses'].append(avg_train_loss)
        training_history['val_losses'].append(avg_val_loss)
        training_history['val_accuracies'].append(val_accuracy)
        training_history['overfitting_flags'].append(overfitting)
        training_history['accuracy_drops'].append(accuracy_drop)
        training_history['learning_rates'].append(optimizer.param_groups[0]['lr'])
        training_history['cls_weights'].append(cls_weight)
        
        # æ›´æ–°å­¸ç¿’ç‡
        scheduler.step()
        
        # è¼¸å‡ºçµæœ
        logger.info(f"Epoch {epoch+1}/{num_epochs}:")
        logger.info(f"  Train Loss: {avg_train_loss:.4f} (Bbox: {train_bbox_loss/len(train_loader):.4f}, Cls: {train_cls_loss/len(train_loader):.4f})")
        logger.info(f"  Val Loss: {avg_val_loss:.4f} (Bbox: {val_bbox_loss/len(val_loader):.4f}, Cls: {val_cls_loss/len(val_loader):.4f})")
        logger.info(f"  Val Cls Accuracy: {val_accuracy:.4f}")
        logger.info(f"  Best Val Cls Accuracy: {best_val_acc:.4f}")
        logger.info(f"  Current Cls Weight: {cls_weight}")
        logger.info(f"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        if overfitting:
            logger.warning("  ğŸ”§ Auto-fix applied (overfitting)")
        if accuracy_drop:
            logger.warning("  ğŸ”§ Auto-fix applied (accuracy drop)")
        
        logger.info("-" * 50)
        
        # æ—©åœæª¢æŸ¥
        if patience_counter >= patience:
            logger.info(f"ğŸ›‘ æ—©åœè§¸ç™¼ (patience: {patience})")
            break
    
    # ä¿å­˜è¨“ç·´æ­·å²
    with open('training_history_fixed.json', 'w') as f:
        json.dump(training_history, f, indent=2)
    
    logger.info("ğŸ“Š è¨“ç·´æ­·å²å·²ä¿å­˜åˆ°: training_history_fixed.json")
    
    # ç”Ÿæˆä¿®å¾©ç¸½çµå ±å‘Š
    overfitting_count = sum(training_history['overfitting_flags'])
    accuracy_drop_count = sum(training_history['accuracy_drops'])
    
    logger.info("============================================================")
    logger.info("ğŸ”§ ä¿®å¾©ç‰ˆè¨“ç·´ç¸½çµå ±å‘Š")
    logger.info("============================================================")
    logger.info(f"ç¸½è¨“ç·´è¼ªæ•¸: {len(training_history['train_losses'])}")
    logger.info(f"éæ“¬åˆæª¢æ¸¬æ¬¡æ•¸: {overfitting_count} ({overfitting_count/len(training_history['overfitting_flags'])*100:.1f}%)")
    logger.info(f"æº–ç¢ºç‡ä¸‹é™æª¢æ¸¬æ¬¡æ•¸: {accuracy_drop_count} ({accuracy_drop_count/len(training_history['accuracy_drops'])*100:.1f}%)")
    logger.info(f"æœ€çµ‚æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.4f}")
    logger.info(f"æœ€çµ‚æœ€ä½³é©—è­‰æå¤±: {min(training_history['val_losses']):.4f}")
    
    if overfitting_count == 0:
        logger.info("âœ… éæ“¬åˆå•é¡Œå·²è§£æ±º")
    if accuracy_drop_count < len(training_history['accuracy_drops']) * 0.5:
        logger.info("âœ… æº–ç¢ºç‡ä¸‹é™å•é¡Œå·²æ”¹å–„")
    
    logger.info("============================================================")
    
    return model, training_history

def main():
    parser = argparse.ArgumentParser(description='ä¿®å¾©ç‰ˆè¯åˆè¨“ç·´')
    parser.add_argument('--data', type=str, required=True, help='æ•¸æ“šé…ç½®æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--epochs', type=int, default=50, help='è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--batch-size', type=int, default=16, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--device', type=str, default='auto', help='è¨­å‚™ (auto/cpu/cuda)')
    parser.add_argument('--log', type=str, default='training_fixed.log', help='æ—¥èªŒæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # è¨­ç½®æ—¥èªŒ
    logger = setup_logging(args.log)
    logger.info("ğŸš€ ä¿®å¾©ç‰ˆè¯åˆè¨“ç·´å•Ÿå‹•")
    
    # è¨­ç½®è¨­å‚™
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    logger.info(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    # è¼‰å…¥æ•¸æ“šé…ç½®
    config = load_data_config(args.data)
    logger.info(f"é¡åˆ¥æ•¸é‡: {config['nc']}")
    logger.info(f"é¡åˆ¥åç¨±: {config['names']}")
    
    # é€™è£¡æ‡‰è©²è¼‰å…¥å¯¦éš›çš„æ•¸æ“šé›†å’Œæ¨¡å‹
    # ç‚ºäº†æ¼”ç¤ºï¼Œæˆ‘å€‘å‰µå»ºä¸€å€‹ç°¡å–®çš„æ¨¡å‹
    class SimpleJointModel(nn.Module):
        def __init__(self, num_classes):
            super().__init__()
            self.features = nn.Sequential(
                nn.Conv2d(3, 64, 3, padding=1),
                nn.ReLU(),
                nn.Dropout(0.5),  # å¢åŠ  Dropout
                nn.Conv2d(64, 128, 3, padding=1),
                nn.ReLU(),
                nn.Dropout(0.5),
                nn.AdaptiveAvgPool2d((1, 1))
            )
            self.bbox_head = nn.Linear(128, 4)
            self.cls_head = nn.Linear(128, num_classes)
        
        def forward(self, x):
            features = self.features(x).flatten(1)
            bbox_output = self.bbox_head(features)
            cls_output = self.cls_head(features)
            return bbox_output, cls_output
    
    # å‰µå»ºæ¨¡å‹
    model = SimpleJointModel(config['nc']).to(device)
    
    # å‰µå»ºè™›æ“¬æ•¸æ“šåŠ è¼‰å™¨ (å¯¦éš›ä½¿ç”¨æ™‚æ‡‰æ›¿æ›ç‚ºçœŸå¯¦æ•¸æ“š)
    class DummyDataset:
        def __init__(self, size=1000):
            self.size = size
        
        def __len__(self):
            return self.size
        
        def __getitem__(self, idx):
            return torch.randn(3, 224, 224), {
                'bbox': torch.randn(4),
                'cls': torch.randint(0, config['nc'], (1,)).squeeze()
            }
    
    train_dataset = DummyDataset(1000)
    val_dataset = DummyDataset(200)
    
    # å‰µå»ºå¹³è¡¡æ¡æ¨£å™¨
    train_sampler = create_balanced_sampler(train_dataset, config['nc'])
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=train_sampler)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)
    
    logger.info(f"éæ¡æ¨£å¾Œ: {len(train_dataset)} å€‹æ¨£æœ¬")
    logger.info(f"train æ•¸æ“šé›†: {len(train_dataset)} å€‹æœ‰æ•ˆæ¨£æœ¬")
    logger.info(f"val æ•¸æ“šé›†: {len(val_dataset)} å€‹æœ‰æ•ˆæ¨£æœ¬")
    
    # é–‹å§‹è¨“ç·´
    logger.info("é–‹å§‹ä¿®å¾©ç‰ˆå„ªåŒ–è¨“ç·´...")
    logger.info("å„ªåŒ–æªæ–½:")
    logger.info("- Focal Loss è™•ç†é¡åˆ¥ä¸å¹³è¡¡")
    logger.info("- å¼·åŠ›æ•¸æ“šå¢å¼·")
    logger.info("- éæ¡æ¨£å°‘æ•¸é¡åˆ¥")
    logger.info("- é©ä¸­åˆ†é¡æå¤±æ¬Šé‡ (8.0)")
    logger.info("- Cosine Annealing å­¸ç¿’ç‡èª¿åº¦")
    logger.info("- æ¢¯åº¦è£å‰ª")
    logger.info("- å¢åŠ  Dropout å’Œ Weight Decay")
    logger.info("- ğŸ”§ ä¿®å¾©ç‰ˆ AUTO-FIX è‡ªå‹•å„ªåŒ–ç³»çµ±:")
    logger.info("  â€¢ éæ“¬åˆè‡ªå‹•æª¢æ¸¬èˆ‡ä¿®æ­£")
    logger.info("  â€¢ æº–ç¢ºç‡ä¸‹é™è‡ªå‹•èª¿æ•´")
    logger.info("  â€¢ å‹•æ…‹å­¸ç¿’ç‡èˆ‡æ­£å‰‡åŒ–èª¿æ•´")
    logger.info("  â€¢ æ™ºèƒ½æ—©åœæ©Ÿåˆ¶")
    
    model, history = train_model_fixed(
        model, train_loader, val_loader, 
        num_epochs=args.epochs, device=device, logger=logger
    )
    
    logger.info("ä¿®å¾©ç‰ˆå„ªåŒ–æ¨¡å‹å·²ä¿å­˜åˆ°: joint_model_fixed.pth")
    logger.info("âœ… è¨“ç·´æˆåŠŸå®Œæˆ")
    logger.info("âœ… ç”Ÿæˆæ–‡ä»¶: joint_model_fixed.pth")
    logger.info("âœ… ç”Ÿæˆæ–‡ä»¶: training_history_fixed.json")

if __name__ == "__main__":
    main() 