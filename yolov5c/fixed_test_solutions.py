#!/usr/bin/env python3
"""
Fixed Test for Gradient Interference Solutions
Test GradNorm and Dual Backbone approaches with correct data formats
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import json
from datetime import datetime
import time

def create_synthetic_data(batch_size=8, num_classes=4):
    """Create synthetic data for testing with correct formats"""
    # Create synthetic images
    images = torch.randn(batch_size, 3, 64, 64)
    
    # Create synthetic detection targets (correct format for CrossEntropyLoss)
    detection_targets = torch.randint(0, num_classes, (batch_size,))
    
    # Create synthetic classification targets (one-hot)
    classification_targets = torch.zeros(batch_size, num_classes)
    for i in range(batch_size):
        class_idx = torch.randint(0, num_classes, (1,)).item()
        classification_targets[i, class_idx] = 1.0
    
    return images, detection_targets, classification_targets

def test_gradnorm_solution():
    """Test GradNorm solution with synthetic data"""
    
    print("üß™ Ê∏¨Ë©¶ GradNorm Ëß£Ê±∫ÊñπÊ°à")
    print("=" * 50)
    
    try:
        # Import GradNorm modules
        from models.gradnorm import GradNormLoss
        
        # Test parameters
        batch_size = 8
        num_classes = 4
        epochs = 5
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"üìä Ê∏¨Ë©¶ÂèÉÊï∏:")
        print(f"   - ÊâπÊ¨°Â§ßÂ∞è: {batch_size}")
        print(f"   - È°ûÂà•Êï∏: {num_classes}")
        print(f"   - Ë®ìÁ∑¥Ëº™Êï∏: {epochs}")
        print(f"   - Ë®≠ÂÇô: {device}")
        
        # Create simple model for testing
        class SimpleTestModel(nn.Module):
            def __init__(self, num_classes=4):
                super().__init__()
                self.backbone = nn.Sequential(
                    nn.Conv2d(3, 32, 3, padding=1),
                    nn.BatchNorm2d(32),
                    nn.ReLU(),
                    nn.AdaptiveAvgPool2d((1, 1))
                )
                self.detection_head = nn.Linear(32, num_classes)
                self.classification_head = nn.Linear(32, num_classes)
            
            def forward(self, x):
                features = self.backbone(x).view(x.size(0), -1)
                detection = self.detection_head(features)
                classification = self.classification_head(features)
                return detection, classification
        
        model = SimpleTestModel(num_classes).to(device)
        
        # Create loss functions
        detection_loss_fn = nn.CrossEntropyLoss()
        classification_loss_fn = nn.BCEWithLogitsLoss()
        
        # Create GradNorm loss
        gradnorm_loss = GradNormLoss(
            detection_loss_fn=detection_loss_fn,
            classification_loss_fn=classification_loss_fn,
            alpha=1.5,
            initial_detection_weight=1.0,
            initial_classification_weight=0.01
        ).to(device)
        
        # Create optimizer
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
        
        # Training loop
        print(f"\nüîÑ ÈñãÂßãË®ìÁ∑¥ (GradNorm)...")
        start_time = time.time()
        
        detection_weights = []
        classification_weights = []
        epoch_losses = []
        
        for epoch in range(epochs):
            epoch_loss = 0
            batch_count = 0
            
            for batch_i in range(10):  # 10 batches per epoch
                # Create synthetic data
                images, detection_targets, classification_targets = create_synthetic_data(batch_size, num_classes)
                images = images.to(device)
                detection_targets = detection_targets.to(device)
                classification_targets = classification_targets.to(device)
                
                # Forward pass
                optimizer.zero_grad()
                detection_outputs, classification_outputs = model(images)
                
                # Compute loss with GradNorm
                total_loss, loss_info = gradnorm_loss(
                    detection_outputs, classification_outputs,
                    detection_targets, classification_targets
                )
                
                # Backward pass
                total_loss.backward()
                optimizer.step()
                
                epoch_loss += loss_info['total_loss']
                batch_count += 1
                detection_weights.append(loss_info['detection_weight'])
                classification_weights.append(loss_info['classification_weight'])
                
                if batch_i % 3 == 0:
                    print(f"   Epoch {epoch+1}, Batch {batch_i+1}: "
                          f"Loss={loss_info['total_loss']:.4f}, "
                          f"Weights: Det={loss_info['detection_weight']:.3f}, "
                          f"Cls={loss_info['classification_weight']:.3f}")
            
            avg_epoch_loss = epoch_loss / batch_count
            epoch_losses.append(avg_epoch_loss)
        
        training_time = time.time() - start_time
        
        # Results
        results = {
            'solution': 'GradNorm',
            'training_time': training_time,
            'final_detection_weight': detection_weights[-1] if detection_weights else 1.0,
            'final_classification_weight': classification_weights[-1] if classification_weights else 0.01,
            'weight_change_detection': detection_weights[-1] - detection_weights[0] if len(detection_weights) > 1 else 0,
            'weight_change_classification': classification_weights[-1] - classification_weights[0] if len(classification_weights) > 1 else 0,
            'avg_loss': np.mean(epoch_losses) if epoch_losses else 0,
            'model_parameters': sum(p.numel() for p in model.parameters())
        }
        
        print(f"\n‚úÖ GradNorm Ê∏¨Ë©¶ÂÆåÊàê!")
        print(f"   - Ë®ìÁ∑¥ÊôÇÈñì: {training_time:.2f} Áßí")
        print(f"   - ÊúÄÁµÇÊ™¢Ê∏¨Ê¨äÈáç: {results['final_detection_weight']:.3f}")
        print(f"   - ÊúÄÁµÇÂàÜÈ°ûÊ¨äÈáç: {results['final_classification_weight']:.3f}")
        print(f"   - Ê¨äÈáçËÆäÂåñ: Ê™¢Ê∏¨={results['weight_change_detection']:.3f}, ÂàÜÈ°û={results['weight_change_classification']:.3f}")
        print(f"   - Âπ≥ÂùáÊêçÂ§±: {results['avg_loss']:.4f}")
        print(f"   - Ê®°ÂûãÂèÉÊï∏: {results['model_parameters']:,}")
        
        return results
        
    except Exception as e:
        print(f"‚ùå GradNorm Ê∏¨Ë©¶Â§±Êïó: {str(e)}")
        return {'solution': 'GradNorm', 'error': str(e)}

def test_dual_backbone_solution():
    """Test Dual Backbone solution with synthetic data"""
    
    print("\nüß™ Ê∏¨Ë©¶ÈõôÁç®Á´ã Backbone Ëß£Ê±∫ÊñπÊ°à")
    print("=" * 50)
    
    try:
        # Import Dual Backbone modules
        from models.dual_backbone import DualBackboneModel, DualBackboneJointTrainer
        
        # Test parameters
        batch_size = 8
        num_classes = 4
        epochs = 5
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"üìä Ê∏¨Ë©¶ÂèÉÊï∏:")
        print(f"   - ÊâπÊ¨°Â§ßÂ∞è: {batch_size}")
        print(f"   - È°ûÂà•Êï∏: {num_classes}")
        print(f"   - Ë®ìÁ∑¥Ëº™Êï∏: {epochs}")
        print(f"   - Ë®≠ÂÇô: {device}")
        
        # Create dual backbone model
        model = DualBackboneModel(
            num_detection_classes=num_classes,
            num_classification_classes=num_classes,
            width_multiple=0.25,  # Smaller for quick test
            depth_multiple=0.25
        ).to(device)
        
        # Create joint trainer
        trainer = DualBackboneJointTrainer(
            model=model,
            detection_lr=1e-3,
            classification_lr=1e-3,
            detection_weight=1.0,
            classification_weight=0.01
        )
        
        # Create loss functions
        class SimpleDetectionLoss(nn.Module):
            def __init__(self):
                super().__init__()
                self.mse_loss = nn.MSELoss()
            
            def forward(self, predictions, targets):
                total_loss = 0
                for pred in predictions:
                    if pred is not None:
                        # Ensure pred has correct shape for MSE loss
                        if pred.dim() > 2:
                            pred = pred.view(pred.size(0), -1)
                        target_zeros = torch.zeros_like(pred)
                        total_loss += self.mse_loss(pred, target_zeros)
                return total_loss
        
        class SimpleClassificationLoss(nn.Module):
            def __init__(self):
                super().__init__()
                self.bce_loss = nn.BCEWithLogitsLoss()
            
            def forward(self, predictions, targets):
                return self.bce_loss(predictions, targets.float())
        
        # Training loop
        print(f"\nüîÑ ÈñãÂßãË®ìÁ∑¥ (ÈõôÁç®Á´ã Backbone)...")
        start_time = time.time()
        
        epoch_losses = []
        
        for epoch in range(epochs):
            epoch_loss = 0
            batch_count = 0
            
            for batch_i in range(10):  # 10 batches per epoch
                # Create synthetic data
                images, detection_targets, classification_targets = create_synthetic_data(batch_size, num_classes)
                images = images.to(device)
                detection_targets = detection_targets.to(device)
                classification_targets = classification_targets.to(device)
                
                # Training step
                metrics = trainer.train_step(
                    images=images,
                    detection_targets=detection_targets,
                    classification_targets=classification_targets,
                    detection_loss_fn=SimpleDetectionLoss(),
                    classification_loss_fn=SimpleClassificationLoss()
                )
                
                epoch_loss += metrics['total_loss']
                batch_count += 1
                
                if batch_i % 3 == 0:
                    print(f"   Epoch {epoch+1}, Batch {batch_i+1}: "
                          f"Loss={metrics['total_loss']:.4f}, "
                          f"Det={metrics['detection_loss']:.4f}, "
                          f"Cls={metrics['classification_loss']:.4f}")
            
            avg_epoch_loss = epoch_loss / batch_count
            epoch_losses.append(avg_epoch_loss)
        
        training_time = time.time() - start_time
        
        # Results
        results = {
            'solution': 'Dual Backbone',
            'training_time': training_time,
            'final_loss': epoch_losses[-1] if epoch_losses else 0,
            'avg_loss': np.mean(epoch_losses) if epoch_losses else 0,
            'model_parameters': sum(p.numel() for p in model.parameters()),
            'detection_parameters': sum(p.numel() for p in model.detection_backbone.parameters()) + 
                                  sum(p.numel() for p in model.detection_head.parameters()),
            'classification_parameters': sum(p.numel() for p in model.classification_backbone.parameters())
        }
        
        print(f"\n‚úÖ ÈõôÁç®Á´ã Backbone Ê∏¨Ë©¶ÂÆåÊàê!")
        print(f"   - Ë®ìÁ∑¥ÊôÇÈñì: {training_time:.2f} Áßí")
        print(f"   - ÊúÄÁµÇÊêçÂ§±: {results['final_loss']:.4f}")
        print(f"   - Âπ≥ÂùáÊêçÂ§±: {results['avg_loss']:.4f}")
        print(f"   - Ê®°ÂûãÂèÉÊï∏Á∏ΩÊï∏: {results['model_parameters']:,}")
        print(f"   - Ê™¢Ê∏¨ÂèÉÊï∏: {results['detection_parameters']:,}")
        print(f"   - ÂàÜÈ°ûÂèÉÊï∏: {results['classification_parameters']:,}")
        
        return results
        
    except Exception as e:
        print(f"‚ùå ÈõôÁç®Á´ã Backbone Ê∏¨Ë©¶Â§±Êïó: {str(e)}")
        return {'solution': 'Dual Backbone', 'error': str(e)}

def compare_test_results(gradnorm_results, dual_backbone_results):
    """Compare test results"""
    
    print("\nüìä Ê∏¨Ë©¶ÁµêÊûúÊØîËºÉ")
    print("=" * 50)
    
    # Create comparison table
    comparison_data = []
    
    if 'error' not in gradnorm_results and 'error' not in dual_backbone_results:
        comparison_data.append({
            'ÊåáÊ®ô': 'Ë®ìÁ∑¥ÊôÇÈñì (Áßí)',
            'GradNorm': f"{gradnorm_results['training_time']:.2f}",
            'ÈõôÁç®Á´ã Backbone': f"{dual_backbone_results['training_time']:.2f}"
        })
        
        comparison_data.append({
            'ÊåáÊ®ô': 'ÊúÄÁµÇÊêçÂ§±',
            'GradNorm': f"{gradnorm_results['avg_loss']:.4f}",
            'ÈõôÁç®Á´ã Backbone': f"{dual_backbone_results['final_loss']:.4f}"
        })
        
        comparison_data.append({
            'ÊåáÊ®ô': 'Ê®°ÂûãÂèÉÊï∏',
            'GradNorm': f"{gradnorm_results['model_parameters']:,}",
            'ÈõôÁç®Á´ã Backbone': f"{dual_backbone_results['model_parameters']:,}"
        })
        
        comparison_data.append({
            'ÊåáÊ®ô': 'Ê™¢Ê∏¨Ê¨äÈáç',
            'GradNorm': f"{gradnorm_results['final_detection_weight']:.3f}",
            'ÈõôÁç®Á´ã Backbone': 'Âõ∫ÂÆö 1.0'
        })
        
        comparison_data.append({
            'ÊåáÊ®ô': 'ÂàÜÈ°ûÊ¨äÈáç',
            'GradNorm': f"{gradnorm_results['final_classification_weight']:.3f}",
            'ÈõôÁç®Á´ã Backbone': 'Âõ∫ÂÆö 0.01'
        })
    
    # Print comparison table
    if comparison_data:
        print(f"{'ÊåáÊ®ô':<15} {'GradNorm':<15} {'ÈõôÁç®Á´ã Backbone':<15}")
        print("-" * 45)
        for row in comparison_data:
            print(f"{row['ÊåáÊ®ô']:<15} {row['GradNorm']:<15} {row['ÈõôÁç®Á´ã Backbone']:<15}")
    
    # Analysis
    print(f"\nüîç ÂàÜÊûêÁµêÊûú:")
    
    if 'error' in gradnorm_results:
        print(f"   ‚ùå GradNorm: {gradnorm_results['error']}")
    else:
        print(f"   ‚úÖ GradNorm: ÊàêÂäüÈÅãË°åÔºåÊ¨äÈáçÂãïÊÖãË™øÊï¥")
        if gradnorm_results['weight_change_detection'] != 0 or gradnorm_results['weight_change_classification'] != 0:
            print(f"      - Ê¨äÈáçËÆäÂåñÊòéÈ°ØÔºåGradNorm Ê≠£Â∏∏Â∑•‰Ωú")
        else:
            print(f"      - Ê¨äÈáçËÆäÂåñËºÉÂ∞èÔºåÂèØËÉΩÈúÄË¶ÅË™øÊï¥ÂèÉÊï∏")
    
    if 'error' in dual_backbone_results:
        print(f"   ‚ùå ÈõôÁç®Á´ã Backbone: {dual_backbone_results['error']}")
    else:
        print(f"   ‚úÖ ÈõôÁç®Á´ã Backbone: ÊàêÂäüÈÅãË°åÔºå‰ªªÂãôÂÆåÂÖ®Ëß£ËÄ¶")
        if dual_backbone_results['model_parameters'] > 100000:
            print(f"      - Ê®°ÂûãÂèÉÊï∏ËºÉÂ§öÔºåË®àÁÆóË≥áÊ∫êÈúÄÊ±ÇÈ´ò")
        else:
            print(f"      - Ê®°ÂûãÂèÉÊï∏ÂêàÁêÜÔºåÈÅ©ÂêàÂø´ÈÄüÊ∏¨Ë©¶")
    
    # Recommendations
    print(f"\nüí° Âª∫Ë≠∞:")
    if 'error' not in gradnorm_results and 'error' not in dual_backbone_results:
        print(f"   1. ÂÖ©Á®ÆËß£Ê±∫ÊñπÊ°àÈÉΩÊàêÂäüÈÅãË°å")
        print(f"   2. GradNorm ÈÅ©ÂêàË≥áÊ∫êÂèóÈôêÁí∞Â¢É")
        print(f"   3. ÈõôÁç®Á´ã Backbone ÈÅ©ÂêàÈ´òÁ≤æÂ∫¶Ë¶ÅÊ±Ç")
        print(f"   4. Âª∫Ë≠∞Âú®ÁúüÂØ¶Êï∏ÊìöÈõÜ‰∏äÈÄ≤Ë°åÈÄ≤‰∏ÄÊ≠•Ê∏¨Ë©¶")
    elif 'error' not in gradnorm_results:
        print(f"   1. GradNorm Ëß£Ê±∫ÊñπÊ°àÂèØË°å")
        print(f"   2. ÈõôÁç®Á´ã Backbone ÈúÄË¶ÅË™øË©¶")
        print(f"   3. Âª∫Ë≠∞ÂÖà‰ΩøÁî® GradNorm ÊñπÊ°à")
    elif 'error' not in dual_backbone_results:
        print(f"   1. ÈõôÁç®Á´ã Backbone Ëß£Ê±∫ÊñπÊ°àÂèØË°å")
        print(f"   2. GradNorm ÈúÄË¶ÅË™øË©¶")
        print(f"   3. Âª∫Ë≠∞‰ΩøÁî®ÈõôÁç®Á´ã Backbone ÊñπÊ°à")
    else:
        print(f"   1. ÂÖ©Á®ÆÊñπÊ°àÈÉΩÈúÄË¶ÅË™øË©¶")
        print(f"   2. Ê™¢Êü•‰æùË≥¥ÂíåÁí∞Â¢ÉÈÖçÁΩÆ")
        print(f"   3. Âª∫Ë≠∞ÂÖàËß£Ê±∫Âü∫Á§éÂïèÈ°å")

def main():
    """Main test function"""
    
    print("üöÄ Ê¢ØÂ∫¶Âπ≤ÊìæËß£Ê±∫ÊñπÊ°à‰øÆÂæ©Ê∏¨Ë©¶")
    print("=" * 60)
    print("Ê∏¨Ë©¶ÊôÇÈñì:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("=" * 60)
    
    # Test GradNorm solution
    gradnorm_results = test_gradnorm_solution()
    
    # Test Dual Backbone solution
    dual_backbone_results = test_dual_backbone_solution()
    
    # Compare results
    compare_test_results(gradnorm_results, dual_backbone_results)
    
    # Save results
    results = {
        'test_time': datetime.now().isoformat(),
        'gradnorm_results': gradnorm_results,
        'dual_backbone_results': dual_backbone_results
    }
    
    with open('fixed_test_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nüìÅ Ê∏¨Ë©¶ÁµêÊûúÂ∑≤‰øùÂ≠òËá≥ 'fixed_test_results.json'")
    print("=" * 60)
    print("‚úÖ ‰øÆÂæ©Ê∏¨Ë©¶ÂÆåÊàêÔºÅ")

if __name__ == '__main__':
    main() 