#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test Model Performance
ç°¡åŒ–çš„æ¨¡å‹æ€§èƒ½æ¸¬è©¦è…³æœ¬
"""

import os
import sys
import argparse
import numpy as np
import torch
import yaml
from pathlib import Path
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append(str(Path(__file__).parent))

try:
    from train_joint_ultimate import UltimateJointModel, UltimateDataset
except ImportError:
    print("ç„¡æ³•å°å…¥UltimateJointModelï¼Œä½¿ç”¨åŸºç¤æ¨¡å‹")
    from train_joint_simple import SimpleJointModel, MixedDataset as UltimateDataset


def test_model_performance(data_yaml, weights_path):
    """æ¸¬è©¦æ¨¡å‹æ€§èƒ½"""
    print("ğŸš€ é–‹å§‹æ¨¡å‹æ€§èƒ½æ¸¬è©¦...")
    
    # è¼‰å…¥æ•¸æ“šé…ç½®
    with open(data_yaml, 'r') as f:
        config = yaml.safe_load(f)
    
    detection_classes = config.get('names', [])
    classification_classes = ['PSAX', 'PLAX', 'A4C']
    num_detection_classes = len(detection_classes)
    
    print(f"âœ… æ•¸æ“šé…ç½®è¼‰å…¥å®Œæˆ:")
    print(f"   - æª¢æ¸¬é¡åˆ¥: {detection_classes}")
    print(f"   - åˆ†é¡é¡åˆ¥: {classification_classes}")
    
    # è¼‰å…¥æ¨¡å‹
    if not Path(weights_path).exists():
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {weights_path}")
    
    model = UltimateJointModel(num_classes=num_detection_classes)
    checkpoint = torch.load(weights_path, map_location='cpu')
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model.eval()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ: {weights_path}")
    
    # è¼‰å…¥æ¸¬è©¦æ•¸æ“š
    data_dir = Path(data_yaml).parent
    test_dataset = UltimateDataset(
        data_dir=str(data_dir),
        split='test',
        transform=None,
        nc=num_detection_classes
    )
    
    test_loader = torch.utils.data.DataLoader(
        test_dataset,
        batch_size=1,
        shuffle=False,
        num_workers=0
    )
    
    print(f"âœ… æ¸¬è©¦æ•¸æ“šè¼‰å…¥å®Œæˆ: {len(test_dataset)} å€‹æ¨£æœ¬")
    
    # æ”¶é›†é æ¸¬çµæœ
    all_bbox_preds = []
    all_bbox_targets = []
    all_cls_preds = []
    all_cls_targets = []
    
    with torch.no_grad():
        for batch_idx, (images, bbox_targets, cls_targets) in enumerate(test_loader):
            images = images.to(device)
            bbox_targets = bbox_targets.to(device)
            cls_targets = cls_targets.to(device)
            
            # å‰å‘å‚³æ’­
            bbox_preds, cls_preds = model(images)
            
            # æ”¶é›†çµæœ
            all_bbox_preds.append(bbox_preds.cpu().numpy())
            all_bbox_targets.append(bbox_targets.cpu().numpy())
            all_cls_preds.append(cls_preds.cpu().numpy())
            all_cls_targets.append(cls_targets.cpu().numpy())
            
            if batch_idx % 50 == 0:
                print(f"  è™•ç†é€²åº¦: {batch_idx}/{len(test_loader)}")
    
    # è½‰æ›ç‚ºnumpyæ•¸çµ„
    bbox_preds = np.concatenate(all_bbox_preds, axis=0)
    bbox_targets = np.concatenate(all_bbox_targets, axis=0)
    cls_preds = np.concatenate(all_cls_preds, axis=0)
    cls_targets = np.concatenate(all_cls_targets, axis=0)
    
    print(f"âœ… é æ¸¬å®Œæˆ: {len(bbox_preds)} å€‹æ¨£æœ¬")
    
    # è¨ˆç®—æª¢æ¸¬æŒ‡æ¨™
    print("\nğŸ“Š æª¢æ¸¬ä»»å‹™æŒ‡æ¨™:")
    bbox_mae = np.mean(np.abs(bbox_preds - bbox_targets))
    print(f"   - Bbox MAE: {bbox_mae:.4f}")
    
    # è¨ˆç®—IoU
    ious = calculate_iou(bbox_preds, bbox_targets)
    mean_iou = np.mean(ious)
    print(f"   - Mean IoU: {mean_iou:.4f}")
    
    # è¨ˆç®—mAP (ç°¡åŒ–ç‰ˆæœ¬)
    map_score = np.mean(ious > 0.5)
    print(f"   - mAP (IoU>0.5): {map_score:.4f}")
    
    # è¨ˆç®—åˆ†é¡æŒ‡æ¨™
    print("\nğŸ“Š åˆ†é¡ä»»å‹™æŒ‡æ¨™:")
    cls_pred_labels = np.argmax(cls_preds, axis=1)
    cls_target_labels = np.argmax(cls_targets, axis=1)
    
    accuracy = np.mean(cls_pred_labels == cls_target_labels)
    print(f"   - æ•´é«”æº–ç¢ºç‡: {accuracy:.4f}")
    
    # è¨ˆç®—è©³ç´°æŒ‡æ¨™
    precision, recall, f1, support = precision_recall_fscore_support(
        cls_target_labels, cls_pred_labels, average='weighted'
    )
    print(f"   - åŠ æ¬ŠPrecision: {precision:.4f}")
    print(f"   - åŠ æ¬ŠRecall: {recall:.4f}")
    print(f"   - åŠ æ¬ŠF1-Score: {f1:.4f}")
    
    # è¨ˆç®—æ··æ·†çŸ©é™£
    cm = confusion_matrix(cls_target_labels, cls_pred_labels)
    print(f"   - æ··æ·†çŸ©é™£:")
    print(cm)
    
    # æ¯å€‹é¡åˆ¥çš„è©³ç´°æŒ‡æ¨™
    class_report = classification_report(
        cls_target_labels, cls_pred_labels,
        target_names=classification_classes,
        output_dict=True
    )
    
    print(f"\nğŸ“Š æ¯å€‹é¡åˆ¥çš„è©³ç´°æŒ‡æ¨™:")
    for i, class_name in enumerate(classification_classes):
        if i < len(class_report):
            class_metrics = class_report[class_name]
            print(f"   - {class_name}: Precision={class_metrics['precision']:.4f}, "
                  f"Recall={class_metrics['recall']:.4f}, F1={class_metrics['f1-score']:.4f}")
    
    # ç”Ÿæˆæ€§èƒ½ç¸½çµ
    print(f"\nğŸ¯ æ€§èƒ½ç¸½çµ:")
    print(f"   - æª¢æ¸¬ä»»å‹™: MAE={bbox_mae:.4f}, IoU={mean_iou:.4f}, mAP={map_score:.4f}")
    print(f"   - åˆ†é¡ä»»å‹™: æº–ç¢ºç‡={accuracy:.4f}, F1={f1:.4f}")
    
    # æ€§èƒ½è©•ä¼°
    print(f"\nğŸ“ˆ æ€§èƒ½è©•ä¼°:")
    print(f"   - æª¢æ¸¬MAE < 0.1: {'âœ… å„ªç§€' if bbox_mae < 0.1 else 'âš ï¸ éœ€è¦æ”¹é€²'}")
    print(f"   - æª¢æ¸¬IoU > 0.5: {'âœ… å„ªç§€' if mean_iou > 0.5 else 'âš ï¸ éœ€è¦æ”¹é€²'}")
    print(f"   - åˆ†é¡æº–ç¢ºç‡ > 0.8: {'âœ… å„ªç§€' if accuracy > 0.8 else 'âš ï¸ éœ€è¦æ”¹é€²'}")
    print(f"   - åˆ†é¡F1 > 0.7: {'âœ… å„ªç§€' if f1 > 0.7 else 'âš ï¸ éœ€è¦æ”¹é€²'}")
    
    return {
        'detection': {
            'bbox_mae': bbox_mae,
            'mean_iou': mean_iou,
            'map_score': map_score
        },
        'classification': {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'confusion_matrix': cm,
            'class_report': class_report
        }
    }


def calculate_iou(preds, targets):
    """è¨ˆç®—IoU"""
    ious = []
    for pred, target in zip(preds, targets):
        # ç°¡åŒ–çš„IoUè¨ˆç®—
        pred_area = pred[2] * pred[3]  # width * height
        target_area = target[2] * target[3]
        
        # è¨ˆç®—äº¤é›†
        x1 = max(pred[0] - pred[2]/2, target[0] - target[2]/2)
        y1 = max(pred[1] - pred[3]/2, target[1] - target[3]/2)
        x2 = min(pred[0] + pred[2]/2, target[0] + target[2]/2)
        y2 = min(pred[1] + pred[3]/2, target[1] + target[3]/2)
        
        if x2 > x1 and y2 > y1:
            intersection = (x2 - x1) * (y2 - y1)
        else:
            intersection = 0
        
        union = pred_area + target_area - intersection
        iou = intersection / (union + 1e-8)
        ious.append(iou)
    
    return np.array(ious)


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='æ¸¬è©¦æ¨¡å‹æ€§èƒ½')
    parser.add_argument('--data', type=str, required=True, help='æ•¸æ“šé…ç½®æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--weights', type=str, required=True, help='æ¨¡å‹æ¬Šé‡æ–‡ä»¶è·¯å¾‘')
    
    args = parser.parse_args()
    
    try:
        metrics = test_model_performance(args.data, args.weights)
        
        print("\nğŸ‰ æ€§èƒ½æ¸¬è©¦å®Œæˆï¼")
        print(f"ğŸ“Š æª¢æ¸¬MAE: {metrics['detection']['bbox_mae']:.4f}")
        print(f"ğŸ“Š åˆ†é¡æº–ç¢ºç‡: {metrics['classification']['accuracy']:.4f}")
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main() 