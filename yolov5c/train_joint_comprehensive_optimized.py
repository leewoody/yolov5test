#!/usr/bin/env python3
"""
ç¶œåˆå„ªåŒ–çš„ joint detection + classification è¨“ç·´è…³æœ¬
åŒæ™‚æå‡æª¢æ¸¬ç²¾åº¦ (IoU/mAP) å’Œ ARé¡åˆ¥åˆ†é¡æ€§èƒ½
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import argparse
import yaml
import numpy as np
from PIL import Image
import torchvision.transforms as transforms
import random
import os

class ComprehensiveOptimizedJointModel(nn.Module):
    """ç¶œåˆå„ªåŒ–çš„è¯åˆæ¨¡å‹ - åŒæ™‚æå‡æª¢æ¸¬å’ŒARåˆ†é¡"""
    
    def __init__(self, num_classes=4):
        super(ComprehensiveOptimizedJointModel, self).__init__()
        
        # æ·±åº¦ç‰¹å¾µæå–backbone
        self.backbone = nn.Sequential(
            # ç¬¬ä¸€å±¤ - åŸºç¤ç‰¹å¾µ
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            
            # ç¬¬äºŒå±¤ - ä¸­ç´šç‰¹å¾µ
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # ç¬¬ä¸‰å±¤ - é«˜ç´šç‰¹å¾µ
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # ç¬¬å››å±¤ - ç²¾ç´°ç‰¹å¾µ
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # ç¬¬äº”å±¤ - æª¢æ¸¬ç‰¹åŒ–
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # å¤šå°ºåº¦ç‰¹å¾µèåˆ
        self.feature_fusion = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(256, 512),
            nn.ReLU(inplace=True)
        )
        
        # æª¢æ¸¬ç‰¹åŒ–æ³¨æ„åŠ›
        self.detection_attention = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, 512),
            nn.Sigmoid()
        )
        
        # ARç‰¹åŒ–æ³¨æ„åŠ›
        self.ar_attention = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, 512),
            nn.Sigmoid()
        )
        
        # æª¢æ¸¬é ­ - æ·±åº¦å„ªåŒ–
        self.detection_head = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512, 1024),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, 8)  # x1,y1,x2,y2,x3,y3,x4,y4
        )
        
        # åˆ†é¡é ­ - ARç‰¹åŒ–
        self.classification_head = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, 3)  # PSAX, PLAX, A4C
        )
        
    def forward(self, x):
        # Backboneç‰¹å¾µæå–
        features = self.backbone(x)
        
        # ç‰¹å¾µèåˆ
        fused_features = self.feature_fusion(features.view(features.size(0), -1))
        
        # æª¢æ¸¬æ³¨æ„åŠ›
        detection_weights = self.detection_attention(fused_features)
        detection_features = features * detection_weights.view(features.size(0), -1, 1, 1)
        
        # ARæ³¨æ„åŠ›
        ar_weights = self.ar_attention(fused_features)
        ar_features = features * ar_weights.view(features.size(0), -1, 1, 1)
        
        # æª¢æ¸¬å’Œåˆ†é¡é æ¸¬
        bbox_pred = self.detection_head(detection_features)
        cls_pred = self.classification_head(ar_features)
        
        return bbox_pred, cls_pred

class ComprehensiveOptimizedDataset(Dataset):
    """ç¶œåˆå„ªåŒ–çš„æ•¸æ“šé›†"""
    
    def __init__(self, data_dir, split='train', transform=None, nc=4, 
                 ar_oversample=True, detection_enhance=True):
        self.data_dir = Path(data_dir)
        self.split = split
        self.transform = transform
        self.nc = nc
        self.ar_oversample = ar_oversample
        self.detection_enhance = detection_enhance
        
        # è¼‰å…¥æ•¸æ“šé›†é…ç½®
        with open(self.data_dir / 'data.yaml', 'r') as f:
            self.data_config = yaml.safe_load(f)
        
        # ç²å–åœ–åƒå’Œæ¨™ç±¤è·¯å¾‘
        self.images_dir = self.data_dir / self.data_config[split]
        self.labels_dir = self.data_dir / self.data_config[split].replace('images', 'labels')
        
        self.image_files = sorted(list(self.images_dir.glob('*.png')))
        
        # ç¶œåˆå„ªåŒ–ç­–ç•¥
        if split == 'train':
            if self.ar_oversample:
                self.image_files = self._ar_oversample()
            if self.detection_enhance:
                self.image_files = self._detection_enhance()
        
        print(f"ğŸ“Š {split} æ•¸æ“šé›†: {len(self.image_files)} å€‹æ¨£æœ¬")
        
    def _ar_oversample(self):
        """ARé¡åˆ¥éæ¡æ¨£ç­–ç•¥"""
        ar_samples = []
        other_samples = []
        
        for img_file in self.image_files:
            label_file = self.labels_dir / f"{img_file.stem}.txt"
            if label_file.exists():
                with open(label_file, 'r') as f:
                    lines = f.readlines()
                    if len(lines) >= 1:
                        try:
                            class_id = int(lines[0].split()[0])
                            if class_id == 0:  # ARé¡åˆ¥
                                ar_samples.append(img_file)
                            else:
                                other_samples.append(img_file)
                        except:
                            other_samples.append(img_file)
                    else:
                        other_samples.append(img_file)
            else:
                other_samples.append(img_file)
        
        # ARé¡åˆ¥éæ¡æ¨£ (å¢åŠ 4å€)
        oversampled_ar = ar_samples * 4
        
        # çµ„åˆæ‰€æœ‰æ¨£æœ¬
        all_samples = oversampled_ar + other_samples
        random.shuffle(all_samples)
        
        print(f"ğŸ”„ ARé¡åˆ¥éæ¡æ¨£: åŸå§‹ARæ¨£æœ¬ {len(ar_samples)}, éæ¡æ¨£å¾Œ {len(oversampled_ar)}")
        
        return all_samples
    
    def _detection_enhance(self):
        """æª¢æ¸¬å¢å¼·ç­–ç•¥ - å¢åŠ æœ‰æª¢æ¸¬æ¨™ç±¤çš„æ¨£æœ¬"""
        detection_samples = []
        no_detection_samples = []
        
        for img_file in self.image_files:
            label_file = self.labels_dir / f"{img_file.stem}.txt"
            if label_file.exists():
                with open(label_file, 'r') as f:
                    lines = f.readlines()
                    if len(lines) >= 1:
                        detection_line = lines[0].strip().split()
                        if len(detection_line) >= 5:
                            # æœ‰æª¢æ¸¬æ¨™ç±¤çš„æ¨£æœ¬
                            detection_samples.append(img_file)
                        else:
                            no_detection_samples.append(img_file)
                    else:
                        no_detection_samples.append(img_file)
            else:
                no_detection_samples.append(img_file)
        
        # æª¢æ¸¬æ¨£æœ¬å¢å¼· (å¢åŠ 2å€)
        enhanced_detection = detection_samples * 2
        
        # çµ„åˆæ‰€æœ‰æ¨£æœ¬
        all_samples = enhanced_detection + no_detection_samples
        random.shuffle(all_samples)
        
        print(f"ğŸ¯ æª¢æ¸¬å¢å¼·: åŸå§‹æª¢æ¸¬æ¨£æœ¬ {len(detection_samples)}, å¢å¼·å¾Œ {len(enhanced_detection)}")
        
        return all_samples
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        # è¼‰å…¥åœ–åƒ
        img_file = self.image_files[idx]
        image = Image.open(img_file).convert('RGB')
        
        # è¼‰å…¥æ¨™ç±¤
        label_file = self.labels_dir / f"{img_file.stem}.txt"
        
        # åˆå§‹åŒ–æ¨™ç±¤
        bbox_target = torch.zeros(8)  # x1,y1,x2,y2,x3,y3,x4,y4
        classification_target = torch.zeros(3)  # PSAX, PLAX, A4C
        
        if label_file.exists():
            with open(label_file, 'r') as f:
                lines = f.readlines()
                
                if len(lines) >= 1:
                    # è§£ææª¢æ¸¬æ¨™ç±¤ (ç¬¬ä¸€è¡Œ)
                    detection_line = lines[0].strip().split()
                    if len(detection_line) >= 5:
                        class_id = int(detection_line[0])
                        center_x = float(detection_line[1])
                        center_y = float(detection_line[2])
                        width = float(detection_line[3])
                        height = float(detection_line[4])
                        
                        # è½‰æ›ç‚ºé‚Šç•Œæ¡†åº§æ¨™
                        x1 = center_x - width/2
                        y1 = center_y - height/2
                        x2 = center_x + width/2
                        y2 = center_y + height/2
                        x3 = center_x + width/2
                        y3 = center_y - height/2
                        x4 = center_x - width/2
                        y4 = center_y + height/2
                        
                        bbox_target = torch.tensor([x1, y1, x2, y2, x3, y3, x4, y4], dtype=torch.float32)
                
                if len(lines) >= 2:
                    # è§£æåˆ†é¡æ¨™ç±¤ (ç¬¬äºŒè¡Œ)
                    classification_line = lines[1].strip().split()
                    if len(classification_line) >= 3:
                        classification_target = torch.tensor([
                            float(classification_line[0]),  # PSAX
                            float(classification_line[1]),  # PLAX
                            float(classification_line[2])   # A4C
                        ], dtype=torch.float32)
        
        # æ‡‰ç”¨è®Šæ›
        if self.transform:
            image = self.transform(image)
        
        return {
            'image': image,
            'bbox': bbox_target,
            'classification': classification_target
        }

def get_comprehensive_optimized_transforms(split='train'):
    """ç¶œåˆå„ªåŒ–çš„æ•¸æ“šè®Šæ›"""
    if split == 'train':
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(p=0.4),
            transforms.RandomRotation(degrees=8),
            transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.08),
            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    else:
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

class FocalLoss(nn.Module):
    """Focal Loss for å›°é›£æ¨£æœ¬å„ªåŒ–"""
    
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class IoULoss(nn.Module):
    """IoUæå¤±å‡½æ•¸ - ç›´æ¥å„ªåŒ–IoU"""
    
    def __init__(self, reduction='mean'):
        super(IoULoss, self).__init__()
        self.reduction = reduction
    
    def forward(self, pred, target):
        # æå–é‚Šç•Œæ¡†åº§æ¨™
        pred_x1, pred_y1, pred_x2, pred_y2 = pred[:, 0], pred[:, 1], pred[:, 2], pred[:, 3]
        target_x1, target_y1, target_x2, target_y2 = target[:, 0], target[:, 1], target[:, 2], target[:, 3]
        
        # è¨ˆç®—äº¤é›†
        inter_x1 = torch.max(pred_x1, target_x1)
        inter_y1 = torch.max(pred_y1, target_y1)
        inter_x2 = torch.min(pred_x2, target_x2)
        inter_y2 = torch.min(pred_y2, target_y2)
        
        inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)
        
        # è¨ˆç®—ä¸¦é›†
        pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)
        target_area = (target_x2 - target_x1) * (target_y2 - target_y1)
        union_area = pred_area + target_area - inter_area
        
        # è¨ˆç®—IoU
        iou = inter_area / (union_area + 1e-6)
        
        # IoUæå¤±
        iou_loss = 1 - iou
        
        if self.reduction == 'mean':
            return iou_loss.mean()
        elif self.reduction == 'sum':
            return iou_loss.sum()
        else:
            return iou_loss

class LabelSmoothingLoss(nn.Module):
    """æ¨™ç±¤å¹³æ»‘æå¤± - æå‡æ³›åŒ–èƒ½åŠ›"""
    
    def __init__(self, classes=3, smoothing=0.1, dim=-1):
        super(LabelSmoothingLoss, self).__init__()
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.cls = classes
        self.dim = dim
    
    def forward(self, pred, target):
        pred = pred.log_softmax(dim=self.dim)
        with torch.no_grad():
            true_dist = torch.zeros_like(pred)
            true_dist.fill_(self.smoothing / (self.cls - 1))
            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))

def train_model_comprehensive_optimized(model, train_loader, val_loader, num_epochs=50, device='cuda', 
                                      bbox_weight=15.0, cls_weight=10.0, enable_earlystop=False):
    """ç¶œåˆå„ªåŒ–çš„è¯åˆè¨“ç·´"""
    model = model.to(device)
    
    # ç¶œåˆå„ªåŒ–çš„æå¤±å‡½æ•¸
    bbox_criterion_iou = IoULoss()
    bbox_criterion_smooth = nn.SmoothL1Loss()
    cls_criterion_focal = FocalLoss(alpha=1, gamma=2)
    cls_criterion_ce = nn.CrossEntropyLoss()
    cls_criterion_smooth = LabelSmoothingLoss(classes=3, smoothing=0.1)
    
    # ç¶œåˆå„ªåŒ–çš„å„ªåŒ–å™¨
    optimizer = optim.AdamW([
        {'params': model.backbone.parameters(), 'lr': 0.0001},
        {'params': model.feature_fusion.parameters(), 'lr': 0.001},
        {'params': model.detection_attention.parameters(), 'lr': 0.001},
        {'params': model.ar_attention.parameters(), 'lr': 0.001},
        {'params': model.detection_head.parameters(), 'lr': 0.001},
        {'params': model.classification_head.parameters(), 'lr': 0.001}
    ], weight_decay=0.01)
    
    # ç¶œåˆå„ªåŒ–çš„å­¸ç¿’ç‡èª¿åº¦
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer, 
        max_lr=[0.001, 0.01, 0.01, 0.01, 0.01, 0.01],
        epochs=num_epochs,
        steps_per_epoch=len(train_loader),
        pct_start=0.3,
        anneal_strategy='cos'
    )
    
    best_val_iou = 0.0
    best_val_ar_acc = 0.0
    best_model_state = None
    patience = 25
    patience_counter = 0
    
    # è¨“ç·´å¾ªç’°
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        bbox_loss_sum = 0
        cls_loss_sum = 0
        iou_sum = 0
        ar_accuracy_sum = 0
        ar_samples = 0
        
        for batch_idx, batch in enumerate(train_loader):
            images = batch['image'].to(device)
            bbox_targets = batch['bbox'].to(device)
            cls_targets = batch['classification'].to(device)
            
            optimizer.zero_grad()
            
            # å‰å‘å‚³æ’­
            bbox_pred, cls_pred = model(images)
            
            # è¨ˆç®—æª¢æ¸¬æå¤±
            bbox_loss_iou = bbox_criterion_iou(bbox_pred, bbox_targets)
            bbox_loss_smooth = bbox_criterion_smooth(bbox_pred, bbox_targets)
            bbox_loss = 0.8 * bbox_loss_iou + 0.2 * bbox_loss_smooth
            
            # è¨ˆç®—åˆ†é¡æå¤±
            cls_loss_focal = cls_criterion_focal(cls_pred, cls_targets.argmax(dim=1))
            cls_loss_ce = cls_criterion_ce(cls_pred, cls_targets.argmax(dim=1))
            cls_loss_smooth = cls_criterion_smooth(cls_pred, cls_targets.argmax(dim=1))
            cls_loss = 0.5 * cls_loss_focal + 0.3 * cls_loss_ce + 0.2 * cls_loss_smooth
            
            # ç¸½æå¤±
            loss = bbox_weight * bbox_loss + cls_weight * cls_loss
            
            # åå‘å‚³æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            scheduler.step()
            
            total_loss += loss.item()
            bbox_loss_sum += bbox_loss.item()
            cls_loss_sum += cls_loss.item()
            
            # è¨ˆç®—IoU
            with torch.no_grad():
                iou = 1 - bbox_loss_iou.item()
                iou_sum += iou
                
                # è¨ˆç®—ARé¡åˆ¥æº–ç¢ºç‡
                cls_pred_softmax = torch.softmax(cls_pred, dim=1)
                cls_pred_class = cls_pred_softmax.argmax(dim=1)
                cls_target_class = cls_targets.argmax(dim=1)
                
                ar_mask = (cls_target_class == 0)
                if ar_mask.sum() > 0:
                    ar_correct = (cls_pred_class[ar_mask] == cls_target_class[ar_mask]).sum()
                    ar_accuracy_sum += ar_correct.item()
                    ar_samples += ar_mask.sum().item()
            
            if batch_idx % 20 == 0:
                ar_acc = ar_accuracy_sum / max(ar_samples, 1)
                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, '
                      f'Loss: {loss.item():.4f} (Bbox: {bbox_loss.item():.4f}, Cls: {cls_loss.item():.4f}), '
                      f'IoU: {iou:.4f}, AR Acc: {ar_acc:.4f}')
        
        # é©—è­‰
        model.eval()
        val_loss = 0
        val_bbox_loss = 0
        val_cls_loss = 0
        val_iou_sum = 0
        val_cls_accuracy = 0
        val_ar_accuracy = 0
        val_samples = 0
        val_ar_samples = 0
        
        with torch.no_grad():
            for batch in val_loader:
                images = batch['image'].to(device)
                bbox_targets = batch['bbox'].to(device)
                cls_targets = batch['classification'].to(device)
                
                bbox_pred, cls_pred = model(images)
                
                # è¨ˆç®—é©—è­‰æå¤±
                bbox_loss_iou = bbox_criterion_iou(bbox_pred, bbox_targets)
                bbox_loss_smooth = bbox_criterion_smooth(bbox_pred, bbox_targets)
                bbox_loss = 0.8 * bbox_loss_iou + 0.2 * bbox_loss_smooth
                
                cls_loss_focal = cls_criterion_focal(cls_pred, cls_targets.argmax(dim=1))
                cls_loss_ce = cls_criterion_ce(cls_pred, cls_targets.argmax(dim=1))
                cls_loss_smooth = cls_criterion_smooth(cls_pred, cls_targets.argmax(dim=1))
                cls_loss = 0.5 * cls_loss_focal + 0.3 * cls_loss_ce + 0.2 * cls_loss_smooth
                
                loss = bbox_weight * bbox_loss + cls_weight * cls_loss
                
                val_loss += loss.item()
                val_bbox_loss += bbox_loss.item()
                val_cls_loss += cls_loss.item()
                
                # è¨ˆç®—IoU
                iou = 1 - bbox_loss_iou.item()
                val_iou_sum += iou
                
                # è¨ˆç®—æº–ç¢ºç‡
                cls_pred_softmax = torch.softmax(cls_pred, dim=1)
                cls_pred_class = cls_pred_softmax.argmax(dim=1)
                cls_target_class = cls_targets.argmax(dim=1)
                
                val_cls_accuracy += (cls_pred_class == cls_target_class).sum().item()
                val_samples += cls_target_class.size(0)
                
                # ARé¡åˆ¥æº–ç¢ºç‡
                ar_mask = (cls_target_class == 0)
                if ar_mask.sum() > 0:
                    ar_correct = (cls_pred_class[ar_mask] == cls_target_class[ar_mask]).sum()
                    val_ar_accuracy += ar_correct.item()
                    val_ar_samples += ar_mask.sum().item()
        
        # è¨ˆç®—å¹³å‡æŒ‡æ¨™
        avg_val_loss = val_loss / len(val_loader)
        avg_val_bbox_loss = val_bbox_loss / len(val_loader)
        avg_val_cls_loss = val_cls_loss / len(val_loader)
        avg_val_iou = val_iou_sum / len(val_loader)
        val_cls_acc = val_cls_accuracy / max(val_samples, 1)
        val_ar_acc = val_ar_accuracy / max(val_ar_samples, 1)
        
        print(f'Epoch {epoch+1}/{num_epochs} - '
              f'Val Loss: {avg_val_loss:.4f} (Bbox: {avg_val_bbox_loss:.4f}, Cls: {avg_val_cls_loss:.4f}), '
              f'Val IoU: {avg_val_iou:.4f}, Val Cls Acc: {val_cls_acc:.4f}, Val AR Acc: {val_ar_acc:.4f}')
        
        # æ—©åœæª¢æŸ¥ - åŸºæ–¼IoUå’ŒARæº–ç¢ºç‡çš„ç¶œåˆæŒ‡æ¨™
        combined_score = avg_val_iou * 0.6 + val_ar_acc * 0.4
        best_combined_score = best_val_iou * 0.6 + best_val_ar_acc * 0.4
        
        if combined_score > best_combined_score:
            best_val_iou = avg_val_iou
            best_val_ar_acc = val_ar_acc
            best_model_state = model.state_dict().copy()
            patience_counter = 0
        else:
            patience_counter += 1
        
        if enable_earlystop and patience_counter >= patience:
            print(f'Early stopping at epoch {epoch+1}')
            break
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if best_model_state:
        model.load_state_dict(best_model_state)
        torch.save(best_model_state, 'joint_model_comprehensive_optimized.pth')
        print(f'âœ… æœ€ä½³ç¶œåˆå„ªåŒ–æ¨¡å‹å·²ä¿å­˜')
        print(f'ğŸ“Š æœ€ä½³IoU: {best_val_iou:.4f}, æœ€ä½³ARæº–ç¢ºç‡: {best_val_ar_acc:.4f}')
    
    return model

def main():
    parser = argparse.ArgumentParser(description='ç¶œåˆå„ªåŒ–çš„è¯åˆè¨“ç·´')
    parser.add_argument('--data', type=str, default='converted_dataset_fixed/data.yaml', help='æ•¸æ“šé›†é…ç½®æ–‡ä»¶')
    parser.add_argument('--epochs', type=int, default=50, help='è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--batch-size', type=int, default=16, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--device', type=str, default='auto', help='è¨­å‚™')
    parser.add_argument('--bbox-weight', type=float, default=15.0, help='æª¢æ¸¬æå¤±æ¬Šé‡')
    parser.add_argument('--cls-weight', type=float, default=10.0, help='åˆ†é¡æå¤±æ¬Šé‡')
    parser.add_argument('--enable-earlystop', action='store_true', help='å•Ÿç”¨æ—©åœ')
    
    args = parser.parse_args()
    
    # è¨­å‚™è¨­ç½®
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    print(f"ğŸš€ é–‹å§‹ç¶œåˆå„ªåŒ–è¯åˆè¨“ç·´...")
    print(f"ğŸ“Š è¨­å‚™: {device}")
    print(f"ğŸ“ˆ é…ç½®: epochs={args.epochs}, batch_size={args.batch_size}")
    print(f"âš–ï¸ æ¬Šé‡: bbox_weight={args.bbox_weight}, cls_weight={args.cls_weight}")
    
    # å‰µå»ºæ•¸æ“šé›†
    train_dataset = ComprehensiveOptimizedDataset(
        data_dir='converted_dataset_fixed',
        split='train',
        transform=get_comprehensive_optimized_transforms('train'),
        ar_oversample=True,
        detection_enhance=True
    )
    
    val_dataset = ComprehensiveOptimizedDataset(
        data_dir='converted_dataset_fixed',
        split='val',
        transform=get_comprehensive_optimized_transforms('val'),
        ar_oversample=False,
        detection_enhance=False
    )
    
    # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)
    
    # å‰µå»ºæ¨¡å‹
    model = ComprehensiveOptimizedJointModel(num_classes=4)
    
    # è¨“ç·´æ¨¡å‹
    trained_model = train_model_comprehensive_optimized(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=args.epochs,
        device=device,
        bbox_weight=args.bbox_weight,
        cls_weight=args.cls_weight,
        enable_earlystop=args.enable_earlystop
    )
    
    print("ğŸ‰ ç¶œåˆå„ªåŒ–è¯åˆè¨“ç·´å®Œæˆï¼")

if __name__ == "__main__":
    main() 