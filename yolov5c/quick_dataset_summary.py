#!/usr/bin/env python3
"""
Quick Dataset Summary
å¿«é€Ÿé¡¯ç¤ºæ•¸æ“šé›†æ‘˜è¦çµ±è¨ˆ
"""

import json
import os

def load_statistics(stats_file):
    """è¼‰å…¥çµ±è¨ˆæ•¸æ“š"""
    with open(stats_file, 'r', encoding='utf-8') as f:
        return json.load(f)

def print_summary(stats_data):
    """æ‰“å°æ‘˜è¦çµ±è¨ˆ"""
    print("=" * 80)
    print("ğŸ“Š æ•¸æ“šé›†çµ±è¨ˆæ‘˜è¦")
    print("=" * 80)
    
    # æª¢æ¸¬é¡åˆ¥çµ±è¨ˆ
    print("\nğŸ” æª¢æ¸¬é¡åˆ¥çµ±è¨ˆ (ç¸½è¨ˆ):")
    print("-" * 40)
    detection_stats = stats_data['detection_stats']['total']
    total_detections = sum(detection_stats.values())
    for class_name, count in detection_stats.items():
        percentage = (count / total_detections * 100) if total_detections > 0 else 0
        print(f"  {class_name:>3}: {count:>4} ({percentage:>5.1f}%)")
    
    # åˆ†é¡é¡åˆ¥çµ±è¨ˆ
    print("\nğŸ·ï¸ åˆ†é¡é¡åˆ¥çµ±è¨ˆ (ç¸½è¨ˆ):")
    print("-" * 40)
    classification_stats = stats_data['classification_stats']['total']
    total_classifications = sum(classification_stats.values())
    for class_name, count in classification_stats.items():
        percentage = (count / total_classifications * 100) if total_classifications > 0 else 0
        print(f"  {class_name:>4}: {count:>4} ({percentage:>5.1f}%)")
    
    # æ•¸æ“šé›†åˆ†å‰²çµ±è¨ˆ
    print("\nğŸ“ æ•¸æ“šé›†åˆ†å‰²çµ±è¨ˆ:")
    print("-" * 40)
    splits = ['train', 'val', 'test']
    for split in splits:
        train_count = len(stats_data['detection_stats'][split])
        val_count = len(stats_data['classification_stats'][split])
        print(f"  {split.upper():>5}: {train_count:>4} æ¨£æœ¬")
    
    # é—œéµç™¼ç¾
    print("\nğŸ” é—œéµç™¼ç¾:")
    print("-" * 40)
    
    # æª¢æ¸¬é¡åˆ¥ä¸å¹³è¡¡
    detection_counts = list(detection_stats.values())
    max_det = max(detection_counts)
    min_det = min(detection_counts)
    det_imbalance = max_det / min_det if min_det > 0 else 0
    print(f"  â€¢ æª¢æ¸¬é¡åˆ¥ä¸å¹³è¡¡æ¯”ä¾‹: {det_imbalance:.1f}:1")
    
    # åˆ†é¡é¡åˆ¥ä¸å¹³è¡¡
    classification_counts = list(classification_stats.values())
    max_cls = max(classification_counts)
    min_cls = min(classification_counts)
    cls_imbalance = max_cls / min_cls if min_cls > 0 else 0
    print(f"  â€¢ åˆ†é¡é¡åˆ¥ä¸å¹³è¡¡æ¯”ä¾‹: {cls_imbalance:.1f}:1")
    
    # æœ€å¸¸è¦‹çµ„åˆ
    combined_stats = stats_data['combined_stats']['total']
    if combined_stats:
        most_common = max(combined_stats.items(), key=lambda x: x[1])
        print(f"  â€¢ æœ€å¸¸è¦‹çµ„åˆ: {most_common[0]} ({most_common[1]} æ¨£æœ¬)")
    
    # æœ€å°‘è¦‹çµ„åˆ
    if combined_stats:
        least_common = min(combined_stats.items(), key=lambda x: x[1])
        print(f"  â€¢ æœ€å°‘è¦‹çµ„åˆ: {least_common[0]} ({least_common[1]} æ¨£æœ¬)")
    
    print("\n" + "=" * 80)

def main():
    stats_file = "dataset_statistics/dataset_statistics.json"
    
    if not os.path.exists(stats_file):
        print("âŒ çµ±è¨ˆæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè«‹å…ˆé‹è¡Œ analyze_dataset_statistics.py")
        return
    
    stats_data = load_statistics(stats_file)
    print_summary(stats_data)

if __name__ == "__main__":
    main() 