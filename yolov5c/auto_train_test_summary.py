#!/usr/bin/env python3
"""
Auto Train Test Summary
è‡ªå‹•è¨“ç·´ã€æ¸¬è©¦ä¸¦ç”Ÿæˆå®Œæ•´æ‘˜è¦å ±å‘Š
"""

import os
import sys
import subprocess
import json
import time
import argparse
import logging
from datetime import datetime
from pathlib import Path

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AutoTrainTestSummary:
    def __init__(self, data_yaml_path, epochs=20, batch_size=16):
        """åˆå§‹åŒ–è‡ªå‹•è¨“ç·´æ¸¬è©¦ç³»çµ±"""
        self.data_yaml_path = data_yaml_path
        self.epochs = epochs
        self.batch_size = batch_size
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = f"auto_train_test_{self.timestamp}"
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è¨­ç½®æ—¥èªŒæ–‡ä»¶
        self.log_file = os.path.join(self.output_dir, "auto_train_test.log")
        self.setup_file_logging()
        
    def setup_file_logging(self):
        """è¨­ç½®æ–‡ä»¶æ—¥èªŒ"""
        file_handler = logging.FileHandler(self.log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    def run_command(self, cmd, description):
        """åŸ·è¡Œå‘½ä»¤ä¸¦è¨˜éŒ„çµæžœ"""
        logger.info(f"ðŸš€ {description}")
        logger.info(f"ðŸ“ åŸ·è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8')
            
            if result.returncode == 0:
                logger.info(f"âœ… {description} æˆåŠŸå®Œæˆ")
                if result.stdout:
                    logger.info(f"ðŸ“¤ è¼¸å‡º: {result.stdout[:500]}...")
            else:
                logger.error(f"âŒ {description} å¤±æ•—")
                logger.error(f"ðŸ“¤ éŒ¯èª¤è¼¸å‡º: {result.stderr}")
                return False
                
            return True
            
        except Exception as e:
            logger.error(f"âŒ {description} åŸ·è¡Œç•°å¸¸: {e}")
            return False
    
    def step1_dataset_analysis(self):
        """æ­¥é©Ÿ1: æ•¸æ“šé›†åˆ†æž"""
        logger.info("=" * 60)
        logger.info("ðŸ“Š æ­¥é©Ÿ1: æ•¸æ“šé›†çµ±è¨ˆåˆ†æž")
        logger.info("=" * 60)
        
        cmd = [
            "python3", "yolov5c/analyze_dataset_statistics.py",
            "--data", self.data_yaml_path,
            "--output", os.path.join(self.output_dir, "dataset_analysis")
        ]
        
        return self.run_command(cmd, "æ•¸æ“šé›†çµ±è¨ˆåˆ†æž")
    
    def step2_training(self):
        """æ­¥é©Ÿ2: è¨“ç·´æ¨¡åž‹"""
        logger.info("=" * 60)
        logger.info("ðŸ‹ï¸ æ­¥é©Ÿ2: æ¨¡åž‹è¨“ç·´")
        logger.info("=" * 60)
        
        # ä½¿ç”¨ä¿®å¾©ç‰ˆè¨“ç·´è…³æœ¬
        cmd = [
            "python3", "yolov5c/train_joint_fixed.py",
            "--data", self.data_yaml_path,
            "--epochs", str(self.epochs),
            "--batch-size", str(self.batch_size),
            "--device", "auto",
            "--log", os.path.join(self.output_dir, "training.log")
        ]
        
        return self.run_command(cmd, "è¯åˆè¨“ç·´")
    
    def step3_validation(self):
        """æ­¥é©Ÿ3: æ¨¡åž‹é©—è­‰"""
        logger.info("=" * 60)
        logger.info("ðŸ” æ­¥é©Ÿ3: æ¨¡åž‹é©—è­‰")
        logger.info("=" * 60)
        
        # æŸ¥æ‰¾æœ€æ–°çš„æ¨¡åž‹æ–‡ä»¶
        model_files = [
            "joint_model_fixed.pth",
            "best_simple_joint_model.pt",
            "joint_model_advanced.pth"
        ]
        
        model_path = None
        for model_file in model_files:
            if os.path.exists(model_file):
                model_path = model_file
                break
        
        if not model_path:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°è¨“ç·´å¥½çš„æ¨¡åž‹æ–‡ä»¶")
            return False
        
        # é‹è¡Œçµ±ä¸€é©—è­‰
        cmd = [
            "python3", "yolov5c/unified_validation.py",
            "--model", model_path,
            "--data", self.data_yaml_path,
            "--output", os.path.join(self.output_dir, "validation_results")
        ]
        
        return self.run_command(cmd, "æ¨¡åž‹é©—è­‰")
    
    def step4_performance_analysis(self):
        """æ­¥é©Ÿ4: æ€§èƒ½åˆ†æž"""
        logger.info("=" * 60)
        logger.info("ðŸ“ˆ æ­¥é©Ÿ4: æ€§èƒ½åˆ†æž")
        logger.info("=" * 60)
        
        # åˆ†æžè¨“ç·´æ­·å²
        history_files = [
            "training_history_fixed.json",
            "training_history_advanced.json"
        ]
        
        for history_file in history_files:
            if os.path.exists(history_file):
                cmd = [
                    "python3", "yolov5c/analyze_advanced_training.py",
                    "--history", history_file,
                    "--output", os.path.join(self.output_dir, f"analysis_{history_file.replace('.json', '')}")
                ]
                self.run_command(cmd, f"è¨“ç·´æ­·å²åˆ†æž ({history_file})")
        
        return True
    
    def step5_generate_summary_report(self):
        """æ­¥é©Ÿ5: ç”Ÿæˆæ‘˜è¦å ±å‘Š"""
        logger.info("=" * 60)
        logger.info("ðŸ“‹ æ­¥é©Ÿ5: ç”Ÿæˆæ‘˜è¦å ±å‘Š")
        logger.info("=" * 60)
        
        report_path = os.path.join(self.output_dir, "summary_report.md")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(self.generate_summary_content())
        
        logger.info(f"âœ… æ‘˜è¦å ±å‘Šå·²ç”Ÿæˆ: {report_path}")
        return True
    
    def generate_summary_content(self):
        """ç”Ÿæˆæ‘˜è¦å ±å‘Šå…§å®¹"""
        content = []
        content.append("# è‡ªå‹•è¨“ç·´æ¸¬è©¦æ‘˜è¦å ±å‘Š")
        content.append("=" * 60)
        content.append(f"**ç”Ÿæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        content.append(f"**æ•¸æ“šé…ç½®**: {self.data_yaml_path}")
        content.append(f"**è¨“ç·´è¼ªæ•¸**: {self.epochs}")
        content.append(f"**æ‰¹æ¬¡å¤§å°**: {self.batch_size}")
        content.append("")
        
        # æ•¸æ“šé›†çµ±è¨ˆæ‘˜è¦
        content.append("## ðŸ“Š æ•¸æ“šé›†çµ±è¨ˆæ‘˜è¦")
        content.append("-" * 40)
        
        dataset_stats_file = os.path.join(self.output_dir, "dataset_analysis", "dataset_statistics.json")
        if os.path.exists(dataset_stats_file):
            try:
                with open(dataset_stats_file, 'r', encoding='utf-8') as f:
                    stats = json.load(f)
                
                # æª¢æ¸¬é¡žåˆ¥çµ±è¨ˆ
                detection_stats = stats['detection_stats']['total']
                total_detections = sum(detection_stats.values())
                content.append("### æª¢æ¸¬é¡žåˆ¥çµ±è¨ˆ")
                for class_name, count in detection_stats.items():
                    percentage = (count / total_detections * 100) if total_detections > 0 else 0
                    content.append(f"- {class_name}: {count} ({percentage:.1f}%)")
                
                # åˆ†é¡žé¡žåˆ¥çµ±è¨ˆ
                classification_stats = stats['classification_stats']['total']
                total_classifications = sum(classification_stats.values())
                content.append("\n### åˆ†é¡žé¡žåˆ¥çµ±è¨ˆ")
                for class_name, count in classification_stats.items():
                    percentage = (count / total_classifications * 100) if total_classifications > 0 else 0
                    content.append(f"- {class_name}: {count} ({percentage:.1f}%)")
                    
            except Exception as e:
                content.append(f"âš ï¸ ç„¡æ³•è¼‰å…¥æ•¸æ“šé›†çµ±è¨ˆ: {e}")
        
        # è¨“ç·´çµæžœæ‘˜è¦
        content.append("\n## ðŸ‹ï¸ è¨“ç·´çµæžœæ‘˜è¦")
        content.append("-" * 40)
        
        # æª¢æŸ¥è¨“ç·´æ—¥èªŒ
        training_log = os.path.join(self.output_dir, "training.log")
        if os.path.exists(training_log):
            try:
                with open(training_log, 'r', encoding='utf-8') as f:
                    log_lines = f.readlines()
                
                # æå–æœ€å¾Œçš„è¨“ç·´çµæžœ
                final_results = []
                for line in reversed(log_lines[-50:]):  # æª¢æŸ¥æœ€å¾Œ50è¡Œ
                    if "Best Val Cls Accuracy" in line or "mAP" in line or "Precision" in line or "Recall" in line:
                        final_results.append(line.strip())
                        if len(final_results) >= 5:  # æœ€å¤šé¡¯ç¤º5è¡Œ
                            break
                
                if final_results:
                    content.append("### æœ€çµ‚è¨“ç·´æŒ‡æ¨™")
                    for result in reversed(final_results):
                        content.append(f"- {result}")
                else:
                    content.append("âš ï¸ æœªæ‰¾åˆ°æœ€çµ‚è¨“ç·´æŒ‡æ¨™")
                    
            except Exception as e:
                content.append(f"âš ï¸ ç„¡æ³•è®€å–è¨“ç·´æ—¥èªŒ: {e}")
        
        # é©—è­‰çµæžœæ‘˜è¦
        content.append("\n## ðŸ” é©—è­‰çµæžœæ‘˜è¦")
        content.append("-" * 40)
        
        validation_dir = os.path.join(self.output_dir, "validation_results")
        if os.path.exists(validation_dir):
            # æŸ¥æ‰¾é©—è­‰å ±å‘Š
            for file in os.listdir(validation_dir):
                if file.endswith('.md') and 'report' in file.lower():
                    content.append(f"ðŸ“„ è©³ç´°é©—è­‰å ±å‘Š: `{validation_dir}/{file}`")
                    break
        
        # æ€§èƒ½åˆ†æžæ‘˜è¦
        content.append("\n## ðŸ“ˆ æ€§èƒ½åˆ†æžæ‘˜è¦")
        content.append("-" * 40)
        
        analysis_dirs = [d for d in os.listdir(self.output_dir) if d.startswith('analysis_')]
        for analysis_dir in analysis_dirs:
            analysis_path = os.path.join(self.output_dir, analysis_dir)
            report_files = [f for f in os.listdir(analysis_path) if f.endswith('.md')]
            for report_file in report_files:
                content.append(f"ðŸ“Š æ€§èƒ½åˆ†æžå ±å‘Š: `{analysis_path}/{report_file}`")
        
        # æ–‡ä»¶åˆ—è¡¨
        content.append("\n## ðŸ“ ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨")
        content.append("-" * 40)
        
        def list_files_recursive(directory, prefix=""):
            files = []
            for item in os.listdir(directory):
                item_path = os.path.join(directory, item)
                if os.path.isfile(item_path):
                    files.append(f"{prefix}ðŸ“„ {item}")
                elif os.path.isdir(item_path):
                    files.append(f"{prefix}ðŸ“ {item}/")
                    files.extend(list_files_recursive(item_path, prefix + "  "))
            return files
        
        files = list_files_recursive(self.output_dir)
        for file in files:
            content.append(file)
        
        # ç¸½çµ
        content.append("\n## ðŸŽ¯ ç¸½çµ")
        content.append("-" * 40)
        content.append("âœ… è‡ªå‹•è¨“ç·´æ¸¬è©¦æµç¨‹å·²å®Œæˆ")
        content.append(f"ðŸ“ æ‰€æœ‰çµæžœä¿å­˜åœ¨: `{self.output_dir}/`")
        content.append("ðŸ“‹ è©³ç´°å ±å‘Šè«‹æŸ¥çœ‹ä¸Šè¿°æ–‡ä»¶")
        
        return "\n".join(content)
    
    def run_full_pipeline(self):
        """é‹è¡Œå®Œæ•´æµç¨‹"""
        logger.info("ðŸš€ é–‹å§‹è‡ªå‹•è¨“ç·´æ¸¬è©¦æµç¨‹")
        logger.info(f"ðŸ“ è¼¸å‡ºç›®éŒ„: {self.output_dir}")
        
        start_time = time.time()
        
        # åŸ·è¡Œå„å€‹æ­¥é©Ÿ
        steps = [
            ("æ•¸æ“šé›†åˆ†æž", self.step1_dataset_analysis),
            ("æ¨¡åž‹è¨“ç·´", self.step2_training),
            ("æ¨¡åž‹é©—è­‰", self.step3_validation),
            ("æ€§èƒ½åˆ†æž", self.step4_performance_analysis),
            ("ç”Ÿæˆæ‘˜è¦", self.step5_generate_summary_report)
        ]
        
        results = {}
        for step_name, step_func in steps:
            logger.info(f"\n{'='*20} {step_name} {'='*20}")
            success = step_func()
            results[step_name] = success
            
            if not success:
                logger.warning(f"âš ï¸ {step_name} æ­¥é©Ÿå¤±æ•—ï¼Œä½†ç¹¼çºŒåŸ·è¡Œå¾ŒçºŒæ­¥é©Ÿ")
        
        # è¨ˆç®—ç¸½æ™‚é–“
        total_time = time.time() - start_time
        hours = int(total_time // 3600)
        minutes = int((total_time % 3600) // 60)
        seconds = int(total_time % 60)
        
        # ç”Ÿæˆæœ€çµ‚æ‘˜è¦
        logger.info("\n" + "="*60)
        logger.info("ðŸŽ‰ è‡ªå‹•è¨“ç·´æ¸¬è©¦æµç¨‹å®Œæˆ")
        logger.info("="*60)
        logger.info(f"â±ï¸ ç¸½è€—æ™‚: {hours:02d}:{minutes:02d}:{seconds:02d}")
        logger.info(f"ðŸ“ çµæžœç›®éŒ„: {self.output_dir}")
        
        # é¡¯ç¤ºæ­¥é©Ÿçµæžœ
        logger.info("\nðŸ“‹ æ­¥é©ŸåŸ·è¡Œçµæžœ:")
        for step_name, success in results.items():
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±æ•—"
            logger.info(f"  {step_name}: {status}")
        
        # é¡¯ç¤ºæ‘˜è¦å ±å‘Šä½ç½®
        summary_report = os.path.join(self.output_dir, "summary_report.md")
        if os.path.exists(summary_report):
            logger.info(f"\nðŸ“„ æ‘˜è¦å ±å‘Š: {summary_report}")
        
        return results

def main():
    parser = argparse.ArgumentParser(description='è‡ªå‹•è¨“ç·´æ¸¬è©¦ä¸¦ç”Ÿæˆæ‘˜è¦å ±å‘Š')
    parser.add_argument('--data', type=str, default='converted_dataset_fixed/data.yaml',
                       help='æ•¸æ“šé…ç½®æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--epochs', type=int, default=20,
                       help='è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--batch-size', type=int, default=16,
                       help='æ‰¹æ¬¡å¤§å°')
    
    args = parser.parse_args()
    
    # æª¢æŸ¥æ•¸æ“šé…ç½®æ–‡ä»¶
    if not os.path.exists(args.data):
        logger.error(f"âŒ æ•¸æ“šé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.data}")
        return
    
    # å‰µå»ºè‡ªå‹•è¨“ç·´æ¸¬è©¦ç³»çµ±
    auto_system = AutoTrainTestSummary(args.data, args.epochs, args.batch_size)
    
    # é‹è¡Œå®Œæ•´æµç¨‹
    results = auto_system.run_full_pipeline()
    
    # é¡¯ç¤ºæ‘˜è¦å ±å‘Š
    summary_report = os.path.join(auto_system.output_dir, "summary_report.md")
    if os.path.exists(summary_report):
        print("\n" + "="*80)
        print("ðŸ“‹ æ‘˜è¦å ±å‘Šé è¦½")
        print("="*80)
        
        with open(summary_report, 'r', encoding='utf-8') as f:
            content = f.read()
            # é¡¯ç¤ºå‰30è¡Œ
            lines = content.split('\n')[:30]
            print('\n'.join(lines))
            if len(content.split('\n')) > 30:
                print("...")
                print(f"ðŸ“„ å®Œæ•´å ±å‘Šè«‹æŸ¥çœ‹: {summary_report}")

if __name__ == "__main__":
    main() 