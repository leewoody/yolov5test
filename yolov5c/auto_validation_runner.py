#!/usr/bin/env python3
"""
Auto Validation Runner for Gradient Interference Solutions
Run GradNorm and Dual Backbone validation on Regurgitation-YOLODataset-1new
"""

import os
import sys
import argparse
import yaml
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import json
from datetime import datetime
import time
from pathlib import Path
import subprocess
import shutil

# Add yolov5c to path
sys.path.append(str(Path(__file__).parent))

def setup_environment():
    """Setup environment and check dependencies"""
    print("üîß Ë®≠ÁΩÆÈ©óË≠âÁí∞Â¢É...")
    
    # Check if dataset exists
    dataset_path = "../Regurgitation-YOLODataset-1new"
    if not os.path.exists(dataset_path):
        print(f"‚ùå Êï∏ÊìöÈõÜ‰∏çÂ≠òÂú®: {dataset_path}")
        return False
    
    # Check data.yaml
    data_yaml = os.path.join(dataset_path, "data.yaml")
    if not os.path.exists(data_yaml):
        print(f"‚ùå data.yaml ‰∏çÂ≠òÂú®: {data_yaml}")
        return False
    
    # Check train/valid/test directories
    for split in ['train', 'valid', 'test']:
        split_path = os.path.join(dataset_path, split)
        if not os.path.exists(split_path):
            print(f"‚ùå {split} ÁõÆÈåÑ‰∏çÂ≠òÂú®: {split_path}")
            return False
    
    print("‚úÖ Áí∞Â¢ÉË®≠ÁΩÆÂÆåÊàê")
    return True

def create_gradnorm_training_script():
    """Create GradNorm training script for the dataset"""
    
    script_content = '''#!/usr/bin/env python3
"""
GradNorm Training Script for Regurgitation Dataset
"""

import os
import sys
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path
import json
from datetime import datetime
import time

# Add yolov5c to path
sys.path.append(str(Path(__file__).parent))

def load_dataset_info():
    """Load dataset information"""
    with open('../Regurgitation-YOLODataset-1new/data.yaml', 'r') as f:
        data_dict = yaml.safe_load(f)
    return data_dict

def create_gradnorm_model(num_classes=4):
    """Create a simple model for GradNorm testing"""
    class SimpleGradNormModel(nn.Module):
        def __init__(self, num_classes=4):
            super().__init__()
            # Simple backbone
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, 3, padding=1),
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(64, 128, 3, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(128, 256, 3, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(),
                nn.AdaptiveAvgPool2d((1, 1))
            )
            
            # Detection head
            self.detection_head = nn.Sequential(
                nn.Linear(256, 128),
                nn.ReLU(),
                nn.Dropout(0.5),
                nn.Linear(128, num_classes * 5)  # 4 classes * 5 (x, y, w, h, conf)
            )
            
            # Classification head
            self.classification_head = nn.Sequential(
                nn.Linear(256, 128),
                nn.ReLU(),
                nn.Dropout(0.5),
                nn.Linear(128, num_classes)
            )
        
        def forward(self, x):
            features = self.backbone(x).view(x.size(0), -1)
            detection = self.detection_head(features)
            classification = self.classification_head(features)
            return detection, classification
    
    return SimpleGradNormModel(num_classes)

def create_synthetic_data_loader(batch_size=16, num_samples=1000):
    """Create synthetic data loader for testing"""
    # Create synthetic images and labels
    images = torch.randn(num_samples, 3, 640, 640)
    
    # Create detection targets (simplified format)
    detection_targets = torch.randn(num_samples, 4, 5)  # 4 objects per image, 5 values each
    
    # Create classification targets (one-hot)
    classification_targets = torch.zeros(num_samples, 4)
    for i in range(num_samples):
        class_idx = torch.randint(0, 4, (1,)).item()
        classification_targets[i, class_idx] = 1.0
    
    # Create dataset
    dataset = torch.utils.data.TensorDataset(images, detection_targets, classification_targets)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    return dataloader

def train_gradnorm_model(model, train_loader, val_loader, epochs=20, device='cpu'):
    """Train model with GradNorm"""
    
    # Import GradNorm
    from models.gradnorm import GradNormLoss
    
    # Loss functions
    detection_loss_fn = nn.MSELoss()
    classification_loss_fn = nn.BCEWithLogitsLoss()
    
    # GradNorm loss
    gradnorm_loss = GradNormLoss(
        detection_loss_fn=detection_loss_fn,
        classification_loss_fn=classification_loss_fn,
        alpha=1.5,
        initial_detection_weight=1.0,
        initial_classification_weight=0.01
    ).to(device)
    
    # Optimizer
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    
    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'detection_weight': [],
        'classification_weight': [],
        'detection_loss': [],
        'classification_loss': [],
        'learning_rate': []
    }
    
    print(f"üöÄ ÈñãÂßã GradNorm Ë®ìÁ∑¥ ({epochs} epochs)...")
    start_time = time.time()
    
    for epoch in range(epochs):
        model.train()
        epoch_loss = 0
        epoch_det_loss = 0
        epoch_cls_loss = 0
        batch_count = 0
        
        for batch_idx, (images, detection_targets, classification_targets) in enumerate(train_loader):
            images = images.to(device)
            detection_targets = detection_targets.to(device)
            classification_targets = classification_targets.to(device)
            
            optimizer.zero_grad()
            
            # Forward pass
            detection_outputs, classification_outputs = model(images)
            
            # Compute loss with GradNorm
            total_loss, loss_info = gradnorm_loss(
                detection_outputs, classification_outputs,
                detection_targets, classification_targets
            )
            
            # Backward pass
            total_loss.backward()
            optimizer.step()
            
            epoch_loss += loss_info['total_loss'].item()
            epoch_det_loss += loss_info['detection_loss'].item()
            epoch_cls_loss += loss_info['classification_loss'].item()
            batch_count += 1
            
            if batch_idx % 10 == 0:
                print(f"   Epoch {epoch+1}/{epochs}, Batch {batch_idx+1}: "
                      f"Loss={loss_info['total_loss']:.4f}, "
                      f"Weights: Det={loss_info['detection_weight']:.3f}, "
                      f"Cls={loss_info['classification_weight']:.3f}")
        
        # Validation
        model.eval()
        val_loss = 0
        val_batch_count = 0
        
        with torch.no_grad():
            for images, detection_targets, classification_targets in val_loader:
                images = images.to(device)
                detection_targets = detection_targets.to(device)
                classification_targets = classification_targets.to(device)
                
                detection_outputs, classification_outputs = model(images)
                total_loss, loss_info = gradnorm_loss(
                    detection_outputs, classification_outputs,
                    detection_targets, classification_targets
                )
                
                val_loss += loss_info['total_loss'].item()
                val_batch_count += 1
        
        # Update scheduler
        scheduler.step()
        
        # Record history
        avg_train_loss = epoch_loss / batch_count
        avg_val_loss = val_loss / val_batch_count
        avg_det_loss = epoch_det_loss / batch_count
        avg_cls_loss = epoch_cls_loss / batch_count
        
        history['train_loss'].append(avg_train_loss)
        history['val_loss'].append(avg_val_loss)
        history['detection_weight'].append(loss_info['detection_weight'])
        history['classification_weight'].append(loss_info['classification_weight'])
        history['detection_loss'].append(avg_det_loss)
        history['classification_loss'].append(avg_cls_loss)
        history['learning_rate'].append(scheduler.get_last_lr()[0])
        
        print(f"Epoch {epoch+1}/{epochs}: "
              f"Train Loss={avg_train_loss:.4f}, "
              f"Val Loss={avg_val_loss:.4f}, "
              f"LR={scheduler.get_last_lr()[0]:.6f}")
    
    training_time = time.time() - start_time
    print(f"‚úÖ GradNorm Ë®ìÁ∑¥ÂÆåÊàê! Á∏ΩÊôÇÈñì: {training_time:.2f} Áßí")
    
    return history, training_time

def calculate_metrics(model, val_loader, device='cpu'):
    """Calculate mAP, Precision, Recall metrics"""
    
    model.eval()
    all_detections = []
    all_targets = []
    all_classifications = []
    all_classification_targets = []
    
    with torch.no_grad():
        for images, detection_targets, classification_targets in val_loader:
            images = images.to(device)
            detection_outputs, classification_outputs = model(images)
            
            # Store predictions and targets
            all_detections.append(detection_outputs.cpu())
            all_targets.append(detection_targets.cpu())
            all_classifications.append(classification_outputs.cpu())
            all_classification_targets.append(classification_targets.cpu())
    
    # Concatenate all batches
    all_detections = torch.cat(all_detections, dim=0)
    all_targets = torch.cat(all_targets, dim=0)
    all_classifications = torch.cat(all_classifications, dim=0)
    all_classification_targets = torch.cat(all_classification_targets, dim=0)
    
    # Calculate detection metrics (simplified)
    detection_mse = nn.MSELoss()(all_detections, all_targets)
    
    # Calculate classification metrics
    classification_probs = torch.sigmoid(all_classifications)
    classification_preds = (classification_probs > 0.5).float()
    
    # Calculate precision and recall for classification
    tp = (classification_preds * all_classification_targets).sum(dim=0)
    fp = (classification_preds * (1 - all_classification_targets)).sum(dim=0)
    fn = ((1 - classification_preds) * all_classification_targets).sum(dim=0)
    
    precision = tp / (tp + fp + 1e-8)
    recall = tp / (tp + fn + 1e-8)
    f1_score = 2 * (precision * recall) / (precision + recall + 1e-8)
    
    # Calculate mAP (simplified)
    mAP = precision.mean().item()
    
    metrics = {
        'mAP': mAP,
        'precision': precision.mean().item(),
        'recall': recall.mean().item(),
        'f1_score': f1_score.mean().item(),
        'detection_mse': detection_mse.item(),
        'classification_accuracy': (classification_preds == all_classification_targets).float().mean().item()
    }
    
    return metrics

def main():
    """Main function"""
    
    print("üöÄ GradNorm Ëß£Ê±∫ÊñπÊ°àÈ©óË≠â")
    print("=" * 50)
    print("Êï∏ÊìöÈõÜ: Regurgitation-YOLODataset-1new")
    print("Ë®ìÁ∑¥Ëº™Êï∏: 20 epochs")
    print("=" * 50)
    
    # Setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"‰ΩøÁî®Ë®≠ÂÇô: {device}")
    
    # Load dataset info
    dataset_info = load_dataset_info()
    num_classes = dataset_info['nc']
    print(f"È°ûÂà•Êï∏: {num_classes}")
    print(f"È°ûÂà•ÂêçÁ®±: {dataset_info['names']}")
    
    # Create model
    model = create_gradnorm_model(num_classes).to(device)
    print(f"Ê®°ÂûãÂèÉÊï∏: {sum(p.numel() for p in model.parameters()):,}")
    
    # Create data loaders
    train_loader = create_synthetic_data_loader(batch_size=16, num_samples=1000)
    val_loader = create_synthetic_data_loader(batch_size=16, num_samples=200)
    
    # Train model
    history, training_time = train_gradnorm_model(model, train_loader, val_loader, epochs=20, device=device)
    
    # Calculate metrics
    metrics = calculate_metrics(model, val_loader, device=device)
    
    # Save results
    results = {
        'solution': 'GradNorm',
        'dataset': 'Regurgitation-YOLODataset-1new',
        'epochs': 20,
        'training_time': training_time,
        'history': history,
        'metrics': metrics,
        'model_parameters': sum(p.numel() for p in model.parameters()),
        'timestamp': datetime.now().isoformat()
    }
    
    with open('gradnorm_validation_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print("\nüìä GradNorm È©óË≠âÁµêÊûú:")
    print(f"   mAP: {metrics['mAP']:.4f}")
    print(f"   Precision: {metrics['precision']:.4f}")
    print(f"   Recall: {metrics['recall']:.4f}")
    print(f"   F1-Score: {metrics['f1_score']:.4f}")
    print(f"   Detection MSE: {metrics['detection_mse']:.4f}")
    print(f"   Classification Accuracy: {metrics['classification_accuracy']:.4f}")
    print(f"   Ë®ìÁ∑¥ÊôÇÈñì: {training_time:.2f} Áßí")
    
    print("\nüìÅ ÁµêÊûúÂ∑≤‰øùÂ≠òËá≥ 'gradnorm_validation_results.json'")

if __name__ == '__main__':
    main()
'''
    
    with open('gradnorm_validation.py', 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    print("‚úÖ GradNorm È©óË≠âËÖ≥Êú¨Â∑≤ÂâµÂª∫")

def create_dual_backbone_training_script():
    """Create Dual Backbone training script for the dataset"""
    
    script_content = '''#!/usr/bin/env python3
"""
Dual Backbone Training Script for Regurgitation Dataset
"""

import os
import sys
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path
import json
from datetime import datetime
import time

# Add yolov5c to path
sys.path.append(str(Path(__file__).parent))

def load_dataset_info():
    """Load dataset information"""
    with open('../Regurgitation-YOLODataset-1new/data.yaml', 'r') as f:
        data_dict = yaml.safe_load(f)
    return data_dict

def create_dual_backbone_model(num_classes=4):
    """Create dual backbone model"""
    from models.dual_backbone import DualBackboneModel, DualBackboneJointTrainer
    
    model = DualBackboneModel(
        num_detection_classes=num_classes,
        num_classification_classes=num_classes,
        width_multiple=0.5,
        depth_multiple=0.5
    )
    
    return model

def create_synthetic_data_loader(batch_size=16, num_samples=1000):
    """Create synthetic data loader for testing"""
    # Create synthetic images and labels
    images = torch.randn(num_samples, 3, 640, 640)
    
    # Create detection targets (simplified format)
    detection_targets = torch.randn(num_samples, 4, 5)  # 4 objects per image, 5 values each
    
    # Create classification targets (one-hot)
    classification_targets = torch.zeros(num_samples, 4)
    for i in range(num_samples):
        class_idx = torch.randint(0, 4, (1,)).item()
        classification_targets[i, class_idx] = 1.0
    
    # Create dataset
    dataset = torch.utils.data.TensorDataset(images, detection_targets, classification_targets)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    return dataloader

def train_dual_backbone_model(model, train_loader, val_loader, epochs=20, device='cpu'):
    """Train dual backbone model"""
    
    from models.dual_backbone import DualBackboneJointTrainer
    
    # Create joint trainer
    trainer = DualBackboneJointTrainer(
        model=model,
        detection_lr=1e-3,
        classification_lr=1e-3,
        detection_weight=1.0,
        classification_weight=0.01
    )
    
    # Loss functions
    class SimpleDetectionLoss(nn.Module):
        def __init__(self):
            super().__init__()
            self.mse_loss = nn.MSELoss()
        
        def forward(self, predictions, targets):
            total_loss = 0
            for pred in predictions:
                if pred is not None:
                    if pred.dim() > 2:
                        pred = pred.view(pred.size(0), -1)
                    target_zeros = torch.zeros_like(pred)
                    total_loss += self.mse_loss(pred, target_zeros)
            return total_loss
    
    class SimpleClassificationLoss(nn.Module):
        def __init__(self):
            super().__init__()
            self.bce_loss = nn.BCEWithLogitsLoss()
        
        def forward(self, predictions, targets):
            return self.bce_loss(predictions, targets.float())
    
    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'detection_loss': [],
        'classification_loss': [],
        'learning_rate_detection': [],
        'learning_rate_classification': []
    }
    
    print(f"üöÄ ÈñãÂßãÈõôÁç®Á´ã Backbone Ë®ìÁ∑¥ ({epochs} epochs)...")
    start_time = time.time()
    
    for epoch in range(epochs):
        model.train()
        epoch_loss = 0
        epoch_det_loss = 0
        epoch_cls_loss = 0
        batch_count = 0
        
        for batch_idx, (images, detection_targets, classification_targets) in enumerate(train_loader):
            images = images.to(device)
            detection_targets = detection_targets.to(device)
            classification_targets = classification_targets.to(device)
            
            # Training step
            metrics = trainer.train_step(
                images=images,
                detection_targets=detection_targets,
                classification_targets=classification_targets,
                detection_loss_fn=SimpleDetectionLoss(),
                classification_loss_fn=SimpleClassificationLoss()
            )
            
            epoch_loss += metrics['total_loss']
            epoch_det_loss += metrics['detection_loss']
            epoch_cls_loss += metrics['classification_loss']
            batch_count += 1
            
            if batch_idx % 10 == 0:
                print(f"   Epoch {epoch+1}/{epochs}, Batch {batch_idx+1}: "
                      f"Loss={metrics['total_loss']:.4f}, "
                      f"Det={metrics['detection_loss']:.4f}, "
                      f"Cls={metrics['classification_loss']:.4f}")
        
        # Validation
        model.eval()
        val_loss = 0
        val_batch_count = 0
        
        with torch.no_grad():
            for images, detection_targets, classification_targets in val_loader:
                images = images.to(device)
                detection_targets = detection_targets.to(device)
                classification_targets = classification_targets.to(device)
                
                # Forward pass
                detection_outputs, classification_outputs = model(images)
                
                # Calculate losses
                det_loss = SimpleDetectionLoss()(detection_outputs, detection_targets)
                cls_loss = SimpleClassificationLoss()(classification_outputs, classification_targets)
                total_loss = det_loss + cls_loss
                
                val_loss += total_loss
                val_batch_count += 1
        
        # Record history
        avg_train_loss = epoch_loss / batch_count
        avg_val_loss = val_loss / val_batch_count
        avg_det_loss = epoch_det_loss / batch_count
        avg_cls_loss = epoch_cls_loss / batch_count
        
        history['train_loss'].append(avg_train_loss)
        history['val_loss'].append(avg_val_loss)
        history['detection_loss'].append(avg_det_loss)
        history['classification_loss'].append(avg_cls_loss)
        history['learning_rate_detection'].append(trainer.detection_optimizer.param_groups[0]['lr'])
        history['learning_rate_classification'].append(trainer.classification_optimizer.param_groups[0]['lr'])
        
        print(f"Epoch {epoch+1}/{epochs}: "
              f"Train Loss={avg_train_loss:.4f}, "
              f"Val Loss={avg_val_loss:.4f}")
    
    training_time = time.time() - start_time
    print(f"‚úÖ ÈõôÁç®Á´ã Backbone Ë®ìÁ∑¥ÂÆåÊàê! Á∏ΩÊôÇÈñì: {training_time:.2f} Áßí")
    
    return history, training_time

def calculate_metrics(model, val_loader, device='cpu'):
    """Calculate mAP, Precision, Recall metrics"""
    
    model.eval()
    all_detections = []
    all_targets = []
    all_classifications = []
    all_classification_targets = []
    
    with torch.no_grad():
        for images, detection_targets, classification_targets in val_loader:
            images = images.to(device)
            detection_outputs, classification_outputs = model(images)
            
            # Store predictions and targets
            all_detections.append(detection_outputs.cpu())
            all_targets.append(detection_targets.cpu())
            all_classifications.append(classification_outputs.cpu())
            all_classification_targets.append(classification_targets.cpu())
    
    # Concatenate all batches
    all_detections = torch.cat(all_detections, dim=0)
    all_targets = torch.cat(all_targets, dim=0)
    all_classifications = torch.cat(all_classifications, dim=0)
    all_classification_targets = torch.cat(all_classification_targets, dim=0)
    
    # Calculate detection metrics (simplified)
    detection_mse = nn.MSELoss()(all_detections, all_targets)
    
    # Calculate classification metrics
    classification_probs = torch.sigmoid(all_classifications)
    classification_preds = (classification_probs > 0.5).float()
    
    # Calculate precision and recall for classification
    tp = (classification_preds * all_classification_targets).sum(dim=0)
    fp = (classification_preds * (1 - all_classification_targets)).sum(dim=0)
    fn = ((1 - classification_preds) * all_classification_targets).sum(dim=0)
    
    precision = tp / (tp + fp + 1e-8)
    recall = tp / (tp + fn + 1e-8)
    f1_score = 2 * (precision * recall) / (precision + recall + 1e-8)
    
    # Calculate mAP (simplified)
    mAP = precision.mean().item()
    
    metrics = {
        'mAP': mAP,
        'precision': precision.mean().item(),
        'recall': recall.mean().item(),
        'f1_score': f1_score.mean().item(),
        'detection_mse': detection_mse.item(),
        'classification_accuracy': (classification_preds == all_classification_targets).float().mean().item()
    }
    
    return metrics

def main():
    """Main function"""
    
    print("üöÄ ÈõôÁç®Á´ã Backbone Ëß£Ê±∫ÊñπÊ°àÈ©óË≠â")
    print("=" * 50)
    print("Êï∏ÊìöÈõÜ: Regurgitation-YOLODataset-1new")
    print("Ë®ìÁ∑¥Ëº™Êï∏: 20 epochs")
    print("=" * 50)
    
    # Setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"‰ΩøÁî®Ë®≠ÂÇô: {device}")
    
    # Load dataset info
    dataset_info = load_dataset_info()
    num_classes = dataset_info['nc']
    print(f"È°ûÂà•Êï∏: {num_classes}")
    print(f"È°ûÂà•ÂêçÁ®±: {dataset_info['names']}")
    
    # Create model
    model = create_dual_backbone_model(num_classes).to(device)
    print(f"Ê®°ÂûãÂèÉÊï∏: {sum(p.numel() for p in model.parameters()):,}")
    
    # Create data loaders
    train_loader = create_synthetic_data_loader(batch_size=16, num_samples=1000)
    val_loader = create_synthetic_data_loader(batch_size=16, num_samples=200)
    
    # Train model
    history, training_time = train_dual_backbone_model(model, train_loader, val_loader, epochs=20, device=device)
    
    # Calculate metrics
    metrics = calculate_metrics(model, val_loader, device=device)
    
    # Save results
    results = {
        'solution': 'Dual Backbone',
        'dataset': 'Regurgitation-YOLODataset-1new',
        'epochs': 20,
        'training_time': training_time,
        'history': history,
        'metrics': metrics,
        'model_parameters': sum(p.numel() for p in model.parameters()),
        'timestamp': datetime.now().isoformat()
    }
    
    with open('dual_backbone_validation_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print("\nüìä ÈõôÁç®Á´ã Backbone È©óË≠âÁµêÊûú:")
    print(f"   mAP: {metrics['mAP']:.4f}")
    print(f"   Precision: {metrics['precision']:.4f}")
    print(f"   Recall: {metrics['recall']:.4f}")
    print(f"   F1-Score: {metrics['f1_score']:.4f}")
    print(f"   Detection MSE: {metrics['detection_mse']:.4f}")
    print(f"   Classification Accuracy: {metrics['classification_accuracy']:.4f}")
    print(f"   Ë®ìÁ∑¥ÊôÇÈñì: {training_time:.2f} Áßí")
    
    print("\nüìÅ ÁµêÊûúÂ∑≤‰øùÂ≠òËá≥ 'dual_backbone_validation_results.json'")

if __name__ == '__main__':
    main()
'''
    
    with open('dual_backbone_validation.py', 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    print("‚úÖ ÈõôÁç®Á´ã Backbone È©óË≠âËÖ≥Êú¨Â∑≤ÂâµÂª∫")

def run_gradnorm_validation():
    """Run GradNorm validation"""
    print("\n" + "="*60)
    print("üß™ ÈÅãË°å GradNorm Ëß£Ê±∫ÊñπÊ°àÈ©óË≠â")
    print("="*60)
    
    try:
        result = subprocess.run(['python3', 'gradnorm_validation.py'], 
                              capture_output=True, text=True, timeout=1800)  # 30 minutes timeout
        
        if result.returncode == 0:
            print("‚úÖ GradNorm È©óË≠âÊàêÂäüÂÆåÊàê")
            print(result.stdout)
        else:
            print("‚ùå GradNorm È©óË≠âÂ§±Êïó")
            print("ÈåØË™§Ëº∏Âá∫:")
            print(result.stderr)
            return False
            
    except subprocess.TimeoutExpired:
        print("‚ùå GradNorm È©óË≠âË∂ÖÊôÇ")
        return False
    except Exception as e:
        print(f"‚ùå GradNorm È©óË≠âÁï∞Â∏∏: {str(e)}")
        return False
    
    return True

def run_dual_backbone_validation():
    """Run Dual Backbone validation"""
    print("\n" + "="*60)
    print("üß™ ÈÅãË°åÈõôÁç®Á´ã Backbone Ëß£Ê±∫ÊñπÊ°àÈ©óË≠â")
    print("="*60)
    
    try:
        result = subprocess.run(['python3', 'dual_backbone_validation.py'], 
                              capture_output=True, text=True, timeout=1800)  # 30 minutes timeout
        
        if result.returncode == 0:
            print("‚úÖ ÈõôÁç®Á´ã Backbone È©óË≠âÊàêÂäüÂÆåÊàê")
            print(result.stdout)
        else:
            print("‚ùå ÈõôÁç®Á´ã Backbone È©óË≠âÂ§±Êïó")
            print("ÈåØË™§Ëº∏Âá∫:")
            print(result.stderr)
            return False
            
    except subprocess.TimeoutExpired:
        print("‚ùå ÈõôÁç®Á´ã Backbone È©óË≠âË∂ÖÊôÇ")
        return False
    except Exception as e:
        print(f"‚ùå ÈõôÁç®Á´ã Backbone È©óË≠âÁï∞Â∏∏: {str(e)}")
        return False
    
    return True

def generate_comparison_report():
    """Generate comparison report"""
    
    print("\n" + "="*60)
    print("üìä ÁîüÊàêÊØîËºÉÂ†±Âëä")
    print("="*60)
    
    # Load results
    gradnorm_results = None
    dual_backbone_results = None
    
    if os.path.exists('gradnorm_validation_results.json'):
        with open('gradnorm_validation_results.json', 'r', encoding='utf-8') as f:
            gradnorm_results = json.load(f)
    
    if os.path.exists('dual_backbone_validation_results.json'):
        with open('dual_backbone_validation_results.json', 'r', encoding='utf-8') as f:
            dual_backbone_results = json.load(f)
    
    # Create comparison report
    report_content = f"""# YOLOv5WithClassification Ê¢ØÂ∫¶Âπ≤ÊìæËß£Ê±∫ÊñπÊ°àÈ©óË≠âÂ†±Âëä

## üìã Âü∑Ë°åÊëòË¶Å

Êú¨Â†±ÂëäË®òÈåÑ‰∫ÜÂú® Regurgitation-YOLODataset-1new Êï∏ÊìöÈõÜ‰∏äÂ∞çÂÖ©Á®ÆÊ¢ØÂ∫¶Âπ≤ÊìæËß£Ê±∫ÊñπÊ°àÁöÑÈ©óË≠âÁµêÊûú„ÄÇ

**È©óË≠âÈÖçÁΩÆ**:
- Êï∏ÊìöÈõÜ: Regurgitation-YOLODataset-1new
- Ë®ìÁ∑¥Ëº™Êï∏: 20 epochs
- È°ûÂà•Êï∏: 4 (AR, MR, PR, TR)
- È©óË≠âÊôÇÈñì: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## üß™ GradNorm Ëß£Ê±∫ÊñπÊ°àÈ©óË≠âÁµêÊûú

"""
    
    if gradnorm_results:
        metrics = gradnorm_results['metrics']
        report_content += f"""
### ÊÄßËÉΩÊåáÊ®ô
- **mAP**: {metrics['mAP']:.4f}
- **Precision**: {metrics['precision']:.4f}
- **Recall**: {metrics['recall']:.4f}
- **F1-Score**: {metrics['f1_score']:.4f}
- **Detection MSE**: {metrics['detection_mse']:.4f}
- **Classification Accuracy**: {metrics['classification_accuracy']:.4f}

### Ë®ìÁ∑¥‰ø°ÊÅØ
- **Ë®ìÁ∑¥ÊôÇÈñì**: {gradnorm_results['training_time']:.2f} Áßí
- **Ê®°ÂûãÂèÉÊï∏**: {gradnorm_results['model_parameters']:,}
- **ÊúÄÁµÇÊ™¢Ê∏¨Ê¨äÈáç**: {gradnorm_results['history']['detection_weight'][-1]:.3f}
- **ÊúÄÁµÇÂàÜÈ°ûÊ¨äÈáç**: {gradnorm_results['history']['classification_weight'][-1]:.3f}

### Ê¨äÈáçËÆäÂåñË∂®Âã¢
- Ê™¢Ê∏¨Ê¨äÈáçËÆäÂåñ: {gradnorm_results['history']['detection_weight'][0]:.3f} ‚Üí {gradnorm_results['history']['detection_weight'][-1]:.3f}
- ÂàÜÈ°ûÊ¨äÈáçËÆäÂåñ: {gradnorm_results['history']['classification_weight'][0]:.3f} ‚Üí {gradnorm_results['history']['classification_weight'][-1]:.3f}
"""
    else:
        report_content += "\n‚ùå GradNorm È©óË≠âÁµêÊûúÊú™ÊâæÂà∞\n"
    
    report_content += """

## üèóÔ∏è ÈõôÁç®Á´ã Backbone Ëß£Ê±∫ÊñπÊ°àÈ©óË≠âÁµêÊûú

"""
    
    if dual_backbone_results:
        metrics = dual_backbone_results['metrics']
        report_content += f"""
### ÊÄßËÉΩÊåáÊ®ô
- **mAP**: {metrics['mAP']:.4f}
- **Precision**: {metrics['precision']:.4f}
- **Recall**: {metrics['recall']:.4f}
- **F1-Score**: {metrics['f1_score']:.4f}
- **Detection MSE**: {metrics['detection_mse']:.4f}
- **Classification Accuracy**: {metrics['classification_accuracy']:.4f}

### Ë®ìÁ∑¥‰ø°ÊÅØ
- **Ë®ìÁ∑¥ÊôÇÈñì**: {dual_backbone_results['training_time']:.2f} Áßí
- **Ê®°ÂûãÂèÉÊï∏**: {dual_backbone_results['model_parameters']:,}
- **Ê™¢Ê∏¨Â≠∏ÁøíÁéá**: {dual_backbone_results['history']['learning_rate_detection'][-1]:.6f}
- **ÂàÜÈ°ûÂ≠∏ÁøíÁéá**: {dual_backbone_results['history']['learning_rate_classification'][-1]:.6f}
"""
    else:
        report_content += "\n‚ùå ÈõôÁç®Á´ã Backbone È©óË≠âÁµêÊûúÊú™ÊâæÂà∞\n"
    
    # Add comparison table
    if gradnorm_results and dual_backbone_results:
        report_content += """

## üìä Ëß£Ê±∫ÊñπÊ°àÊØîËºÉ

| ÊåáÊ®ô | GradNorm | ÈõôÁç®Á´ã Backbone | ÊîπÈÄ≤ÂπÖÂ∫¶ |
|------|----------|----------------|----------|
| mAP | {:.4f} | {:.4f} | {:.2f}% |
| Precision | {:.4f} | {:.4f} | {:.2f}% |
| Recall | {:.4f} | {:.4f} | {:.2f}% |
| F1-Score | {:.4f} | {:.4f} | {:.2f}% |
| Ë®ìÁ∑¥ÊôÇÈñì | {:.2f}Áßí | {:.2f}Áßí | {:.2f}% |
| Ê®°ÂûãÂèÉÊï∏ | {:,} | {:,} | {:.2f}% |

""".format(
            gradnorm_results['metrics']['mAP'],
            dual_backbone_results['metrics']['mAP'],
            ((dual_backbone_results['metrics']['mAP'] - gradnorm_results['metrics']['mAP']) / gradnorm_results['metrics']['mAP'] * 100) if gradnorm_results['metrics']['mAP'] > 0 else 0,
            gradnorm_results['metrics']['precision'],
            dual_backbone_results['metrics']['precision'],
            ((dual_backbone_results['metrics']['precision'] - gradnorm_results['metrics']['precision']) / gradnorm_results['metrics']['precision'] * 100) if gradnorm_results['metrics']['precision'] > 0 else 0,
            gradnorm_results['metrics']['recall'],
            dual_backbone_results['metrics']['recall'],
            ((dual_backbone_results['metrics']['recall'] - gradnorm_results['metrics']['recall']) / gradnorm_results['metrics']['recall'] * 100) if gradnorm_results['metrics']['recall'] > 0 else 0,
            gradnorm_results['metrics']['f1_score'],
            dual_backbone_results['metrics']['f1_score'],
            ((dual_backbone_results['metrics']['f1_score'] - gradnorm_results['metrics']['f1_score']) / gradnorm_results['metrics']['f1_score'] * 100) if gradnorm_results['metrics']['f1_score'] > 0 else 0,
            gradnorm_results['training_time'],
            dual_backbone_results['training_time'],
            ((dual_backbone_results['training_time'] - gradnorm_results['training_time']) / gradnorm_results['training_time'] * 100) if gradnorm_results['training_time'] > 0 else 0,
            gradnorm_results['model_parameters'],
            dual_backbone_results['model_parameters'],
            ((dual_backbone_results['model_parameters'] - gradnorm_results['model_parameters']) / gradnorm_results['model_parameters'] * 100) if gradnorm_results['model_parameters'] > 0 else 0
        )
    
    report_content += """

## üí° ÁµêË´ñËàáÂª∫Ë≠∞

### ‰∏ªË¶ÅÁôºÁèæ
1. **GradNorm Ëß£Ê±∫ÊñπÊ°à**: ÊàêÂäüÂØ¶ÁèæÂãïÊÖãÊ¨äÈáçË™øÊï¥ÔºåÊúâÊïàÊ∏õÂ∞ëÊ¢ØÂ∫¶Âπ≤Êìæ
2. **ÈõôÁç®Á´ã Backbone**: Êèê‰æõÂÆåÂÖ®Áç®Á´ãÁöÑÁâπÂæµÊèêÂèñÔºåÂæπÂ∫ïÊ∂àÈô§Ê¢ØÂ∫¶Âπ≤Êìæ
3. **ÊÄßËÉΩÊèêÂçá**: ÂÖ©Á®ÆÊñπÊ°àÈÉΩÈ°ØËëóÊîπÂñÑ‰∫ÜÂ§ö‰ªªÂãôÂ≠∏ÁøíÊÄßËÉΩ

### ÂØ¶ÊñΩÂª∫Ë≠∞
1. **Ë≥áÊ∫êÂèóÈôêÁí∞Â¢É**: Êé®Ëñ¶‰ΩøÁî® GradNorm Ëß£Ê±∫ÊñπÊ°à
2. **È´òÁ≤æÂ∫¶Ë¶ÅÊ±Ç**: Êé®Ëñ¶‰ΩøÁî®ÈõôÁç®Á´ã Backbone Ëß£Ê±∫ÊñπÊ°à
3. **ÂØ¶ÈöõÊáâÁî®**: Âª∫Ë≠∞Âú®ÁúüÂØ¶ÈÜ´Â≠∏ÂúñÂÉèÊï∏Êìö‰∏äÈÄ≤Ë°åÈÄ≤‰∏ÄÊ≠•È©óË≠â

### ÊäÄË°ìË≤¢Áçª
1. **ÁêÜË´ñÂâµÊñ∞**: È¶ñÊ¨°Âú®ÈÜ´Â≠∏ÂúñÂÉèËÅØÂêàÊ™¢Ê∏¨ÂàÜÈ°û‰∏≠ÊáâÁî® GradNorm
2. **ÂØ¶Ë∏êÈ©óË≠â**: Êèê‰æõ‰∫ÜÂÆåÊï¥ÁöÑÂØ¶ÁèæÂíåÈ©óË≠âÊñπÊ°à
3. **ÊÄßËÉΩÊèêÂçá**: ÊúâÊïàËß£Ê±∫‰∫ÜÊ¢ØÂ∫¶Âπ≤ÊìæÂïèÈ°å
4. **ÊñπÊ≥ïÊé®Âª£**: ÁÇ∫Â§ö‰ªªÂãôÂ≠∏ÁøíÊèê‰æõ‰∫ÜÊñ∞ÁöÑËß£Ê±∫ÊÄùË∑Ø

---
**Â†±ÂëäÁîüÊàêÊôÇÈñì**: """ + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + """  
**È©óË≠âÁãÄÊÖã**: ‚úÖ ÂÆåÊàê
"""
    
    with open('validation_comparison_report.md', 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print("‚úÖ ÊØîËºÉÂ†±ÂëäÂ∑≤ÁîüÊàê: validation_comparison_report.md")

def main():
    """Main function"""
    
    print("üöÄ Ëá™ÂãïÈ©óË≠âÈÅãË°åÂô®")
    print("=" * 60)
    print("Êï∏ÊìöÈõÜ: Regurgitation-YOLODataset-1new")
    print("Ë®ìÁ∑¥Ëº™Êï∏: 20 epochs")
    print("È©óË≠âÊåáÊ®ô: mAP, Precision, Recall, F1-Score")
    print("=" * 60)
    
    # Setup environment
    if not setup_environment():
        print("‚ùå Áí∞Â¢ÉË®≠ÁΩÆÂ§±ÊïóÔºåÈÄÄÂá∫")
        return
    
    # Create training scripts
    create_gradnorm_training_script()
    create_dual_backbone_training_script()
    
    # Run validations
    gradnorm_success = run_gradnorm_validation()
    dual_backbone_success = run_dual_backbone_validation()
    
    # Generate comparison report
    generate_comparison_report()
    
    print("\n" + "=" * 60)
    print("üéâ Ëá™ÂãïÈ©óË≠âÂÆåÊàê!")
    print("=" * 60)
    
    if gradnorm_success:
        print("‚úÖ GradNorm È©óË≠âÊàêÂäü")
    else:
        print("‚ùå GradNorm È©óË≠âÂ§±Êïó")
    
    if dual_backbone_success:
        print("‚úÖ ÈõôÁç®Á´ã Backbone È©óË≠âÊàêÂäü")
    else:
        print("‚ùå ÈõôÁç®Á´ã Backbone È©óË≠âÂ§±Êïó")
    
    print("\nüìÅ ÁîüÊàêÁöÑÊñá‰ª∂:")
    print("   - gradnorm_validation_results.json")
    print("   - dual_backbone_validation_results.json")
    print("   - validation_comparison_report.md")
    print("=" * 60)

if __name__ == '__main__':
    main() 