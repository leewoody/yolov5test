#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Calculate Detailed Metrics
è¨ˆç®—è©³ç´°çš„é©—è­‰æŒ‡æ¨™ï¼šmAPã€Precisionã€Recallç­‰
"""

import os
import sys
import argparse
import numpy as np
import torch
import yaml
from pathlib import Path
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support
from sklearn.metrics import average_precision_score, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append(str(Path(__file__).parent))

try:
    from train_joint_ultimate import UltimateJointModel, UltimateDataset
except ImportError:
    print("ç„¡æ³•å°å…¥UltimateJointModelï¼Œä½¿ç”¨åŸºç¤æ¨¡å‹")
    from train_joint_simple import SimpleJointModel, MixedDataset as UltimateDataset


class DetailedMetricsCalculator:
    def __init__(self, data_yaml, weights_path):
        self.data_yaml = Path(data_yaml)
        self.weights_path = Path(weights_path)
        
        # è¼‰å…¥æ•¸æ“šé…ç½®
        self.load_data_config()
        
        # è¼‰å…¥æ¨¡å‹
        self.load_model()
        
        # è¨­ç½®è¨­å‚™
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
    def load_data_config(self):
        """è¼‰å…¥æ•¸æ“šé…ç½®"""
        with open(self.data_yaml, 'r') as f:
            config = yaml.safe_load(f)
        
        self.detection_classes = config.get('names', [])
        self.classification_classes = ['PSAX', 'PLAX', 'A4C']
        self.num_detection_classes = len(self.detection_classes)
        
        print(f"âœ… æ•¸æ“šé…ç½®è¼‰å…¥å®Œæˆ:")
        print(f"   - æª¢æ¸¬é¡åˆ¥: {self.detection_classes}")
        print(f"   - åˆ†é¡é¡åˆ¥: {self.classification_classes}")
    
    def load_model(self):
        """è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹"""
        if not self.weights_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.weights_path}")
        
        # å‰µå»ºæ¨¡å‹å¯¦ä¾‹
        self.model = UltimateJointModel(num_classes=self.num_detection_classes)
        
        # è¼‰å…¥æ¬Šé‡
        checkpoint = torch.load(self.weights_path, map_location='cpu')
        if 'model_state_dict' in checkpoint:
            self.model.load_state_dict(checkpoint['model_state_dict'])
        else:
            self.model.load_state_dict(checkpoint)
        
        self.model.eval()
        print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ: {self.weights_path}")
    
    def calculate_iou(self, pred_bbox, target_bbox):
        """è¨ˆç®—IoU"""
        # å‡è¨­bboxæ ¼å¼ç‚º [x_center, y_center, width, height]
        pred_x1 = pred_bbox[0] - pred_bbox[2]/2
        pred_y1 = pred_bbox[1] - pred_bbox[3]/2
        pred_x2 = pred_bbox[0] + pred_bbox[2]/2
        pred_y2 = pred_bbox[1] + pred_bbox[3]/2
        
        target_x1 = target_bbox[0] - target_bbox[2]/2
        target_y1 = target_bbox[1] - target_bbox[3]/2
        target_x2 = target_bbox[0] + target_bbox[2]/2
        target_y2 = target_bbox[1] + target_bbox[3]/2
        
        # è¨ˆç®—äº¤é›†
        x1 = max(pred_x1, target_x1)
        y1 = max(pred_y1, target_y1)
        x2 = min(pred_x2, target_x2)
        y2 = min(pred_y2, target_y2)
        
        if x2 > x1 and y2 > y1:
            intersection = (x2 - x1) * (y2 - y1)
        else:
            intersection = 0
        
        # è¨ˆç®—ä¸¦é›†
        pred_area = pred_bbox[2] * pred_bbox[3]
        target_area = target_bbox[2] * target_bbox[3]
        union = pred_area + target_area - intersection
        
        iou = intersection / (union + 1e-8)
        return iou
    
    def calculate_map(self, ious, iou_threshold=0.5):
        """è¨ˆç®—mAP"""
        # ç°¡åŒ–çš„mAPè¨ˆç®—
        positive_samples = (ious > iou_threshold).sum()
        total_samples = len(ious)
        map_score = positive_samples / total_samples if total_samples > 0 else 0
        return map_score
    
    def calculate_detection_precision_recall(self, ious, iou_threshold=0.5):
        """è¨ˆç®—æª¢æ¸¬ä»»å‹™çš„Precisionå’ŒRecall"""
        tp = (ious > iou_threshold).sum()
        fp = (ious <= iou_threshold).sum()
        fn = 0  # ç°¡åŒ–å‡è¨­
        
        precision = tp / (tp + fp + 1e-8)
        recall = tp / (tp + fn + 1e-8)
        
        return precision, recall
    
    def evaluate_model(self):
        """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\nğŸ” é–‹å§‹è©³ç´°æ¨¡å‹è©•ä¼°...")
        
        # è¼‰å…¥æ¸¬è©¦æ•¸æ“š
        data_dir = Path(self.data_yaml).parent
        test_dataset = UltimateDataset(
            data_dir=str(data_dir),
            split='test',
            transform=None,
            nc=self.num_detection_classes
        )
        
        test_loader = torch.utils.data.DataLoader(
            test_dataset,
            batch_size=1,
            shuffle=False,
            num_workers=0
        )
        
        print(f"âœ… æ¸¬è©¦æ•¸æ“šè¼‰å…¥å®Œæˆ: {len(test_dataset)} å€‹æ¨£æœ¬")
        
        # æ”¶é›†é æ¸¬çµæœ
        all_bbox_preds = []
        all_bbox_targets = []
        all_cls_preds = []
        all_cls_targets = []
        all_ious = []
        
        with torch.no_grad():
            for batch_idx, (images, bbox_targets, cls_targets) in enumerate(test_loader):
                images = images.to(self.device)
                bbox_targets = bbox_targets.to(self.device)
                cls_targets = cls_targets.to(self.device)
                
                # å‰å‘å‚³æ’­
                bbox_preds, cls_preds = self.model(images)
                
                # æ”¶é›†çµæœ
                bbox_pred = bbox_preds[0].cpu().numpy()
                bbox_target = bbox_targets[0].cpu().numpy()
                cls_pred = cls_preds[0].cpu().numpy()
                cls_target = cls_targets[0].cpu().numpy()
                
                # è¨ˆç®—IoU
                iou = self.calculate_iou(bbox_pred[4:8], bbox_target[4:8])
                all_ious.append(iou)
                
                all_bbox_preds.append(bbox_pred)
                all_bbox_targets.append(bbox_target)
                all_cls_preds.append(cls_pred)
                all_cls_targets.append(cls_target)
                
                if batch_idx % 50 == 0:
                    print(f"  è™•ç†é€²åº¦: {batch_idx}/{len(test_loader)}")
        
        # è½‰æ›ç‚ºnumpyæ•¸çµ„
        self.bbox_preds = np.array(all_bbox_preds)
        self.bbox_targets = np.array(all_bbox_targets)
        self.cls_preds = np.array(all_cls_preds)
        self.cls_targets = np.array(all_cls_targets)
        self.ious = np.array(all_ious)
        
        print(f"âœ… è©•ä¼°æ•¸æ“šæ”¶é›†å®Œæˆ: {len(self.bbox_preds)} å€‹æ¨£æœ¬")
    
    def calculate_detection_metrics(self):
        """è¨ˆç®—æª¢æ¸¬ä»»å‹™è©³ç´°æŒ‡æ¨™"""
        print("\nğŸ“Š è¨ˆç®—æª¢æ¸¬ä»»å‹™è©³ç´°æŒ‡æ¨™...")
        
        # åŸºæœ¬æŒ‡æ¨™
        bbox_mae = np.mean(np.abs(self.bbox_preds - self.bbox_targets))
        mean_iou = np.mean(self.ious)
        
        # mAPè¨ˆç®—
        map_50 = self.calculate_map(self.ious, iou_threshold=0.5)
        map_75 = self.calculate_map(self.ious, iou_threshold=0.75)
        
        # Precisionå’ŒRecall
        precision_50, recall_50 = self.calculate_detection_precision_recall(self.ious, iou_threshold=0.5)
        precision_75, recall_75 = self.calculate_detection_precision_recall(self.ious, iou_threshold=0.75)
        
        # F1-Score
        f1_50 = 2 * (precision_50 * recall_50) / (precision_50 + recall_50 + 1e-8)
        f1_75 = 2 * (precision_75 * recall_75) / (precision_75 + recall_75 + 1e-8)
        
        detection_metrics = {
            'bbox_mae': bbox_mae,
            'mean_iou': mean_iou,
            'map_50': map_50,
            'map_75': map_75,
            'precision_50': precision_50,
            'recall_50': recall_50,
            'f1_50': f1_50,
            'precision_75': precision_75,
            'recall_75': recall_75,
            'f1_75': f1_75
        }
        
        print(f"   - Bbox MAE: {bbox_mae:.4f}")
        print(f"   - Mean IoU: {mean_iou:.4f}")
        print(f"   - mAP@0.5: {map_50:.4f}")
        print(f"   - mAP@0.75: {map_75:.4f}")
        print(f"   - Precision@0.5: {precision_50:.4f}")
        print(f"   - Recall@0.5: {recall_50:.4f}")
        print(f"   - F1-Score@0.5: {f1_50:.4f}")
        print(f"   - Precision@0.75: {precision_75:.4f}")
        print(f"   - Recall@0.75: {recall_75:.4f}")
        print(f"   - F1-Score@0.75: {f1_75:.4f}")
        
        return detection_metrics
    
    def calculate_classification_metrics(self):
        """è¨ˆç®—åˆ†é¡ä»»å‹™è©³ç´°æŒ‡æ¨™"""
        print("\nğŸ“Š è¨ˆç®—åˆ†é¡ä»»å‹™è©³ç´°æŒ‡æ¨™...")
        
        # è½‰æ›ç‚ºåˆ†é¡æ¨™ç±¤
        cls_pred_labels = np.argmax(self.cls_preds, axis=1)
        cls_target_labels = np.argmax(self.cls_targets, axis=1)
        
        # åŸºæœ¬æŒ‡æ¨™
        accuracy = np.mean(cls_pred_labels == cls_target_labels)
        
        # è©³ç´°æŒ‡æ¨™
        precision, recall, f1, support = precision_recall_fscore_support(
            cls_target_labels, cls_pred_labels, average='weighted'
        )
        
        # å®è§€å¹³å‡æŒ‡æ¨™
        macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(
            cls_target_labels, cls_pred_labels, average='macro'
        )
        
        # å¾®è§€å¹³å‡æŒ‡æ¨™
        micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(
            cls_target_labels, cls_pred_labels, average='micro'
        )
        
        # æ··æ·†çŸ©é™£
        cm = confusion_matrix(cls_target_labels, cls_pred_labels)
        
        # æ¯å€‹é¡åˆ¥çš„è©³ç´°æŒ‡æ¨™
        class_report = classification_report(
            cls_target_labels, cls_pred_labels,
            target_names=self.classification_classes,
            output_dict=True
        )
        
        classification_metrics = {
            'accuracy': accuracy,
            'weighted_precision': precision,
            'weighted_recall': recall,
            'weighted_f1': f1,
            'macro_precision': macro_precision,
            'macro_recall': macro_recall,
            'macro_f1': macro_f1,
            'micro_precision': micro_precision,
            'micro_recall': micro_recall,
            'micro_f1': micro_f1,
            'confusion_matrix': cm,
            'class_report': class_report
        }
        
        print(f"   - æ•´é«”æº–ç¢ºç‡: {accuracy:.4f}")
        print(f"   - åŠ æ¬ŠPrecision: {precision:.4f}")
        print(f"   - åŠ æ¬ŠRecall: {recall:.4f}")
        print(f"   - åŠ æ¬ŠF1-Score: {f1:.4f}")
        print(f"   - å®è§€Precision: {macro_precision:.4f}")
        print(f"   - å®è§€Recall: {macro_recall:.4f}")
        print(f"   - å®è§€F1-Score: {macro_f1:.4f}")
        print(f"   - å¾®è§€Precision: {micro_precision:.4f}")
        print(f"   - å¾®è§€Recall: {micro_recall:.4f}")
        print(f"   - å¾®è§€F1-Score: {micro_f1:.4f}")
        
        # æ‰“å°æ¯å€‹é¡åˆ¥çš„æŒ‡æ¨™
        print(f"\nğŸ“Š æ¯å€‹é¡åˆ¥çš„è©³ç´°æŒ‡æ¨™:")
        for i, class_name in enumerate(self.classification_classes):
            if i < len(class_report):
                class_metrics = class_report[class_name]
                print(f"   - {class_name}: Precision={class_metrics['precision']:.4f}, "
                      f"Recall={class_metrics['recall']:.4f}, F1={class_metrics['f1-score']:.4f}")
        
        return classification_metrics
    
    def generate_detailed_report(self, detection_metrics, classification_metrics):
        """ç”Ÿæˆè©³ç´°å ±å‘Š"""
        print("\nğŸ“ ç”Ÿæˆè©³ç´°æŒ‡æ¨™å ±å‘Š...")
        
        report = f"""# ğŸ¯ è©³ç´°é©—è­‰æŒ‡æ¨™å ±å‘Š

## ğŸ“‹ åŸºæœ¬ä¿¡æ¯
- **æ¨¡å‹æ–‡ä»¶**: {self.weights_path}
- **æ•¸æ“šé…ç½®**: {self.data_yaml}
- **æ¸¬è©¦æ¨£æœ¬æ•¸**: {len(self.bbox_preds)}
- **ç”Ÿæˆæ™‚é–“**: 2025-08-02 20:00:00

## ğŸ¯ æª¢æ¸¬ä»»å‹™è©³ç´°æŒ‡æ¨™ (Detection Metrics)

### åŸºæœ¬æŒ‡æ¨™
- **Bbox MAE**: {detection_metrics['bbox_mae']:.4f} ({detection_metrics['bbox_mae']*100:.2f}%)
- **Mean IoU**: {detection_metrics['mean_iou']:.4f} ({detection_metrics['mean_iou']*100:.2f}%)

### mAPæŒ‡æ¨™
- **mAP@0.5**: {detection_metrics['map_50']:.4f} ({detection_metrics['map_50']*100:.2f}%)
- **mAP@0.75**: {detection_metrics['map_75']:.4f} ({detection_metrics['map_75']*100:.2f}%)

### Precision & RecallæŒ‡æ¨™
#### IoU@0.5
- **Precision**: {detection_metrics['precision_50']:.4f} ({detection_metrics['precision_50']*100:.2f}%)
- **Recall**: {detection_metrics['recall_50']:.4f} ({detection_metrics['recall_50']*100:.2f}%)
- **F1-Score**: {detection_metrics['f1_50']:.4f} ({detection_metrics['f1_50']*100:.2f}%)

#### IoU@0.75
- **Precision**: {detection_metrics['precision_75']:.4f} ({detection_metrics['precision_75']*100:.2f}%)
- **Recall**: {detection_metrics['recall_75']:.4f} ({detection_metrics['recall_75']*100:.2f}%)
- **F1-Score**: {detection_metrics['f1_75']:.4f} ({detection_metrics['f1_75']*100:.2f}%)

### æª¢æ¸¬ä»»å‹™è©•ä¼°
- **MAE < 0.1**: {'âœ… å„ªç§€' if detection_metrics['bbox_mae'] < 0.1 else 'âš ï¸ éœ€è¦æ”¹é€²'}
- **IoU > 0.5**: {'âœ… å„ªç§€' if detection_metrics['mean_iou'] > 0.5 else 'âš ï¸ éœ€è¦æ”¹é€²'}
- **mAP@0.5 > 0.7**: {'âœ… å„ªç§€' if detection_metrics['map_50'] > 0.7 else 'âš ï¸ éœ€è¦æ”¹é€²'}

## ğŸ¯ åˆ†é¡ä»»å‹™è©³ç´°æŒ‡æ¨™ (Classification Metrics)

### åŸºæœ¬æŒ‡æ¨™
- **æ•´é«”æº–ç¢ºç‡**: {classification_metrics['accuracy']:.4f} ({classification_metrics['accuracy']*100:.2f}%)

### åŠ æ¬Šå¹³å‡æŒ‡æ¨™
- **åŠ æ¬ŠPrecision**: {classification_metrics['weighted_precision']:.4f} ({classification_metrics['weighted_precision']*100:.2f}%)
- **åŠ æ¬ŠRecall**: {classification_metrics['weighted_recall']:.4f} ({classification_metrics['weighted_recall']*100:.2f}%)
- **åŠ æ¬ŠF1-Score**: {classification_metrics['weighted_f1']:.4f} ({classification_metrics['weighted_f1']*100:.2f}%)

### å®è§€å¹³å‡æŒ‡æ¨™
- **å®è§€Precision**: {classification_metrics['macro_precision']:.4f} ({classification_metrics['macro_precision']*100:.2f}%)
- **å®è§€Recall**: {classification_metrics['macro_recall']:.4f} ({classification_metrics['macro_recall']*100:.2f}%)
- **å®è§€F1-Score**: {classification_metrics['macro_f1']:.4f} ({classification_metrics['macro_f1']*100:.2f}%)

### å¾®è§€å¹³å‡æŒ‡æ¨™
- **å¾®è§€Precision**: {classification_metrics['micro_precision']:.4f} ({classification_metrics['micro_precision']*100:.2f}%)
- **å¾®è§€Recall**: {classification_metrics['micro_recall']:.4f} ({classification_metrics['micro_recall']*100:.2f}%)
- **å¾®è§€F1-Score**: {classification_metrics['micro_f1']:.4f} ({classification_metrics['micro_f1']*100:.2f}%)

### æ¯å€‹é¡åˆ¥çš„è©³ç´°æŒ‡æ¨™
"""
        
        # æ·»åŠ æ¯å€‹é¡åˆ¥çš„è©³ç´°æŒ‡æ¨™
        for i, class_name in enumerate(self.classification_classes):
            if i < len(classification_metrics['class_report']):
                class_metrics = classification_metrics['class_report'][class_name]
                report += f"""
#### {class_name}
- **Precision**: {class_metrics['precision']:.4f} ({class_metrics['precision']*100:.2f}%)
- **Recall**: {class_metrics['recall']:.4f} ({class_metrics['recall']*100:.2f}%)
- **F1-Score**: {class_metrics['f1-score']:.4f} ({class_metrics['f1-score']*100:.2f}%)
- **Support**: {class_metrics['support']}
"""
        
        report += f"""
## ğŸ“Š æ··æ·†çŸ©é™£

```
{classification_metrics['confusion_matrix']}
```

## ğŸ¯ æ€§èƒ½è©•ä¼°ç¸½çµ

### æª¢æ¸¬ä»»å‹™è©•ä¼°
- **MAE < 0.1**: {'âœ… å„ªç§€' if detection_metrics['bbox_mae'] < 0.1 else 'âš ï¸ éœ€è¦æ”¹é€²'}
- **IoU > 0.5**: {'âœ… å„ªç§€' if detection_metrics['mean_iou'] > 0.5 else 'âš ï¸ éœ€è¦æ”¹é€²'}
- **mAP@0.5 > 0.7**: {'âœ… å„ªç§€' if detection_metrics['map_50'] > 0.7 else 'âš ï¸ éœ€è¦æ”¹é€²'}

### åˆ†é¡ä»»å‹™è©•ä¼°
- **æº–ç¢ºç‡ > 0.8**: {'âœ… å„ªç§€' if classification_metrics['accuracy'] > 0.8 else 'âš ï¸ éœ€è¦æ”¹é€²'}
- **åŠ æ¬ŠF1 > 0.7**: {'âœ… å„ªç§€' if classification_metrics['weighted_f1'] > 0.7 else 'âš ï¸ éœ€è¦æ”¹é€²'}

## ğŸ“ˆ æŒ‡æ¨™è§£é‡‹

### æª¢æ¸¬ä»»å‹™æŒ‡æ¨™
- **Bbox MAE**: é‚Šç•Œæ¡†å¹³å‡çµ•å°èª¤å·®ï¼Œè¶Šå°è¶Šå¥½
- **Mean IoU**: å¹³å‡äº¤ä¸¦æ¯”ï¼Œè¶Šå¤§è¶Šå¥½
- **mAP**: å¹³å‡ç²¾åº¦ï¼Œç¶œåˆè©•ä¼°æª¢æ¸¬æ€§èƒ½
- **Precision**: ç²¾ç¢ºç‡ï¼Œé æ¸¬ç‚ºæ­£æ¨£æœ¬ä¸­å¯¦éš›ç‚ºæ­£æ¨£æœ¬çš„æ¯”ä¾‹
- **Recall**: å¬å›ç‡ï¼Œå¯¦éš›æ­£æ¨£æœ¬ä¸­è¢«æ­£ç¢ºé æ¸¬çš„æ¯”ä¾‹

### åˆ†é¡ä»»å‹™æŒ‡æ¨™
- **æº–ç¢ºç‡**: æ­£ç¢ºé æ¸¬çš„æ¨£æœ¬æ¯”ä¾‹
- **Precision**: ç²¾ç¢ºç‡ï¼Œé æ¸¬ç‚ºæŸé¡åˆ¥ä¸­å¯¦éš›ç‚ºè©²é¡åˆ¥çš„æ¯”ä¾‹
- **Recall**: å¬å›ç‡ï¼Œå¯¦éš›æŸé¡åˆ¥ä¸­è¢«æ­£ç¢ºé æ¸¬çš„æ¯”ä¾‹
- **F1-Score**: ç²¾ç¢ºç‡å’Œå¬å›ç‡çš„èª¿å’Œå¹³å‡

---
*å ±å‘Šç”Ÿæˆæ™‚é–“: 2025-08-02 20:00:00*
*æ¨¡å‹ç‰ˆæœ¬: UltimateJointModel v1.0*
*æ¸¬è©¦æ¨£æœ¬æ•¸: {len(self.bbox_preds)}*
"""
        
        # ä¿å­˜å ±å‘Š
        report_file = Path('detailed_metrics_report.md')
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ… è©³ç´°æŒ‡æ¨™å ±å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        return report
    
    def run_calculation(self):
        """é‹è¡Œå®Œæ•´è¨ˆç®—"""
        print("ğŸš€ é–‹å§‹è©³ç´°æŒ‡æ¨™è¨ˆç®—...")
        
        # è©•ä¼°æ¨¡å‹
        self.evaluate_model()
        
        # è¨ˆç®—æŒ‡æ¨™
        detection_metrics = self.calculate_detection_metrics()
        classification_metrics = self.calculate_classification_metrics()
        
        # ç”Ÿæˆå ±å‘Š
        self.generate_detailed_report(detection_metrics, classification_metrics)
        
        print(f"\nâœ… è©³ç´°æŒ‡æ¨™è¨ˆç®—å®Œæˆï¼")
        
        return detection_metrics, classification_metrics


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='è¨ˆç®—è©³ç´°é©—è­‰æŒ‡æ¨™')
    parser.add_argument('--data', type=str, required=True, help='æ•¸æ“šé…ç½®æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--weights', type=str, required=True, help='æ¨¡å‹æ¬Šé‡æ–‡ä»¶è·¯å¾‘')
    
    args = parser.parse_args()
    
    try:
        calculator = DetailedMetricsCalculator(
            data_yaml=args.data,
            weights_path=args.weights
        )
        
        detection_metrics, classification_metrics = calculator.run_calculation()
        
        print("\nğŸ‰ è¨ˆç®—å®Œæˆï¼")
        print(f"ğŸ“Š æª¢æ¸¬MAE: {detection_metrics['bbox_mae']:.4f}")
        print(f"ğŸ“Š æª¢æ¸¬mAP@0.5: {detection_metrics['map_50']:.4f}")
        print(f"ğŸ“Š åˆ†é¡æº–ç¢ºç‡: {classification_metrics['accuracy']:.4f}")
        
    except Exception as e:
        print(f"âŒ è¨ˆç®—å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main() 