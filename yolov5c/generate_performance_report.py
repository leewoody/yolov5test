#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Generate Performance Report
ç”Ÿæˆè©³ç´°çš„æ€§èƒ½åˆ†æå ±å‘Šï¼ŒåŒ…æ‹¬mAPã€Precisionã€Recallç­‰æŒ‡æ¨™
"""

import os
import sys
import argparse
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import yaml
from datetime import datetime
import torch
import torch.nn as nn
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support
from sklearn.metrics import average_precision_score, precision_recall_curve
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append(str(Path(__file__).parent))

try:
    from train_joint_ultimate import UltimateJointModel, UltimateDataset
except ImportError:
    print("ç„¡æ³•å°å…¥UltimateJointModelï¼Œä½¿ç”¨åŸºç¤æ¨¡å‹")
    from train_joint_simple import SimpleJointModel, MixedDataset as UltimateDataset


class PerformanceAnalyzer:
    def __init__(self, data_yaml, weights_path, output_dir="performance_analysis"):
        self.data_yaml = Path(data_yaml)
        self.weights_path = Path(weights_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # è¼‰å…¥æ•¸æ“šé…ç½®
        self.load_data_config()
        
        # è¼‰å…¥æ¨¡å‹
        self.load_model()
        
        # è¨­ç½®è¨­å‚™
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
    def load_data_config(self):
        """è¼‰å…¥æ•¸æ“šé…ç½®"""
        with open(self.data_yaml, 'r') as f:
            config = yaml.safe_load(f)
        
        self.detection_classes = config.get('names', [])
        self.classification_classes = ['PSAX', 'PLAX', 'A4C']
        self.num_detection_classes = len(self.detection_classes)
        self.num_classification_classes = len(self.classification_classes)
        
        print(f"âœ… æ•¸æ“šé…ç½®è¼‰å…¥å®Œæˆ:")
        print(f"   - æª¢æ¸¬é¡åˆ¥: {self.detection_classes}")
        print(f"   - åˆ†é¡é¡åˆ¥: {self.classification_classes}")
    
    def load_model(self):
        """è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹"""
        if not self.weights_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.weights_path}")
        
        # å‰µå»ºæ¨¡å‹å¯¦ä¾‹
        if 'UltimateJointModel' in globals():
            self.model = UltimateJointModel(
                num_classes=self.num_detection_classes
            )
        else:
            self.model = SimpleJointModel(
                num_detection_classes=self.num_detection_classes,
                num_classification_classes=self.num_classification_classes
            )
        
        # è¼‰å…¥æ¬Šé‡
        checkpoint = torch.load(self.weights_path, map_location='cpu')
        if 'model_state_dict' in checkpoint:
            self.model.load_state_dict(checkpoint['model_state_dict'])
        else:
            self.model.load_state_dict(checkpoint)
        
        self.model.eval()
        print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ: {self.weights_path}")
    
    def evaluate_model(self):
        """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\nğŸ” é–‹å§‹æ¨¡å‹è©•ä¼°...")
        
        # è¼‰å…¥æ¸¬è©¦æ•¸æ“š
        test_dataset = UltimateDataset(
            data_dir=str(self.data_yaml),
            split='test',
            transform=None,
            nc=self.num_detection_classes
        )
        
        test_loader = torch.utils.data.DataLoader(
            test_dataset,
            batch_size=1,
            shuffle=False,
            num_workers=0
        )
        
        # æ”¶é›†é æ¸¬çµæœ
        all_bbox_preds = []
        all_bbox_targets = []
        all_cls_preds = []
        all_cls_targets = []
        all_images = []
        
        with torch.no_grad():
            for batch_idx, (images, bbox_targets, cls_targets) in enumerate(test_loader):
                images = images.to(self.device)
                bbox_targets = bbox_targets.to(self.device)
                cls_targets = cls_targets.to(self.device)
                
                # å‰å‘å‚³æ’­
                bbox_preds, cls_preds = self.model(images)
                
                # æ”¶é›†çµæœ
                all_bbox_preds.append(bbox_preds.cpu().numpy())
                all_bbox_targets.append(bbox_targets.cpu().numpy())
                all_cls_preds.append(cls_preds.cpu().numpy())
                all_cls_targets.append(cls_targets.cpu().numpy())
                all_images.append(images.cpu().numpy())
                
                if batch_idx % 50 == 0:
                    print(f"  è™•ç†é€²åº¦: {batch_idx}/{len(test_loader)}")
        
        # è½‰æ›ç‚ºnumpyæ•¸çµ„
        self.bbox_preds = np.concatenate(all_bbox_preds, axis=0)
        self.bbox_targets = np.concatenate(all_bbox_targets, axis=0)
        self.cls_preds = np.concatenate(all_cls_preds, axis=0)
        self.cls_targets = np.concatenate(all_cls_targets, axis=0)
        self.images = np.concatenate(all_images, axis=0)
        
        print(f"âœ… è©•ä¼°æ•¸æ“šæ”¶é›†å®Œæˆ: {len(self.bbox_preds)} å€‹æ¨£æœ¬")
    
    def calculate_detection_metrics(self):
        """è¨ˆç®—æª¢æ¸¬ä»»å‹™æŒ‡æ¨™"""
        print("\nğŸ“Š è¨ˆç®—æª¢æ¸¬ä»»å‹™æŒ‡æ¨™...")
        
        # è¨ˆç®—MAE
        bbox_mae = np.mean(np.abs(self.bbox_preds - self.bbox_targets))
        
        # è¨ˆç®—IoU
        ious = self.calculate_iou(self.bbox_preds, self.bbox_targets)
        mean_iou = np.mean(ious)
        
        # è¨ˆç®—mAP (ç°¡åŒ–ç‰ˆæœ¬)
        map_score = self.calculate_simplified_map()
        
        # è¨ˆç®—Precisionå’ŒRecall
        precision, recall = self.calculate_detection_pr()
        
        detection_metrics = {
            'bbox_mae': bbox_mae,
            'mean_iou': mean_iou,
            'map_score': map_score,
            'precision': precision,
            'recall': recall,
            'f1_score': 2 * (precision * recall) / (precision + recall + 1e-8)
        }
        
        print(f"   - Bbox MAE: {bbox_mae:.4f}")
        print(f"   - Mean IoU: {mean_iou:.4f}")
        print(f"   - mAP: {map_score:.4f}")
        print(f"   - Precision: {precision:.4f}")
        print(f"   - Recall: {recall:.4f}")
        print(f"   - F1-Score: {detection_metrics['f1_score']:.4f}")
        
        return detection_metrics
    
    def calculate_classification_metrics(self):
        """è¨ˆç®—åˆ†é¡ä»»å‹™æŒ‡æ¨™"""
        print("\nğŸ“Š è¨ˆç®—åˆ†é¡ä»»å‹™æŒ‡æ¨™...")
        
        # è½‰æ›ç‚ºåˆ†é¡æ¨™ç±¤
        cls_pred_labels = np.argmax(self.cls_preds, axis=1)
        cls_target_labels = np.argmax(self.cls_targets, axis=1)
        
        # è¨ˆç®—æº–ç¢ºç‡
        accuracy = np.mean(cls_pred_labels == cls_target_labels)
        
        # è¨ˆç®—è©³ç´°æŒ‡æ¨™
        precision, recall, f1, support = precision_recall_fscore_support(
            cls_target_labels, cls_pred_labels, average='weighted'
        )
        
        # è¨ˆç®—æ··æ·†çŸ©é™£
        cm = confusion_matrix(cls_target_labels, cls_pred_labels)
        
        # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„æŒ‡æ¨™
        class_report = classification_report(
            cls_target_labels, cls_pred_labels,
            target_names=self.classification_classes,
            output_dict=True
        )
        
        classification_metrics = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'support': support,
            'confusion_matrix': cm,
            'class_report': class_report
        }
        
        print(f"   - æ•´é«”æº–ç¢ºç‡: {accuracy:.4f}")
        print(f"   - åŠ æ¬ŠPrecision: {precision:.4f}")
        print(f"   - åŠ æ¬ŠRecall: {recall:.4f}")
        print(f"   - åŠ æ¬ŠF1-Score: {f1:.4f}")
        
        # æ‰“å°æ¯å€‹é¡åˆ¥çš„æŒ‡æ¨™
        for i, class_name in enumerate(self.classification_classes):
            if i < len(class_report):
                class_metrics = class_report[class_name]
                print(f"   - {class_name}: Precision={class_metrics['precision']:.4f}, "
                      f"Recall={class_metrics['recall']:.4f}, F1={class_metrics['f1-score']:.4f}")
        
        return classification_metrics
    
    def calculate_iou(self, preds, targets):
        """è¨ˆç®—IoU"""
        ious = []
        for pred, target in zip(preds, targets):
            # ç°¡åŒ–çš„IoUè¨ˆç®—
            pred_area = pred[2] * pred[3]  # width * height
            target_area = target[2] * target[3]
            
            # è¨ˆç®—äº¤é›†
            x1 = max(pred[0] - pred[2]/2, target[0] - target[2]/2)
            y1 = max(pred[1] - pred[3]/2, target[1] - target[3]/2)
            x2 = min(pred[0] + pred[2]/2, target[0] + target[2]/2)
            y2 = min(pred[1] + pred[3]/2, target[1] + target[3]/2)
            
            if x2 > x1 and y2 > y1:
                intersection = (x2 - x1) * (y2 - y1)
            else:
                intersection = 0
            
            union = pred_area + target_area - intersection
            iou = intersection / (union + 1e-8)
            ious.append(iou)
        
        return np.array(ious)
    
    def calculate_simplified_map(self):
        """è¨ˆç®—ç°¡åŒ–ç‰ˆæœ¬çš„mAP"""
        # ä½¿ç”¨IoU > 0.5ä½œç‚ºæ­£æ¨£æœ¬
        ious = self.calculate_iou(self.bbox_preds, self.bbox_targets)
        positive_samples = (ious > 0.5).sum()
        total_samples = len(ious)
        
        # ç°¡åŒ–çš„mAPè¨ˆç®—
        map_score = positive_samples / total_samples if total_samples > 0 else 0
        return map_score
    
    def calculate_detection_pr(self):
        """è¨ˆç®—æª¢æ¸¬ä»»å‹™çš„Precisionå’ŒRecall"""
        ious = self.calculate_iou(self.bbox_preds, self.bbox_targets)
        
        # ä½¿ç”¨IoU > 0.5ä½œç‚ºæ­£æ¨£æœ¬
        tp = (ious > 0.5).sum()
        fp = (ious <= 0.5).sum()
        fn = 0  # ç°¡åŒ–å‡è¨­
        
        precision = tp / (tp + fp + 1e-8)
        recall = tp / (tp + fn + 1e-8)
        
        return precision, recall
    
    def generate_visualizations(self, detection_metrics, classification_metrics):
        """ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨"""
        print("\nğŸ“ˆ ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨...")
        
        # è¨­ç½®ä¸­æ–‡å­—é«”
        plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. åˆ†é¡æ··æ·†çŸ©é™£
        plt.figure(figsize=(10, 8))
        cm = classification_metrics['confusion_matrix']
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=self.classification_classes,
                   yticklabels=self.classification_classes)
        plt.title('åˆ†é¡æ··æ·†çŸ©é™£', fontsize=16)
        plt.xlabel('é æ¸¬æ¨™ç±¤', fontsize=12)
        plt.ylabel('çœŸå¯¦æ¨™ç±¤', fontsize=12)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'classification_confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. æª¢æ¸¬æ€§èƒ½æŒ‡æ¨™
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # Bbox MAEåˆ†å¸ƒ
        bbox_errors = np.abs(self.bbox_preds - self.bbox_targets)
        axes[0, 0].hist(bbox_errors.flatten(), bins=50, alpha=0.7, color='blue')
        axes[0, 0].set_title('Bbox MAE åˆ†å¸ƒ')
        axes[0, 0].set_xlabel('MAE')
        axes[0, 0].set_ylabel('é »ç‡')
        
        # IoUåˆ†å¸ƒ
        ious = self.calculate_iou(self.bbox_preds, self.bbox_targets)
        axes[0, 1].hist(ious, bins=50, alpha=0.7, color='green')
        axes[0, 1].set_title('IoU åˆ†å¸ƒ')
        axes[0, 1].set_xlabel('IoU')
        axes[0, 1].set_ylabel('é »ç‡')
        
        # åˆ†é¡æº–ç¢ºç‡
        cls_pred_labels = np.argmax(self.cls_preds, axis=1)
        cls_target_labels = np.argmax(self.cls_targets, axis=1)
        correct_predictions = (cls_pred_labels == cls_target_labels)
        axes[1, 0].bar(['æ­£ç¢º', 'éŒ¯èª¤'], [correct_predictions.sum(), (~correct_predictions).sum()], 
                      color=['green', 'red'])
        axes[1, 0].set_title('åˆ†é¡é æ¸¬çµæœ')
        axes[1, 0].set_ylabel('æ¨£æœ¬æ•¸é‡')
        
        # æ€§èƒ½æŒ‡æ¨™ç¸½çµ
        metrics_text = f"""
æª¢æ¸¬ä»»å‹™:
- Bbox MAE: {detection_metrics['bbox_mae']:.4f}
- Mean IoU: {detection_metrics['mean_iou']:.4f}
- mAP: {detection_metrics['map_score']:.4f}
- Precision: {detection_metrics['precision']:.4f}
- Recall: {detection_metrics['recall']:.4f}

åˆ†é¡ä»»å‹™:
- æº–ç¢ºç‡: {classification_metrics['accuracy']:.4f}
- Precision: {classification_metrics['precision']:.4f}
- Recall: {classification_metrics['recall']:.4f}
- F1-Score: {classification_metrics['f1_score']:.4f}
        """
        axes[1, 1].text(0.1, 0.5, metrics_text, transform=axes[1, 1].transAxes,
                       fontsize=10, verticalalignment='center')
        axes[1, 1].set_title('æ€§èƒ½æŒ‡æ¨™ç¸½çµ')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'performance_metrics.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… å¯è¦–åŒ–åœ–è¡¨å·²ä¿å­˜åˆ°: {self.output_dir}")
    
    def generate_report(self, detection_metrics, classification_metrics):
        """ç”Ÿæˆè©³ç´°å ±å‘Š"""
        print("\nğŸ“ ç”Ÿæˆè©³ç´°å ±å‘Š...")
        
        report = f"""# ğŸ¤– å¤šä»»å‹™æ¨¡å‹æ€§èƒ½åˆ†æå ±å‘Š

## ğŸ“‹ åŸºæœ¬ä¿¡æ¯
- **æ¨¡å‹æ–‡ä»¶**: {self.weights_path}
- **æ•¸æ“šé…ç½®**: {self.data_yaml}
- **æ¸¬è©¦æ¨£æœ¬æ•¸**: {len(self.bbox_preds)}
- **ç”Ÿæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ¯ æª¢æ¸¬ä»»å‹™æ€§èƒ½

### ä¸»è¦æŒ‡æ¨™
- **Bbox MAE**: {detection_metrics['bbox_mae']:.4f}
- **Mean IoU**: {detection_metrics['mean_iou']:.4f}
- **mAP**: {detection_metrics['map_score']:.4f}
- **Precision**: {detection_metrics['precision']:.4f}
- **Recall**: {detection_metrics['recall']:.4f}
- **F1-Score**: {detection_metrics['f1_score']:.4f}

### æª¢æ¸¬é¡åˆ¥: {self.detection_classes}

## ğŸ¯ åˆ†é¡ä»»å‹™æ€§èƒ½

### ä¸»è¦æŒ‡æ¨™
- **æ•´é«”æº–ç¢ºç‡**: {classification_metrics['accuracy']:.4f}
- **åŠ æ¬ŠPrecision**: {classification_metrics['precision']:.4f}
- **åŠ æ¬ŠRecall**: {classification_metrics['recall']:.4f}
- **åŠ æ¬ŠF1-Score**: {classification_metrics['f1_score']:.4f}

### åˆ†é¡é¡åˆ¥: {self.classification_classes}

### æ¯å€‹é¡åˆ¥çš„è©³ç´°æŒ‡æ¨™
"""
        
        # æ·»åŠ æ¯å€‹é¡åˆ¥çš„è©³ç´°æŒ‡æ¨™
        for i, class_name in enumerate(self.classification_classes):
            if i < len(classification_metrics['class_report']):
                class_metrics = classification_metrics['class_report'][class_name]
                report += f"""
#### {class_name}
- **Precision**: {class_metrics['precision']:.4f}
- **Recall**: {class_metrics['recall']:.4f}
- **F1-Score**: {class_metrics['f1-score']:.4f}
- **Support**: {class_metrics['support']}
"""
        
        report += f"""
## ğŸ“Š æ··æ·†çŸ©é™£

åˆ†é¡æ··æ·†çŸ©é™£å·²ä¿å­˜ç‚ºåœ–ç‰‡: `classification_confusion_matrix.png`

## ğŸ“ˆ æ€§èƒ½åœ–è¡¨

è©³ç´°æ€§èƒ½åœ–è¡¨å·²ä¿å­˜ç‚º: `performance_metrics.png`

## ğŸ¯ æ€§èƒ½è©•ä¼°

### æª¢æ¸¬ä»»å‹™è©•ä¼°
- **MAE < 0.1**: {'âœ… å„ªç§€' if detection_metrics['bbox_mae'] < 0.1 else 'âš ï¸ éœ€è¦æ”¹é€²'}
- **IoU > 0.5**: {'âœ… å„ªç§€' if detection_metrics['mean_iou'] > 0.5 else 'âš ï¸ éœ€è¦æ”¹é€²'}
- **mAP > 0.7**: {'âœ… å„ªç§€' if detection_metrics['map_score'] > 0.7 else 'âš ï¸ éœ€è¦æ”¹é€²'}

### åˆ†é¡ä»»å‹™è©•ä¼°
- **æº–ç¢ºç‡ > 0.8**: {'âœ… å„ªç§€' if classification_metrics['accuracy'] > 0.8 else 'âš ï¸ éœ€è¦æ”¹é€²'}
- **F1-Score > 0.7**: {'âœ… å„ªç§€' if classification_metrics['f1_score'] > 0.7 else 'âš ï¸ éœ€è¦æ”¹é€²'}

## ğŸš€ æ”¹é€²å»ºè­°

### æª¢æ¸¬ä»»å‹™æ”¹é€²
1. **å¢åŠ è¨“ç·´æ•¸æ“š**: ç‰¹åˆ¥æ˜¯IoUè¼ƒä½çš„æ¨£æœ¬
2. **èª¿æ•´æ¨¡å‹æ¶æ§‹**: ä½¿ç”¨æ›´æ·±çš„ç¶²çµ¡
3. **å„ªåŒ–æå¤±å‡½æ•¸**: èª¿æ•´bboxæå¤±æ¬Šé‡
4. **æ•¸æ“šå¢å¼·**: å¢åŠ æ›´å¤šæ¨£åŒ–çš„æ•¸æ“šå¢å¼·

### åˆ†é¡ä»»å‹™æ”¹é€²
1. **é¡åˆ¥å¹³è¡¡**: è™•ç†é¡åˆ¥ä¸å¹³è¡¡å•é¡Œ
2. **ç‰¹å¾µæå–**: æ”¹é€²ç‰¹å¾µæå–èƒ½åŠ›
3. **æ­£å‰‡åŒ–**: å¢åŠ Dropoutç­‰æ­£å‰‡åŒ–æŠ€è¡“
4. **å­¸ç¿’ç‡èª¿åº¦**: å„ªåŒ–å­¸ç¿’ç‡ç­–ç•¥

---
*å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        # ä¿å­˜å ±å‘Š
        report_file = self.output_dir / 'performance_analysis_report.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ… è©³ç´°å ±å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        return report
    
    def run_analysis(self):
        """é‹è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ é–‹å§‹æ€§èƒ½åˆ†æ...")
        
        # è©•ä¼°æ¨¡å‹
        self.evaluate_model()
        
        # è¨ˆç®—æŒ‡æ¨™
        detection_metrics = self.calculate_detection_metrics()
        classification_metrics = self.calculate_classification_metrics()
        
        # ç”Ÿæˆå¯è¦–åŒ–
        self.generate_visualizations(detection_metrics, classification_metrics)
        
        # ç”Ÿæˆå ±å‘Š
        self.generate_report(detection_metrics, classification_metrics)
        
        print(f"\nâœ… æ€§èƒ½åˆ†æå®Œæˆï¼çµæœä¿å­˜åœ¨: {self.output_dir}")
        
        return detection_metrics, classification_metrics


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='ç”Ÿæˆæ€§èƒ½åˆ†æå ±å‘Š')
    parser.add_argument('--data', type=str, required=True, help='æ•¸æ“šé…ç½®æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--weights', type=str, required=True, help='æ¨¡å‹æ¬Šé‡æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--output-dir', type=str, default='performance_analysis', help='è¼¸å‡ºç›®éŒ„')
    
    args = parser.parse_args()
    
    try:
        analyzer = PerformanceAnalyzer(
            data_yaml=args.data,
            weights_path=args.weights,
            output_dir=args.output_dir
        )
        
        detection_metrics, classification_metrics = analyzer.run_analysis()
        
        print("\nğŸ‰ åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {args.output_dir}")
        print(f"ğŸ“Š æª¢æ¸¬MAE: {detection_metrics['bbox_mae']:.4f}")
        print(f"ğŸ“Š åˆ†é¡æº–ç¢ºç‡: {classification_metrics['accuracy']:.4f}")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±æ•—: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 