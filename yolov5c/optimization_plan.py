#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Optimization Plan for 70% Performance Improvement
70%æ€§èƒ½æå‡å„ªåŒ–è¨ˆåŠƒ
"""

import os
import sys
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from PIL import Image
import yaml
from pathlib import Path
import random
from collections import Counter
import torch.nn.functional as F
import torchvision.models as models

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append(str(Path(__file__).parent))

class AdvancedJointModel(nn.Module):
    """é«˜ç´šè¯åˆæ¨¡å‹ - ä½¿ç”¨é è¨“ç·´éª¨å¹¹ç¶²çµ¡"""
    def __init__(self, num_detection_classes=4, num_classification_classes=3):
        super(AdvancedJointModel, self).__init__()
        
        # ä½¿ç”¨é è¨“ç·´çš„ResNet50ä½œç‚ºéª¨å¹¹ç¶²çµ¡
        self.backbone = models.resnet50(pretrained=True)
        # ç§»é™¤æœ€å¾Œçš„åˆ†é¡å±¤
        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])
        
        # ç‰¹å¾µç¶­åº¦
        feature_dim = 2048
        
        # æ³¨æ„åŠ›æ©Ÿåˆ¶
        self.attention = nn.MultiheadAttention(feature_dim, num_heads=8, batch_first=True)
        
        # æª¢æ¸¬é ­ - æ›´æ·±çš„ç¶²çµ¡
        self.detection_head = nn.Sequential(
            nn.Linear(feature_dim, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(1024, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, num_detection_classes * 4)  # 4å€‹æª¢æ¸¬é¡åˆ¥ï¼Œæ¯å€‹4å€‹å€¼
        )
        
        # åˆ†é¡é ­ - æ›´æ·±çš„ç¶²çµ¡
        self.classification_head = nn.Sequential(
            nn.Linear(feature_dim, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(1024, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, num_classification_classes)
        )
        
        # åˆå§‹åŒ–æ¬Šé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æ¬Šé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # éª¨å¹¹ç¶²çµ¡ç‰¹å¾µæå–
        features = self.backbone(x)
        features = features.view(features.size(0), -1)  # å±•å¹³
        
        # æ³¨æ„åŠ›æ©Ÿåˆ¶
        features_reshaped = features.unsqueeze(1)  # [batch, 1, features]
        attended_features, _ = self.attention(features_reshaped, features_reshaped, features_reshaped)
        attended_features = attended_features.squeeze(1)  # [batch, features]
        
        # æª¢æ¸¬é æ¸¬
        detection_output = self.detection_head(attended_features)
        detection_output = detection_output.view(-1, 4, 4)  # [batch, 4, 4]
        
        # åˆ†é¡é æ¸¬
        classification_output = self.classification_head(attended_features)
        
        return detection_output, classification_output


class AdvancedFocalLoss(nn.Module):
    """é«˜ç´šFocal Loss - è™•ç†é¡åˆ¥ä¸å¹³è¡¡"""
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(AdvancedFocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        # è½‰æ›ç‚ºåˆ†é¡æ¨™ç±¤
        if targets.dim() > 1:
            targets = torch.argmax(targets, dim=1)
        
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class AdvancedDataset(Dataset):
    """é«˜ç´šæ•¸æ“šé›† - åŒ…å«å¼·åŒ–çš„æ•¸æ“šå¢å¼·å’Œå¹³è¡¡ç­–ç•¥"""
    def __init__(self, data_dir, split='train', transform=None, nc=4, oversample_minority=True):
        self.data_dir = Path(data_dir)
        self.split = split
        self.nc = nc
        self.oversample_minority = oversample_minority
        
        # è¼‰å…¥æ•¸æ“šé…ç½®
        with open(self.data_dir / 'data.yaml', 'r') as f:
            config = yaml.safe_load(f)
        
        self.class_names = config.get('names', [])
        self.images_dir = self.data_dir / split / 'images'
        self.labels_dir = self.data_dir / split / 'labels'
        
        # ç²å–æ‰€æœ‰åœ–åƒæ–‡ä»¶
        self.image_files = list(self.images_dir.glob('*.png')) + list(self.images_dir.glob('*.jpg'))
        
        # æ™ºèƒ½éæ¡æ¨£
        if self.oversample_minority and split == 'train':
            self.image_files = self._advanced_oversample()
        
        print(f"{split} æ•¸æ“šé›†: {len(self.image_files)} å€‹æœ‰æ•ˆæ¨£æœ¬")
        
        # è¨­ç½®è½‰æ›
        if transform is None:
            self.transform = self._get_advanced_transforms(split)
        else:
            self.transform = transform
    
    def _advanced_oversample(self):
        """é«˜ç´šéæ¡æ¨£ç­–ç•¥"""
        # åˆ†æé¡åˆ¥åˆ†å¸ƒ
        class_counts = Counter()
        for img_file in self.image_files:
            label_file = self.labels_dir / f"{img_file.stem}.txt"
            if label_file.exists():
                with open(label_file, 'r') as f:
                    lines = f.readlines()
                    if len(lines) >= 1:
                        try:
                            class_id = int(lines[0].split()[0])
                            class_counts[class_id] += 1
                        except:
                            pass
        
        print(f"åŸå§‹é¡åˆ¥åˆ†å¸ƒ: {dict(class_counts)}")
        
        # è¨ˆç®—ç›®æ¨™æ¨£æœ¬æ•¸
        max_count = max(class_counts.values()) if class_counts else 1000
        target_count = int(max_count * 1.5)  # å¢åŠ 50%
        
        # éæ¡æ¨£
        oversampled_files = []
        for img_file in self.image_files:
            oversampled_files.append(img_file)
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦éæ¡æ¨£
            label_file = self.labels_dir / f"{img_file.stem}.txt"
            if label_file.exists():
                with open(label_file, 'r') as f:
                    lines = f.readlines()
                    if len(lines) >= 1:
                        try:
                            class_id = int(lines[0].split()[0])
                            current_count = class_counts[class_id]
                            
                            # è¨ˆç®—éæ¡æ¨£å€æ•¸
                            if current_count < target_count:
                                oversample_factor = max(1, int(target_count / current_count))
                                for _ in range(oversample_factor - 1):
                                    oversampled_files.append(img_file)
                        except:
                            pass
        
        print(f"éæ¡æ¨£å¾Œ: {len(oversampled_files)} å€‹æ¨£æœ¬")
        return oversampled_files
    
    def _get_advanced_transforms(self, split):
        """é«˜ç´šæ•¸æ“šå¢å¼·"""
        if split == 'train':
            return transforms.Compose([
                transforms.Resize((256, 256)),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomVerticalFlip(p=0.3),
                transforms.RandomRotation(degrees=15),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),
                transforms.RandomPerspective(distortion_scale=0.1, p=0.3),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            return transforms.Compose([
                transforms.Resize((256, 256)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        img_file = self.image_files[idx]
        label_file = self.labels_dir / f"{img_file.stem}.txt"
        
        # è¼‰å…¥åœ–åƒ
        image = Image.open(img_file).convert('RGB')
        image = self.transform(image)
        
        # è¼‰å…¥æ¨™ç±¤
        bbox_target = np.zeros(4)
        cls_target = np.zeros(3)
        
        if label_file.exists():
            with open(label_file, 'r') as f:
                lines = f.readlines()
                if len(lines) >= 2:
                    # æª¢æ¸¬æ¨™ç±¤ (ç¬¬ä¸€è¡Œ)
                    bbox_parts = lines[0].strip().split()
                    if len(bbox_parts) >= 5:
                        bbox_target = np.array([float(x) for x in bbox_parts[1:5]])
                    
                    # åˆ†é¡æ¨™ç±¤ (ç¬¬äºŒè¡Œ)
                    cls_parts = lines[1].strip().split()
                    if len(cls_parts) == 3:
                        cls_target = np.array([float(x) for x in cls_parts])
        
        return image, torch.FloatTensor(bbox_target), torch.FloatTensor(cls_target)


class AdvancedTrainer:
    """é«˜ç´šè¨“ç·´å™¨"""
    def __init__(self, model, train_loader, val_loader, device, 
                 bbox_weight=1.0, cls_weight=2.0, learning_rate=1e-4):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.bbox_weight = bbox_weight
        self.cls_weight = cls_weight
        
        # å„ªåŒ–å™¨ - ä½¿ç”¨AdamW
        self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
        
        # å­¸ç¿’ç‡èª¿åº¦å™¨ - ä½¿ç”¨CosineAnnealingWarmRestarts
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, T_0=10, T_mult=2, eta_min=1e-6
        )
        
        # æå¤±å‡½æ•¸
        self.bbox_loss = nn.SmoothL1Loss()
        self.cls_loss = AdvancedFocalLoss(alpha=1, gamma=2)
        
        # æ—©åœ
        self.best_val_loss = float('inf')
        self.patience = 15
        self.patience_counter = 0
    
    def train_epoch(self):
        """è¨“ç·´ä¸€å€‹epoch"""
        self.model.train()
        total_loss = 0
        bbox_loss_sum = 0
        cls_loss_sum = 0
        
        for batch_idx, (images, bbox_targets, cls_targets) in enumerate(self.train_loader):
            images = images.to(self.device)
            bbox_targets = bbox_targets.to(self.device)
            cls_targets = cls_targets.to(self.device)
            
            # å‰å‘å‚³æ’­
            bbox_preds, cls_preds = self.model(images)
            
            # è¨ˆç®—æå¤±
            bbox_loss = self.bbox_loss(bbox_preds.view(-1, 4), bbox_targets)
            cls_loss = self.cls_loss(cls_preds, cls_targets)
            
            total_loss = self.bbox_weight * bbox_loss + self.cls_weight * cls_loss
            
            # åå‘å‚³æ’­
            self.optimizer.zero_grad()
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            bbox_loss_sum += bbox_loss.item()
            cls_loss_sum += cls_loss.item()
            
            if batch_idx % 50 == 0:
                print(f"Batch {batch_idx}/{len(self.train_loader)}, "
                      f"Loss: {total_loss.item():.4f} "
                      f"(Bbox: {bbox_loss.item():.4f}, Cls: {cls_loss.item():.4f})")
        
        return bbox_loss_sum / len(self.train_loader), cls_loss_sum / len(self.train_loader)
    
    def validate(self):
        """é©—è­‰"""
        self.model.eval()
        total_loss = 0
        bbox_loss_sum = 0
        cls_loss_sum = 0
        correct_predictions = 0
        total_predictions = 0
        
        with torch.no_grad():
            for images, bbox_targets, cls_targets in self.val_loader:
                images = images.to(self.device)
                bbox_targets = bbox_targets.to(self.device)
                cls_targets = cls_targets.to(self.device)
                
                # å‰å‘å‚³æ’­
                bbox_preds, cls_preds = self.model(images)
                
                # è¨ˆç®—æå¤±
                bbox_loss = self.bbox_loss(bbox_preds.view(-1, 4), bbox_targets)
                cls_loss = self.cls_loss(cls_preds, cls_targets)
                
                total_loss = self.bbox_weight * bbox_loss + self.cls_weight * cls_loss
                
                bbox_loss_sum += bbox_loss.item()
                cls_loss_sum += cls_loss.item()
                
                # è¨ˆç®—åˆ†é¡æº–ç¢ºç‡
                pred_labels = torch.argmax(cls_preds, dim=1)
                true_labels = torch.argmax(cls_targets, dim=1)
                correct_predictions += (pred_labels == true_labels).sum().item()
                total_predictions += pred_labels.size(0)
        
        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
        
        return (bbox_loss_sum / len(self.val_loader), 
                cls_loss_sum / len(self.val_loader), 
                accuracy)
    
    def train(self, num_epochs):
        """å®Œæ•´è¨“ç·´æµç¨‹"""
        print("ğŸš€ é–‹å§‹é«˜ç´šå„ªåŒ–è¨“ç·´...")
        
        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch+1}/{num_epochs}")
            
            # è¨“ç·´
            train_bbox_loss, train_cls_loss = self.train_epoch()
            
            # é©—è­‰
            val_bbox_loss, val_cls_loss, val_accuracy = self.validate()
            
            # å­¸ç¿’ç‡èª¿åº¦
            self.scheduler.step()
            
            print(f"Train - Bbox Loss: {train_bbox_loss:.4f}, Cls Loss: {train_cls_loss:.4f}")
            print(f"Val - Bbox Loss: {val_bbox_loss:.4f}, Cls Loss: {val_cls_loss:.4f}, Accuracy: {val_accuracy:.4f}")
            
            # æ—©åœæª¢æŸ¥
            val_loss = val_bbox_loss + val_cls_loss
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(self.model.state_dict(), 'best_optimized_model.pth')
                print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (val_loss: {val_loss:.4f})")
            else:
                self.patience_counter += 1
                if self.patience_counter >= self.patience:
                    print(f"ğŸ›‘ æ—©åœè§¸ç™¼ (patience: {self.patience})")
                    break


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='70%æ€§èƒ½æå‡å„ªåŒ–è¨ˆåŠƒ')
    parser.add_argument('--data', type=str, required=True, help='æ•¸æ“šé…ç½®æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--epochs', type=int, default=50, help='è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--batch-size', type=int, default=16, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning-rate', type=float, default=1e-4, help='å­¸ç¿’ç‡')
    parser.add_argument('--bbox-weight', type=float, default=1.0, help='æª¢æ¸¬æå¤±æ¬Šé‡')
    parser.add_argument('--cls-weight', type=float, default=2.0, help='åˆ†é¡æå¤±æ¬Šé‡')
    
    args = parser.parse_args()
    
    # è¨­ç½®è¨­å‚™
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    # è¼‰å…¥æ•¸æ“š
    data_dir = Path(args.data).parent
    
    train_dataset = AdvancedDataset(data_dir, split='train', nc=4)
    val_dataset = AdvancedDataset(data_dir, split='val', nc=4, oversample_minority=False)
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)
    
    # å‰µå»ºæ¨¡å‹
    model = AdvancedJointModel(num_detection_classes=4, num_classification_classes=3)
    model.to(device)
    
    # å‰µå»ºè¨“ç·´å™¨
    trainer = AdvancedTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        device=device,
        bbox_weight=args.bbox_weight,
        cls_weight=args.cls_weight,
        learning_rate=args.learning_rate
    )
    
    # é–‹å§‹è¨“ç·´
    trainer.train(args.epochs)
    
    print("ğŸ‰ å„ªåŒ–è¨“ç·´å®Œæˆï¼")


if __name__ == "__main__":
    main() 