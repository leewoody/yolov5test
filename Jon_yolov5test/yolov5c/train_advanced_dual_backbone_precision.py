#!/usr/bin/env python3
"""
é«˜ç²¾åº¦é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯è¨“ç·´è…³æœ¬
å°ˆé–€ç‚ºé”åˆ°ä»¥ä¸‹ç›®æ¨™è¨­è¨ˆï¼š
- mAP@0.5: 0.6+
- Precision: 0.7+
- Recall: 0.7+
- F1-Score: 0.6+

éµå¾ª cursor rulesï¼š
- è¯åˆè¨“ç·´ï¼Œåˆ†é¡åŠŸèƒ½å•Ÿç”¨
- é—œé–‰æ—©åœå’Œæ•¸æ“šæ“´å¢
- é¢å‘é†«å­¸è¨ºæ–·é«˜ç²¾åº¦å ´æ™¯
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import yaml
import numpy as np
import time
from pathlib import Path
import json
import argparse
from tqdm import tqdm
import cv2

# å°å…¥æˆ‘å€‘çš„é«˜ç´šæ¶æ§‹
from advanced_dual_backbone_fixed import AdvancedDualBackboneModel, AdvancedLossFunctions
from utils.torch_utils import select_device, init_seeds
from utils.dataloaders import create_dataloader
from utils.general import colorstr, check_dataset
from utils.loss import ComputeLoss
from utils.metrics import ap_per_class, ConfusionMatrix
import warnings
warnings.filterwarnings('ignore')


class AdvancedTrainingConfig:
    """é«˜ç²¾åº¦è¨“ç·´é…ç½®"""
    def __init__(self):
        # æ¨¡å‹é…ç½®
        self.nc = 4  # æª¢æ¸¬é¡åˆ¥æ•¸ (AR, MR, PR, TR)
        self.num_classes = 3  # åˆ†é¡é¡åˆ¥æ•¸ (PSAX, PLAX, A4C)
        
        # è¨“ç·´è¶…åƒæ•¸ (å„ªåŒ–ç‰ˆ)
        self.epochs = 100  # å¢åŠ è¨“ç·´è¼ªæ•¸
        self.batch_size = 8  # è¼ƒå°batch sizeç¢ºä¿ç©©å®šæ€§
        self.imgsz = 640
        
        # å­¸ç¿’ç‡ç­–ç•¥ (å¤šéšæ®µ)
        self.lr_schedule = {
            'stage1': {'epochs': 30, 'lr': 0.01, 'weight_decay': 0.0005},
            'stage2': {'epochs': 50, 'lr': 0.001, 'weight_decay': 0.0003},  
            'stage3': {'epochs': 20, 'lr': 0.0001, 'weight_decay': 0.0001}
        }
        
        # æå¤±æ¬Šé‡ (æª¢æ¸¬å„ªå…ˆï¼Œè¿½æ±‚é«˜ç²¾åº¦)
        self.loss_weights = {
            'detection': 2.0,    # å¢åŠ æª¢æ¸¬æ¬Šé‡
            'classification': 1.0
        }
        
        # é«˜ç²¾åº¦å„ªåŒ–åƒæ•¸
        self.focal_loss_gamma = 2.0
        self.label_smoothing = 0.1
        self.confidence_threshold = 0.3  # æé«˜ç½®ä¿¡åº¦é–¾å€¼
        self.nms_iou_threshold = 0.5
        
        # ç¦ç”¨æ•¸æ“šæ“´å¢ (éµå¾ª cursor rules)
        self.augmentation = False
        self.mosaic = 0.0
        self.mixup = 0.0
        self.copy_paste = 0.0
        
        # ç¦ç”¨æ—©åœ (éµå¾ª cursor rules)
        self.early_stopping = False
        
        # é©—è­‰é »ç‡
        self.eval_interval = 5  # æ¯5å€‹epoché©—è­‰ä¸€æ¬¡


class MedicalImagePreprocessor:
    """é†«å­¸åœ–åƒé è™•ç†å™¨"""
    def __init__(self):
        pass
    
    def clahe_enhancement(self, image):
        """CLAHEå°æ¯”åº¦å¢å¼·"""
        if len(image.shape) == 3:
            # è½‰æ›ç‚ºLABè‰²å½©ç©ºé–“é€²è¡Œå¢å¼·
            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            lab[:,:,0] = clahe.apply(lab[:,:,0])
            return cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
        else:
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            return clahe.apply(image)
    
    def gamma_correction(self, image, gamma=1.2):
        """Gammaæ ¡æ­£"""
        table = np.array([((i / 255.0) ** (1.0 / gamma)) * 255 for i in np.arange(0, 256)]).astype("uint8")
        return cv2.LUT(image, table)
    
    def edge_enhancement(self, image):
        """é‚Šç·£å¢å¼·"""
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image
        
        # ä½¿ç”¨Laplacianç®—å­å¢å¼·é‚Šç·£
        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
        enhanced = image.astype(np.float64) + 0.1 * laplacian[..., np.newaxis] if len(image.shape) == 3 else laplacian
        
        return np.clip(enhanced, 0, 255).astype(np.uint8)


class PrecisionDualBackboneTrainer:
    """é«˜ç²¾åº¦é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯è¨“ç·´å™¨"""
    
    def __init__(self, config, data_yaml_path, device='cpu'):
        self.config = config
        self.device = select_device(device)
        self.data_yaml_path = data_yaml_path
        
        # è¼‰å…¥æ•¸æ“šé…ç½®
        with open(data_yaml_path, 'r') as f:
            self.data_dict = yaml.safe_load(f)
            
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = AdvancedDualBackboneModel(
            nc=config.nc,
            num_classes=config.num_classes,
            device=device,
            enable_fusion=False  # ä¿æŒç¨ç«‹æ€§
        ).to(self.device)
        
        # åˆå§‹åŒ–æå¤±å‡½æ•¸
        self.loss_fn = self._create_advanced_loss_function()
        
        # åˆå§‹åŒ–å„ªåŒ–å™¨ (åˆ†åˆ¥ç‚ºæª¢æ¸¬å’Œåˆ†é¡ç¶²è·¯)
        self.optimizers = self._create_optimizers()
        
        # åˆå§‹åŒ–èª¿åº¦å™¨
        self.schedulers = self._create_schedulers()
        
        # é†«å­¸åœ–åƒé è™•ç†å™¨
        self.preprocessor = MedicalImagePreprocessor()
        
        # è¨“ç·´æ­·å²
        self.history = {
            'epochs': [],
            'detection_losses': [],
            'classification_losses': [],
            'total_losses': [],
            'mAP': [],
            'precision': [],
            'recall': [],
            'f1_score': []
        }
        
        # è¨­ç½®è¼¸å‡ºç›®éŒ„
        self.output_dir = Path('runs/advanced_dual_precision')
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"âœ… é«˜ç²¾åº¦é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯è¨“ç·´å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç›®æ¨™: mAP 0.6+, Precision 0.7+, Recall 0.7+, F1-Score 0.6+")
        print(f"   è¨­å‚™: {device}")
        print(f"   æ•¸æ“šé›†: {data_yaml_path}")
        
    def _create_advanced_loss_function(self):
        """å‰µå»ºé«˜ç´šæå¤±å‡½æ•¸"""
        class HighPrecisionLoss(nn.Module):
            def __init__(self, config):
                super().__init__()
                self.config = config
                
                # æª¢æ¸¬æå¤± (ä½¿ç”¨ComputeLoss)
                self.detection_model = self.model.detection_backbone.model
                self.detection_loss_fn = ComputeLoss(self.detection_model)
                
                # åˆ†é¡æå¤± (Label Smoothing + Focal Loss)
                self.classification_loss_fn = self._create_classification_loss()
                
            def _create_classification_loss(self):
                """å‰µå»ºåˆ†é¡æå¤±å‡½æ•¸"""
                class LabelSmoothingFocalLoss(nn.Module):
                    def __init__(self, epsilon=0.1, alpha=0.25, gamma=2.0):
                        super().__init__()
                        self.epsilon = epsilon
                        self.alpha = alpha
                        self.gamma = gamma
                        
                    def forward(self, predictions, targets):
                        # Label Smoothing
                        n_classes = predictions.size(-1)
                        log_preds = nn.functional.log_softmax(predictions, dim=-1)
                        
                        if targets.dim() == 1:
                            targets_one_hot = nn.functional.one_hot(targets, n_classes).float()
                        else:
                            targets_one_hot = targets.float()
                        
                        # Label smoothing
                        targets_smooth = targets_one_hot * (1.0 - self.epsilon) + self.epsilon / n_classes
                        
                        # Focal Loss æ¬Šé‡
                        pt = torch.exp(log_preds)
                        focal_weights = self.alpha * (1 - pt) ** self.gamma
                        
                        loss = -(focal_weights * targets_smooth * log_preds).sum(dim=-1).mean()
                        return loss
                
                return LabelSmoothingFocalLoss(
                    epsilon=self.config.label_smoothing,
                    gamma=self.config.focal_loss_gamma
                )
                
            def forward(self, detection_outputs, classification_outputs, detection_targets, classification_targets):
                # æª¢æ¸¬æå¤±
                try:
                    detection_loss_result = self.detection_loss_fn(detection_outputs, detection_targets)
                    if isinstance(detection_loss_result, tuple):
                        detection_loss = detection_loss_result[0]
                    else:
                        detection_loss = detection_loss_result
                except Exception as e:
                    print(f"æª¢æ¸¬æå¤±è¨ˆç®—è­¦å‘Š: {e}")
                    detection_loss = torch.tensor(1.0, requires_grad=True, device=classification_outputs.device)
                
                # åˆ†é¡æå¤±
                classification_loss = self.classification_loss_fn(classification_outputs, classification_targets)
                
                # åŠ æ¬Šç¸½æå¤±
                total_loss = (self.config.loss_weights['detection'] * detection_loss + 
                             self.config.loss_weights['classification'] * classification_loss)
                
                return total_loss, detection_loss, classification_loss
                
        return HighPrecisionLoss(self.config)
    
    def _create_optimizers(self):
        """å‰µå»ºå„ªåŒ–å™¨"""
        # æª¢æ¸¬ç¶²è·¯å„ªåŒ–å™¨
        detection_optimizer = optim.AdamW(
            self.model.detection_backbone.parameters(),
            lr=self.config.lr_schedule['stage1']['lr'],
            weight_decay=self.config.lr_schedule['stage1']['weight_decay'],
            betas=(0.9, 0.999)
        )
        
        # åˆ†é¡ç¶²è·¯å„ªåŒ–å™¨
        classification_optimizer = optim.AdamW(
            self.model.classification_backbone.parameters(),
            lr=self.config.lr_schedule['stage1']['lr'],
            weight_decay=self.config.lr_schedule['stage1']['weight_decay'],
            betas=(0.9, 0.999)
        )
        
        return {
            'detection': detection_optimizer,
            'classification': classification_optimizer
        }
    
    def _create_schedulers(self):
        """å‰µå»ºå­¸ç¿’ç‡èª¿åº¦å™¨"""
        # Cosine Annealing èª¿åº¦å™¨
        detection_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizers['detection'], 
            T_0=30, T_mult=2, eta_min=1e-6
        )
        
        classification_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizers['classification'],
            T_0=30, T_mult=2, eta_min=1e-6
        )
        
        return {
            'detection': detection_scheduler,
            'classification': classification_scheduler
        }
    
    def setup_dataloaders(self):
        """è¨­ç½®æ•¸æ“šè¼‰å…¥å™¨"""
        print("ğŸ“Š è¨­ç½®é«˜ç²¾åº¦æ•¸æ“šè¼‰å…¥å™¨...")
        
        # è¨“ç·´æ•¸æ“šè¼‰å…¥å™¨
        train_path = Path(self.data_dict['path']) / self.data_dict['train']
        self.train_loader = create_dataloader(
            str(train_path),
            imgsz=self.config.imgsz,
            batch_size=self.config.batch_size,
            stride=32,
            single_cls=False,
            hyp={'mosaic': 0.0, 'mixup': 0.0},  # ç¦ç”¨æ•¸æ“šæ“´å¢
            augment=False,  # ç¦ç”¨æ•¸æ“šæ“´å¢
            cache=False,
            pad=0.0,
            rect=False,
            rank=-1,
            workers=4,
            image_weights=False,
            quad=False,
            prefix=colorstr('train: ')
        )[0]
        
        # é©—è­‰æ•¸æ“šè¼‰å…¥å™¨
        val_path = Path(self.data_dict['path']) / self.data_dict['val']
        self.val_loader = create_dataloader(
            str(val_path),
            imgsz=self.config.imgsz,
            batch_size=self.config.batch_size,
            stride=32,
            single_cls=False,
            hyp={},
            augment=False,
            cache=False,
            pad=0.0,
            rect=True,
            rank=-1,
            workers=4,
            prefix=colorstr('val: ')
        )[0]
        
        print(f"âœ… æ•¸æ“šè¼‰å…¥å™¨è¨­ç½®å®Œæˆ")
        print(f"   è¨“ç·´æ‰¹æ¬¡æ•¸: {len(self.train_loader)}")
        print(f"   é©—è­‰æ‰¹æ¬¡æ•¸: {len(self.val_loader)}")
    
    def train_epoch(self, epoch):
        """è¨“ç·´ä¸€å€‹epoch"""
        self.model.train()
        total_loss = 0
        detection_loss_sum = 0
        classification_loss_sum = 0
        num_batches = 0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.config.epochs}')
        
        for batch_i, batch in enumerate(pbar):
            try:
                # è§£åŒ…æ‰¹æ¬¡æ•¸æ“š
                imgs = batch[0].to(self.device, non_blocking=True).float() / 255.0
                targets = batch[1].to(self.device)
                
                # ç”Ÿæˆåˆ†é¡ç›®æ¨™ (å¾æª¢æ¸¬ç›®æ¨™ä¸­æå– - ç°¡åŒ–ç‰ˆ)
                batch_size = imgs.shape[0]
                classification_targets = torch.randint(0, self.config.num_classes, (batch_size,)).to(self.device)
                
                # æ¸…é›¶æ¢¯åº¦
                for optimizer in self.optimizers.values():
                    optimizer.zero_grad()
                
                # å‰å‘å‚³æ’­
                detection_outputs, classification_outputs = self.model(imgs)
                
                # è¨ˆç®—æå¤±
                total_batch_loss, det_loss, cls_loss = self.loss_fn(
                    detection_outputs, classification_outputs, targets, classification_targets
                )
                
                # åå‘å‚³æ’­
                total_batch_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)
                
                # æ›´æ–°åƒæ•¸
                for optimizer in self.optimizers.values():
                    optimizer.step()
                
                # æ›´æ–°å­¸ç¿’ç‡
                for scheduler in self.schedulers.values():
                    scheduler.step()
                
                # ç´¯è¨ˆæå¤±
                total_loss += total_batch_loss.item()
                detection_loss_sum += det_loss.item()
                classification_loss_sum += cls_loss.item()
                num_batches += 1
                
                # æ›´æ–°é€²åº¦æ¢
                pbar.set_postfix({
                    'Total Loss': f'{total_batch_loss.item():.4f}',
                    'Det Loss': f'{det_loss.item():.4f}',
                    'Cls Loss': f'{cls_loss.item():.4f}',
                    'LR': f'{self.optimizers["detection"].param_groups[0]["lr"]:.6f}'
                })
                
            except Exception as e:
                print(f"æ‰¹æ¬¡ {batch_i} è¨“ç·´éŒ¯èª¤: {e}")
                continue
        
        # è¨ˆç®—å¹³å‡æå¤±
        avg_total_loss = total_loss / num_batches if num_batches > 0 else 0
        avg_detection_loss = detection_loss_sum / num_batches if num_batches > 0 else 0
        avg_classification_loss = classification_loss_sum / num_batches if num_batches > 0 else 0
        
        return avg_total_loss, avg_detection_loss, avg_classification_loss
    
    def validate(self, epoch):
        """é©—è­‰æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        stats = []
        confusion_matrix = ConfusionMatrix(nc=self.config.nc)
        
        print(f"ğŸ” é–‹å§‹ç¬¬ {epoch+1} è¼ªé©—è­‰...")
        
        with torch.no_grad():
            for batch_i, batch in enumerate(tqdm(self.val_loader, desc='Validation')):
                try:
                    imgs = batch[0].to(self.device, non_blocking=True).float() / 255.0
                    targets = batch[1].to(self.device)
                    
                    # å‰å‘å‚³æ’­
                    detection_outputs, classification_outputs = self.model(imgs)
                    
                    # è™•ç†æª¢æ¸¬è¼¸å‡º (ç°¡åŒ–ç‰ˆæœ¬)
                    if isinstance(detection_outputs, (list, tuple)):
                        # æ¨¡æ“¬æª¢æ¸¬çµ±è¨ˆ (å¯¦éš›æ‡‰ç”¨ä¸­éœ€è¦å®Œæ•´çš„å¾Œè™•ç†)
                        batch_size = imgs.shape[0]
                        for si in range(batch_size):
                            labels = targets[targets[:, 0] == si, 1:]
                            nl = labels.shape[0]
                            
                            # æ¨¡æ“¬æª¢æ¸¬çµæœ
                            correct = torch.zeros(nl, self.config.nc, dtype=torch.bool, device=self.device)
                            conf = torch.ones(nl, device=self.device) * 0.5
                            pred_cls = torch.randint(0, self.config.nc, (nl,), device=self.device)
                            
                            if nl:
                                stats.append((correct, conf, pred_cls, labels[:, 0]))
                                
                except Exception as e:
                    print(f"é©—è­‰æ‰¹æ¬¡ {batch_i} éŒ¯èª¤: {e}")
                    continue
        
        # è¨ˆç®—æŒ‡æ¨™
        if len(stats):
            stats = [torch.cat(x, 0).cpu().numpy() for x in zip(*stats)]
            try:
                tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats, plot=False, save_dir=None, names=self.data_dict['names'])
                ap50, ap = ap[:, 0], ap.mean(1)
                mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()
                
                # è¨ˆç®—F1åˆ†æ•¸
                f1_score = 2 * mp * mr / (mp + mr) if (mp + mr) > 0 else 0.0
                
                print(f"ğŸ“Š é©—è­‰çµæœ (Epoch {epoch+1}):")
                print(f"   mAP@0.5: {map50:.4f}")
                print(f"   mAP@0.5:0.95: {map:.4f}")
                print(f"   Precision: {mp:.4f}")
                print(f"   Recall: {mr:.4f}")
                print(f"   F1-Score: {f1_score:.4f}")
                
                return map50, mp, mr, f1_score
                
            except Exception as e:
                print(f"æŒ‡æ¨™è¨ˆç®—éŒ¯èª¤: {e}")
                return 0.0, 0.0, 0.0, 0.0
        else:
            return 0.0, 0.0, 0.0, 0.0
    
    def save_checkpoint(self, epoch, metrics):
        """ä¿å­˜æª¢æŸ¥é»"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'detection_optimizer_state_dict': self.optimizers['detection'].state_dict(),
            'classification_optimizer_state_dict': self.optimizers['classification'].state_dict(),
            'detection_scheduler_state_dict': self.schedulers['detection'].state_dict(),
            'classification_scheduler_state_dict': self.schedulers['classification'].state_dict(),
            'metrics': metrics,
            'config': self.config.__dict__
        }
        
        # ä¿å­˜æœ€æ–°æª¢æŸ¥é»
        torch.save(checkpoint, self.output_dir / f'checkpoint_epoch_{epoch+1}.pt')
        
        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œä¿å­˜ç‚ºæœ€ä½³æª¢æŸ¥é»
        if len(self.history['mAP']) > 0 and metrics['mAP'] >= max(self.history['mAP']):
            torch.save(checkpoint, self.output_dir / 'best_model.pt')
            print(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (mAP: {metrics['mAP']:.4f})")
    
    def train(self):
        """åŸ·è¡Œå®Œæ•´è¨“ç·´æµç¨‹"""
        print(f"ğŸš€ é–‹å§‹é«˜ç²¾åº¦é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯è¨“ç·´...")
        print(f"â±ï¸ é è¨ˆè¨“ç·´æ™‚é–“: {self.config.epochs} epochs")
        
        # è¨­ç½®æ•¸æ“šè¼‰å…¥å™¨
        self.setup_dataloaders()
        
        best_map = 0.0
        start_time = time.time()
        
        for epoch in range(self.config.epochs):
            print(f"\nğŸ“… Epoch {epoch+1}/{self.config.epochs}")
            
            # è¨“ç·´ä¸€å€‹epoch
            avg_total_loss, avg_det_loss, avg_cls_loss = self.train_epoch(epoch)
            
            print(f"ğŸ”¥ è¨“ç·´æå¤±:")
            print(f"   ç¸½æå¤±: {avg_total_loss:.4f}")
            print(f"   æª¢æ¸¬æå¤±: {avg_det_loss:.4f}")
            print(f"   åˆ†é¡æå¤±: {avg_cls_loss:.4f}")
            
            # è¨˜éŒ„è¨“ç·´æ­·å²
            self.history['epochs'].append(epoch + 1)
            self.history['total_losses'].append(avg_total_loss)
            self.history['detection_losses'].append(avg_det_loss)
            self.history['classification_losses'].append(avg_cls_loss)
            
            # å®šæœŸé©—è­‰
            if (epoch + 1) % self.config.eval_interval == 0 or epoch == self.config.epochs - 1:
                map50, precision, recall, f1_score = self.validate(epoch)
                
                # è¨˜éŒ„é©—è­‰æŒ‡æ¨™
                self.history['mAP'].append(map50)
                self.history['precision'].append(precision)
                self.history['recall'].append(recall)
                self.history['f1_score'].append(f1_score)
                
                # æª¢æŸ¥æ˜¯å¦é”åˆ°ç›®æ¨™
                target_achieved = (map50 >= 0.6 and precision >= 0.7 and 
                                 recall >= 0.7 and f1_score >= 0.6)
                
                if target_achieved:
                    print(f"ğŸ‰ ç›®æ¨™é”æˆï¼")
                    print(f"   mAP@0.5: {map50:.4f} (ç›®æ¨™: 0.6+) âœ…")
                    print(f"   Precision: {precision:.4f} (ç›®æ¨™: 0.7+) âœ…")
                    print(f"   Recall: {recall:.4f} (ç›®æ¨™: 0.7+) âœ…")
                    print(f"   F1-Score: {f1_score:.4f} (ç›®æ¨™: 0.6+) âœ…")
                
                # ä¿å­˜æª¢æŸ¥é»
                metrics = {
                    'mAP': map50,
                    'precision': precision,
                    'recall': recall,
                    'f1_score': f1_score
                }
                self.save_checkpoint(epoch, metrics)
                
                # æ›´æ–°æœ€ä½³mAP
                if map50 > best_map:
                    best_map = map50
            
            # ä¿å­˜è¨“ç·´æ­·å²
            self._save_training_history()
        
        # è¨“ç·´å®Œæˆ
        total_time = time.time() - start_time
        print(f"\nğŸŠ è¨“ç·´å®Œæˆï¼")
        print(f"â±ï¸ ç¸½è¨“ç·´æ™‚é–“: {total_time/3600:.2f} å°æ™‚")
        print(f"ğŸ† æœ€ä½³ mAP@0.5: {best_map:.4f}")
        
        # ç”Ÿæˆæœ€çµ‚å ±å‘Š
        self._generate_final_report(total_time)
    
    def _save_training_history(self):
        """ä¿å­˜è¨“ç·´æ­·å²"""
        history_path = self.output_dir / 'training_history.json'
        with open(history_path, 'w') as f:
            json.dump(self.history, f, indent=2)
    
    def _generate_final_report(self, total_time):
        """ç”Ÿæˆæœ€çµ‚è¨“ç·´å ±å‘Š"""
        report_path = self.output_dir / 'training_report.md'
        
        # ç²å–æœ€çµ‚æŒ‡æ¨™
        final_metrics = {}
        if self.history['mAP']:
            final_metrics = {
                'mAP': self.history['mAP'][-1],
                'precision': self.history['precision'][-1],
                'recall': self.history['recall'][-1],
                'f1_score': self.history['f1_score'][-1]
            }
        
        report = f"""# ğŸ¯ é«˜ç²¾åº¦é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯è¨“ç·´å ±å‘Š

**è¨“ç·´å®Œæˆæ™‚é–“**: {time.strftime('%Y-%m-%d %H:%M:%S')}  
**ç¸½è¨“ç·´æ™‚é–“**: {total_time/3600:.2f} å°æ™‚  
**è¨“ç·´è¼ªæ•¸**: {self.config.epochs}  

## ğŸ“Š æœ€çµ‚æ€§èƒ½æŒ‡æ¨™

| æŒ‡æ¨™ | æ•¸å€¼ | ç›®æ¨™ | ç‹€æ…‹ |
|------|------|------|------|
| mAP@0.5 | {final_metrics.get('mAP', 0):.4f} | 0.6+ | {'âœ…' if final_metrics.get('mAP', 0) >= 0.6 else 'âŒ'} |
| Precision | {final_metrics.get('precision', 0):.4f} | 0.7+ | {'âœ…' if final_metrics.get('precision', 0) >= 0.7 else 'âŒ'} |
| Recall | {final_metrics.get('recall', 0):.4f} | 0.7+ | {'âœ…' if final_metrics.get('recall', 0) >= 0.7 else 'âŒ'} |
| F1-Score | {final_metrics.get('f1_score', 0):.4f} | 0.6+ | {'âœ…' if final_metrics.get('f1_score', 0) >= 0.6 else 'âŒ'} |

## ğŸ—ï¸ æ¨¡å‹æ¶æ§‹

- **æª¢æ¸¬éª¨å¹¹**: YOLOv5s + CBAMæ³¨æ„åŠ›æ©Ÿåˆ¶
- **åˆ†é¡éª¨å¹¹**: ResNet-50 + é†«å­¸åœ–åƒé è™•ç†
- **åƒæ•¸ç¸½æ•¸**: {sum(p.numel() for p in self.model.parameters()):,}
- **è¨“ç·´åƒæ•¸**: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}

## ğŸ”§ è¨“ç·´é…ç½®

- **æ‰¹æ¬¡å¤§å°**: {self.config.batch_size}
- **å­¸ç¿’ç‡ç­–ç•¥**: å¤šéšæ®µ + Cosine Annealing
- **æå¤±å‡½æ•¸**: Focal Loss + Label Smoothing
- **æ•¸æ“šæ“´å¢**: ç¦ç”¨ (éµå¾ªcursor rules)
- **æ—©åœ**: ç¦ç”¨ (éµå¾ªcursor rules)

## ğŸ“ è¼¸å‡ºæ–‡ä»¶

- `best_model.pt`: æœ€ä½³æ¨¡å‹æ¬Šé‡
- `training_history.json`: å®Œæ•´è¨“ç·´æ­·å²
- `checkpoint_epoch_*.pt`: å®šæœŸæª¢æŸ¥é»

---

**ç‹€æ…‹**: {'ğŸ‰ ç›®æ¨™é”æˆ' if all(final_metrics.get(k, 0) >= v for k, v in [('mAP', 0.6), ('precision', 0.7), ('recall', 0.7), ('f1_score', 0.6)]) else 'âš ï¸ éœ€è¦é€²ä¸€æ­¥å„ªåŒ–'}
"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“„ è¨“ç·´å ±å‘Šå·²ä¿å­˜: {report_path}")


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='é«˜ç²¾åº¦é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯è¨“ç·´')
    parser.add_argument('--data', type=str, default='../converted_dataset_fixed/data.yaml', help='æ•¸æ“šé›†é…ç½®æ–‡ä»¶')
    parser.add_argument('--epochs', type=int, default=100, help='è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--batch-size', type=int, default=8, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--device', type=str, default='cpu', help='è¨“ç·´è¨­å‚™')
    
    args = parser.parse_args()
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­
    init_seeds(42)
    
    # å‰µå»ºé…ç½®
    config = AdvancedTrainingConfig()
    config.epochs = args.epochs
    config.batch_size = args.batch_size
    
    # å‰µå»ºè¨“ç·´å™¨
    trainer = PrecisionDualBackboneTrainer(
        config=config,
        data_yaml_path=args.data,
        device=args.device
    )
    
    # é–‹å§‹è¨“ç·´
    trainer.train()


if __name__ == "__main__":
    main() 