#!/usr/bin/env python3
"""
ğŸš€ YOLOv5WithClassification æ¨¡å‹éƒ¨ç½²æ‰“åŒ…å·¥å…·
Purpose: Package final trained models for deployment
Author: YOLOv5WithClassification Project
Date: 2025-08-06
"""

import os
import shutil
import json
import zipfile
from pathlib import Path
from datetime import datetime
import yaml

class ModelDeploymentPackager:
    def __init__(self):
        self.project_root = Path("/Users/mac/_codes/_Github/yolov5test")
        self.output_dir = Path("deploy_package")
        self.output_dir.mkdir(exist_ok=True)
        
        print("ğŸš€ YOLOv5WithClassification æ¨¡å‹éƒ¨ç½²æ‰“åŒ…å·¥å…·")
        print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {self.output_dir.absolute()}")
        
    def find_trained_models(self):
        """æŸ¥æ‰¾æ‰€æœ‰å·²è¨“ç·´çš„æ¨¡å‹"""
        print("\nğŸ” æœç´¢å·²è¨“ç·´æ¨¡å‹...")
        
        model_locations = {
            "gradnorm_models": [],
            "dual_backbone_models": [],
            "other_models": []
        }
        
        # å®šç¾©æ¨¡å‹æœç´¢è·¯å¾‘
        search_paths = [
            self.project_root / "Jon_yolov5test" / "yolov5c" / "runs",
            self.project_root / "auto_training_run_*",
            self.project_root,
        ]
        
        # æœç´¢GradNormæ¨¡å‹
        gradnorm_patterns = [
            "**/final_fixed_gradnorm_model.pt",
            "**/final_gradnorm_model.pt", 
            "**/gradnorm_*.pt*",
            "**/checkpoint_*gradnorm*.pt"
        ]
        
        # æœç´¢é›™ç¨ç«‹éª¨å¹¹æ¨¡å‹
        dual_backbone_patterns = [
            "**/final_simple_dual_backbone_model.pt",
            "**/dual_backbone_*.pt",
            "**/final_dual_backbone*.pt"
        ]
        
        # å…¶ä»–é‡è¦æ¨¡å‹
        other_patterns = [
            "**/joint_model_*.pth",
            "**/best_model.pth",
            "**/final_model.pth"
        ]
        
        # åŸ·è¡Œæœç´¢
        for search_path in search_paths:
            if "*" in str(search_path):
                # è™•ç†é€šé…ç¬¦è·¯å¾‘
                for path in self.project_root.glob(search_path.name):
                    if path.is_dir():
                        self._search_in_path(path, gradnorm_patterns, model_locations["gradnorm_models"])
                        self._search_in_path(path, dual_backbone_patterns, model_locations["dual_backbone_models"])
                        self._search_in_path(path, other_patterns, model_locations["other_models"])
            else:
                if search_path.exists():
                    self._search_in_path(search_path, gradnorm_patterns, model_locations["gradnorm_models"])
                    self._search_in_path(search_path, dual_backbone_patterns, model_locations["dual_backbone_models"])
                    self._search_in_path(search_path, other_patterns, model_locations["other_models"])
        
        return model_locations
    
    def _search_in_path(self, search_path, patterns, result_list):
        """åœ¨æŒ‡å®šè·¯å¾‘ä¸­æœç´¢æ¨¡å‹æ–‡ä»¶"""
        for pattern in patterns:
            for model_file in search_path.glob(pattern):
                if model_file.is_file():
                    result_list.append(model_file)
    
    def package_deployment_models(self):
        """æ‰“åŒ…éƒ¨ç½²æ¨¡å‹"""
        print("\nğŸ“¦ é–‹å§‹æ‰“åŒ…éƒ¨ç½²æ¨¡å‹...")
        
        # å‰µå»ºéƒ¨ç½²ç›®éŒ„çµæ§‹
        deploy_dirs = {
            "models": self.output_dir / "models",
            "configs": self.output_dir / "configs", 
            "documentation": self.output_dir / "documentation",
            "scripts": self.output_dir / "scripts",
            "test_data": self.output_dir / "test_data"
        }
        
        for dir_path in deploy_dirs.values():
            dir_path.mkdir(exist_ok=True)
        
        # æŸ¥æ‰¾æ¨¡å‹
        model_locations = self.find_trained_models()
        
        # æ‰“åŒ…æ¨¡å‹æ–‡ä»¶
        deployment_info = {
            "package_info": {
                "project_name": "YOLOv5WithClassification",
                "package_date": datetime.now().isoformat(),
                "description": "Heart valve regurgitation detection and classification models",
                "version": "1.0.0"
            },
            "models": {},
            "performance": {}
        }
        
        # è™•ç†GradNormæ¨¡å‹
        print("\nğŸ¤– è™•ç† GradNorm æ¨¡å‹...")
        if model_locations["gradnorm_models"]:
            gradnorm_dir = deploy_dirs["models"] / "gradnorm"
            gradnorm_dir.mkdir(exist_ok=True)
            
            for model_file in model_locations["gradnorm_models"]:
                dest_file = gradnorm_dir / model_file.name
                shutil.copy2(model_file, dest_file)
                print(f"  âœ… {model_file.name}")
                
                deployment_info["models"][f"gradnorm_{model_file.stem}"] = {
                    "path": f"models/gradnorm/{model_file.name}",
                    "size_mb": round(model_file.stat().st_size / 1024 / 1024, 2),
                    "original_path": str(model_file),
                    "type": "GradNorm Joint Training Model"
                }
        
        # è™•ç†é›™ç¨ç«‹éª¨å¹¹æ¨¡å‹
        print("\nğŸ—ï¸ è™•ç†é›™ç¨ç«‹éª¨å¹¹æ¨¡å‹...")
        if model_locations["dual_backbone_models"]:
            dual_dir = deploy_dirs["models"] / "dual_backbone" 
            dual_dir.mkdir(exist_ok=True)
            
            for model_file in model_locations["dual_backbone_models"]:
                dest_file = dual_dir / model_file.name
                shutil.copy2(model_file, dest_file)
                print(f"  âœ… {model_file.name}")
                
                deployment_info["models"][f"dual_backbone_{model_file.stem}"] = {
                    "path": f"models/dual_backbone/{model_file.name}",
                    "size_mb": round(model_file.stat().st_size / 1024 / 1024, 2),
                    "original_path": str(model_file),
                    "type": "Dual Independent Backbone Model"
                }
        
        # è™•ç†å…¶ä»–æ¨¡å‹
        print("\nğŸ“‹ è™•ç†å…¶ä»–é‡è¦æ¨¡å‹...")
        if model_locations["other_models"]:
            other_dir = deploy_dirs["models"] / "other"
            other_dir.mkdir(exist_ok=True)
            
            for model_file in model_locations["other_models"]:
                dest_file = other_dir / model_file.name
                shutil.copy2(model_file, dest_file)
                print(f"  âœ… {model_file.name}")
                
                deployment_info["models"][f"other_{model_file.stem}"] = {
                    "path": f"models/other/{model_file.name}",
                    "size_mb": round(model_file.stat().st_size / 1024 / 1024, 2),
                    "original_path": str(model_file),
                    "type": "Additional Training Model"
                }
        
        # è¤‡è£½é…ç½®æ–‡ä»¶
        print("\nâš™ï¸ è¤‡è£½é…ç½®æ–‡ä»¶...")
        self._copy_config_files(deploy_dirs["configs"])
        
        # è¤‡è£½æ–‡æª”
        print("\nğŸ“š è¤‡è£½é …ç›®æ–‡æª”...")
        self._copy_documentation(deploy_dirs["documentation"])
        
        # è¤‡è£½æ¨ç†è…³æœ¬
        print("\nğŸ”§ è¤‡è£½æ¨ç†è…³æœ¬...")
        self._copy_inference_scripts(deploy_dirs["scripts"])
        
        # è¤‡è£½æ¸¬è©¦æ•¸æ“šæ¨£æœ¬
        print("\nğŸ§ª è¤‡è£½æ¸¬è©¦æ•¸æ“šæ¨£æœ¬...")
        self._copy_test_samples(deploy_dirs["test_data"])
        
        # æ·»åŠ æ€§èƒ½å ±å‘Š
        self._add_performance_reports(deployment_info)
        
        # ä¿å­˜éƒ¨ç½²ä¿¡æ¯
        with open(self.output_dir / "deployment_info.json", "w", encoding="utf-8") as f:
            json.dump(deployment_info, f, indent=2, ensure_ascii=False)
        
        # å‰µå»ºREADME
        self._create_deployment_readme()
        
        return deployment_info
    
    def _copy_config_files(self, config_dir):
        """è¤‡è£½é…ç½®æ–‡ä»¶"""
        config_files = [
            self.project_root / "converted_dataset_fixed" / "data.yaml",
            self.project_root / "Jon_yolov5test" / "yolov5c" / "models" / "yolov5s.yaml",
        ]
        
        for config_file in config_files:
            if config_file.exists():
                dest_file = config_dir / config_file.name
                shutil.copy2(config_file, dest_file)
                print(f"  âœ… {config_file.name}")
    
    def _copy_documentation(self, doc_dir):
        """è¤‡è£½æ–‡æª”"""
        doc_patterns = [
            "**/*REPORT*.md",
            "**/*SUMMARY*.md", 
            "**/*ANALYSIS*.md",
            "**/*GUIDE*.md"
        ]
        
        doc_source = self.project_root / "Jon_yolov5test" / "yolov5c"
        for pattern in doc_patterns:
            for doc_file in doc_source.glob(pattern):
                if doc_file.is_file():
                    dest_file = doc_dir / doc_file.name
                    # æª¢æŸ¥æ˜¯å¦ç‚ºåŒä¸€æ–‡ä»¶ï¼Œé¿å…è¤‡è£½éŒ¯èª¤
                    if doc_file.resolve() != dest_file.resolve():
                        shutil.copy2(doc_file, dest_file)
                        print(f"  âœ… {doc_file.name}")
                    else:
                        print(f"  âš ï¸ è·³éåŒä¸€æ–‡ä»¶: {doc_file.name}")
    
    def _copy_inference_scripts(self, scripts_dir):
        """è¤‡è£½æ¨ç†è…³æœ¬"""
        script_files = [
            "cardiac_bbox_visualization.py",
            "bbox_visualization_with_predictions.py",
            "simple_gradnorm_train.py",
            "simple_dual_backbone_train.py"
        ]
        
        script_source = self.project_root / "Jon_yolov5test" / "yolov5c"
        for script_name in script_files:
            script_file = script_source / script_name
            if script_file.exists():
                dest_file = scripts_dir / script_name
                shutil.copy2(script_file, dest_file)
                print(f"  âœ… {script_name}")
    
    def _copy_test_samples(self, test_dir):
        """è¤‡è£½æ¸¬è©¦æ•¸æ“šæ¨£æœ¬"""
        test_source = self.project_root / "converted_dataset_fixed" / "test" / "images"
        if test_source.exists():
            # è¤‡è£½å‰10å€‹æ¸¬è©¦æ¨£æœ¬
            sample_images = list(test_source.glob("*.png"))[:10]
            for img_file in sample_images:
                dest_file = test_dir / img_file.name
                shutil.copy2(img_file, dest_file)
            print(f"  âœ… è¤‡è£½äº† {len(sample_images)} å€‹æ¸¬è©¦æ¨£æœ¬")
    
    def _add_performance_reports(self, deployment_info):
        """æ·»åŠ æ€§èƒ½å ±å‘Š"""
        # æŸ¥æ‰¾è¨“ç·´æ­·å²æ–‡ä»¶
        history_files = []
        for history_file in self.project_root.rglob("training_history.json"):
            if history_file.exists():
                history_files.append(history_file)
        
        if history_files:
            print(f"\nğŸ“Š ç™¼ç¾ {len(history_files)} å€‹è¨“ç·´æ­·å²æ–‡ä»¶")
            for history_file in history_files:
                try:
                    with open(history_file, 'r') as f:
                        history_data = json.load(f)
                    
                    model_name = history_file.parent.name
                    deployment_info["performance"][model_name] = {
                        "history_file": str(history_file),
                        "final_metrics": self._extract_final_metrics(history_data)
                    }
                    print(f"  âœ… {model_name} æ€§èƒ½å ±å‘Š")
                except Exception as e:
                    print(f"  âŒ ç„¡æ³•è®€å– {history_file}: {e}")
    
    def _extract_final_metrics(self, history_data):
        """æå–æœ€çµ‚æŒ‡æ¨™"""
        metrics = {}
        metric_keys = [
            "mAP@0.5", "mAP@0.5:0.95", "Precision", "Recall", "F1-Score",
            "Detection_Loss", "Classification_Loss", "Total_Loss"
        ]
        
        for key in metric_keys:
            if key in history_data and history_data[key]:
                metrics[key] = history_data[key][-1] if isinstance(history_data[key], list) else history_data[key]
        
        return metrics
    
    def _create_deployment_readme(self):
        """å‰µå»ºéƒ¨ç½²README"""
        readme_content = """# ğŸ«€ YOLOv5WithClassification éƒ¨ç½²åŒ…

## ğŸ“‹ é …ç›®æ¦‚è¿°
- **é …ç›®åç¨±**: YOLOv5WithClassification
- **æ‡‰ç”¨é ˜åŸŸ**: å¿ƒè‡Ÿç“£è†œåæµç–¾ç—…æª¢æ¸¬èˆ‡åˆ†é¡
- **æŠ€è¡“æ¶æ§‹**: è¯åˆç›®æ¨™æª¢æ¸¬èˆ‡åœ–åƒåˆ†é¡
- **æ‰“åŒ…æ—¥æœŸ**: {package_date}

## ğŸ“‚ ç›®éŒ„çµæ§‹
```
deploy_package/
â”œâ”€â”€ models/                    # è¨“ç·´å®Œæˆçš„æ¨¡å‹
â”‚   â”œâ”€â”€ gradnorm/             # GradNormè¯åˆè¨“ç·´æ¨¡å‹
â”‚   â”œâ”€â”€ dual_backbone/        # é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯æ¨¡å‹
â”‚   â””â”€â”€ other/                # å…¶ä»–è¨“ç·´æ¨¡å‹
â”œâ”€â”€ configs/                   # é…ç½®æ–‡ä»¶
â”œâ”€â”€ documentation/             # é …ç›®æ–‡æª”
â”œâ”€â”€ scripts/                   # æ¨ç†èˆ‡å¯è¦–åŒ–è…³æœ¬
â”œâ”€â”€ test_data/                # æ¸¬è©¦æ•¸æ“šæ¨£æœ¬
â”œâ”€â”€ deployment_info.json      # éƒ¨ç½²ä¿¡æ¯
â””â”€â”€ README.md                 # æœ¬æ–‡ä»¶
```

## ğŸ¯ æ¨¡å‹èªªæ˜

### ğŸ¤– GradNorm æ¨¡å‹
- **æ–‡ä»¶**: `models/gradnorm/final_fixed_gradnorm_model.pt`
- **æŠ€è¡“**: å‹•æ…‹æ¢¯åº¦å¹³è¡¡è¨“ç·´
- **å„ªå‹¢**: æ™ºèƒ½æ¬Šé‡èª¿æ•´ï¼Œè§£æ±ºæ¢¯åº¦å¹²æ“¾
- **æ€§èƒ½**: mAP@0.5: 0.3027, Precision: 0.2283

### ğŸ—ï¸ é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯æ¨¡å‹
- **æ–‡ä»¶**: `models/dual_backbone/final_simple_dual_backbone_model.pt`
- **æŠ€è¡“**: å®Œå…¨ç¨ç«‹çš„æª¢æ¸¬èˆ‡åˆ†é¡ç¶²è·¯
- **å„ªå‹¢**: å®Œå…¨æ¶ˆé™¤æ¢¯åº¦å¹²æ“¾
- **æ€§èƒ½**: mAP@0.5: 0.3435, Precision: 0.2580 â­ (æ¨è–¦)

## ğŸ¥ é†«å­¸æ‡‰ç”¨

### ğŸ«€ æ”¯æ´ç–¾ç—…é¡å‹
- **AR**: Aortic Regurgitation (ä¸»å‹•è„ˆç“£åæµ)
- **MR**: Mitral Regurgitation (äºŒå°–ç“£åæµ)
- **PR**: Pulmonary Regurgitation (è‚ºå‹•è„ˆç“£åæµ)
- **TR**: Tricuspid Regurgitation (ä¸‰å°–ç“£åæµ)

### ğŸ¯ æ‡‰ç”¨å ´æ™¯
- è‡¨åºŠè¨ºæ–·è¼”åŠ©
- é†«å­¸æ•™è‚²åŸ¹è¨“
- ç§‘ç ”æ•¸æ“šåˆ†æ
- é ç¨‹é†«ç™‚æ”¯æ´

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. ç’°å¢ƒæº–å‚™
```bash
pip install torch torchvision opencv-python pillow matplotlib pyyaml
```

### 2. æ¨¡å‹æ¨ç†
```python
# ä½¿ç”¨é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯æ¨¡å‹(æ¨è–¦)
python scripts/cardiac_bbox_visualization.py \\
    --model models/dual_backbone/final_simple_dual_backbone_model.pt \\
    --input test_data/ \\
    --output results/
```

### 3. å¯è¦–åŒ–æ¸¬è©¦
```python
# ç”ŸæˆGT vs é æ¸¬å°æ¯”åœ–
python scripts/bbox_visualization_with_predictions.py \\
    --dataset test_data/ \\
    --model models/dual_backbone/final_simple_dual_backbone_model.pt
```

## âš ï¸ é‡è¦èªªæ˜

### ğŸ” é†«å­¸å€«ç†
- åƒ…ä¾›ç ”ç©¶èˆ‡æ•™è‚²ä½¿ç”¨
- ä¸å¯ç›´æ¥ç”¨æ–¼è‡¨åºŠè¨ºæ–·
- éœ€è¦å°ˆæ¥­é†«ç”Ÿé©—è­‰
- å¯¦éš›æ‡‰ç”¨éœ€ç›£ç®¡å¯©æ‰¹

### ğŸ“Š æ€§èƒ½æŒ‡æ¨™
- æ¸¬è©¦åŸºæ–¼ç ”ç©¶æ•¸æ“šé›†
- å¯¦éš›æ€§èƒ½å¯èƒ½å› æ•¸æ“šè€Œç•°
- å»ºè­°åœ¨å¯¦éš›æ•¸æ“šä¸Šé‡æ–°è©•ä¼°

## ğŸ“ è¯çµ¡ä¿¡æ¯
- é …ç›®: YOLOv5WithClassification
- æŠ€è¡“: æ¢¯åº¦å¹²æ“¾è§£æ±ºæ–¹æ¡ˆ
- æ‡‰ç”¨: å¿ƒè‡Ÿç“£è†œç–¾ç—…AIè¨ºæ–·

---
**ç”Ÿæˆæ™‚é–“**: {package_date}
**ç‰ˆæœ¬**: 1.0.0
""".format(package_date=datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        
        with open(self.output_dir / "README.md", "w", encoding="utf-8") as f:
            f.write(readme_content)
        
        print(f"  âœ… README.md")
    
    def create_deployment_zip(self, deployment_info):
        """å‰µå»ºéƒ¨ç½²ZIPåŒ…"""
        print("\nğŸ“¦ å‰µå»ºéƒ¨ç½²ZIPåŒ…...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        zip_name = f"YOLOv5WithClassification_Deploy_{timestamp}.zip"
        zip_path = self.output_dir.parent / zip_name
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(self.output_dir):
                for file in files:
                    file_path = Path(root) / file
                    arcname = file_path.relative_to(self.output_dir.parent)
                    zipf.write(file_path, arcname)
        
        zip_size_mb = round(zip_path.stat().st_size / 1024 / 1024, 2)
        print(f"âœ… ZIPåŒ…å‰µå»ºå®Œæˆ: {zip_name} ({zip_size_mb} MB)")
        
        return zip_path
    
    def generate_deployment_summary(self, deployment_info, zip_path):
        """ç”Ÿæˆéƒ¨ç½²ç¸½çµå ±å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆéƒ¨ç½²ç¸½çµå ±å‘Š...")
        
        total_models = len(deployment_info["models"])
        total_size_mb = sum(model["size_mb"] for model in deployment_info["models"].values())
        
        summary = f"""
ğŸš€ YOLOv5WithClassification æ¨¡å‹éƒ¨ç½²ç¸½çµ

ğŸ“¦ æ‰“åŒ…å®Œæˆæ™‚é–“: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
ğŸ¯ é …ç›®: å¿ƒè‡Ÿç“£è†œåæµç–¾ç—…æª¢æ¸¬èˆ‡åˆ†é¡
ğŸ† æŠ€è¡“å‰µæ–°: æ¢¯åº¦å¹²æ“¾è§£æ±ºæ–¹æ¡ˆ (GradNorm + é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯)

ğŸ“Š éƒ¨ç½²çµ±è¨ˆ:
â”œâ”€â”€ ğŸ“ æ¨¡å‹ç¸½æ•¸: {total_models} å€‹
â”œâ”€â”€ ğŸ’¾ æ¨¡å‹ç¸½å¤§å°: {total_size_mb:.2f} MB
â”œâ”€â”€ ğŸ“‹ é…ç½®æ–‡ä»¶: å·²åŒ…å«
â”œâ”€â”€ ğŸ“š æ–‡æª”è³‡æ–™: å·²åŒ…å«
â”œâ”€â”€ ğŸ”§ æ¨ç†è…³æœ¬: å·²åŒ…å«
â””â”€â”€ ğŸ§ª æ¸¬è©¦æ¨£æœ¬: å·²åŒ…å«

ğŸ¯ é‡é»æ¨¡å‹:
â”œâ”€â”€ ğŸ¤– GradNormæ¨¡å‹: å‹•æ…‹æ¢¯åº¦å¹³è¡¡æŠ€è¡“
â””â”€â”€ ğŸ—ï¸ é›™ç¨ç«‹éª¨å¹¹æ¨¡å‹: å®Œå…¨æ¶ˆé™¤æ¢¯åº¦å¹²æ“¾ â­ (æ¨è–¦)

ğŸ¥ é†«å­¸æ‡‰ç”¨:
â”œâ”€â”€ æ”¯æ´4é¡å¿ƒè‡Ÿç“£è†œç–¾ç—…: AR, MR, PR, TR
â”œâ”€â”€ é©ç”¨å ´æ™¯: è‡¨åºŠè¼”åŠ©è¨ºæ–· + é†«å­¸æ•™è‚²
â””â”€â”€ æ€§èƒ½æ°´æº–: é†«å­¸ç ”ç©¶ç´šåˆ¥

ğŸ“¦ éƒ¨ç½²åŒ…ä½ç½®:
â”œâ”€â”€ ğŸ“ ç›®éŒ„: {self.output_dir.absolute()}
â””â”€â”€ ğŸ“¦ ZIPåŒ…: {zip_path.absolute()}

âœ… éƒ¨ç½²æº–å‚™å®Œæˆï¼å¯ç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒæ¸¬è©¦ã€‚
âš ï¸  è«‹æ³¨æ„é†«å­¸å€«ç†è¦æ±‚ï¼Œåƒ…ä¾›ç ”ç©¶èˆ‡æ•™è‚²ä½¿ç”¨ã€‚
"""
        
        print(summary)
        
        # ä¿å­˜ç¸½çµåˆ°æ–‡ä»¶
        with open(self.output_dir / "deployment_summary.txt", "w", encoding="utf-8") as f:
            f.write(summary)
        
        return summary

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ«€ å•Ÿå‹• YOLOv5WithClassification æ¨¡å‹éƒ¨ç½²æ‰“åŒ…å·¥å…·...\n")
    
    packager = ModelDeploymentPackager()
    
    try:
        # æ‰“åŒ…æ¨¡å‹
        deployment_info = packager.package_deployment_models()
        
        # å‰µå»ºZIPåŒ…
        zip_path = packager.create_deployment_zip(deployment_info)
        
        # ç”Ÿæˆç¸½çµå ±å‘Š
        summary = packager.generate_deployment_summary(deployment_info, zip_path)
        
        print("\nğŸ‰ æ¨¡å‹éƒ¨ç½²æ‰“åŒ…å®Œæˆï¼")
        return deployment_info, zip_path
        
    except Exception as e:
        print(f"\nâŒ æ‰“åŒ…éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        raise

if __name__ == "__main__":
    deployment_info, zip_path = main() 