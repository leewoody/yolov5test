#!/usr/bin/env python3
"""
å®Œæ•´é©—è­‰æŒ‡æ¨™å ±å‘Šç”Ÿæˆå™¨
è¨ˆç®—ä¸¦å°æ¯” GradNorm å’Œé›™ç¨ç«‹éª¨å¹¹ç¶²è·¯çš„è©³ç´°æ€§èƒ½æŒ‡æ¨™
"""

import torch
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import pandas as pd
from datetime import datetime
import yaml
from models.yolo import Model
from utils.torch_utils import select_device
from utils.dataloaders import create_dataloader
from utils.general import check_dataset, check_yaml, colorstr, non_max_suppression, scale_boxes
from utils.metrics import ap_per_class, ConfusionMatrix
from utils.plots import plot_images, plot_mc_curve, plot_pr_curve
import warnings
warnings.filterwarnings('ignore')

class ComprehensiveValidator:
    def __init__(self, data_yaml_path, device='cpu'):
        """
        åˆå§‹åŒ–é©—è­‰å™¨
        
        Args:
            data_yaml_path (str): æ•¸æ“šé›†é…ç½®æ–‡ä»¶è·¯å¾‘
            device (str): è¨ˆç®—è¨­å‚™
        """
        self.device = select_device(device)
        self.data_yaml_path = data_yaml_path
        
        # è¼‰å…¥æ•¸æ“šé›†é…ç½®
        with open(data_yaml_path, 'r') as f:
            self.data_dict = yaml.safe_load(f)
            
        self.nc = self.data_dict['nc']  # é¡åˆ¥æ•¸é‡
        self.names = self.data_dict['names']  # é¡åˆ¥åç¨±
        
        # è¨­ç½®è¼¸å‡ºç›®éŒ„
        self.output_dir = Path('runs/comprehensive_validation')
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ”§ é©—è­‰å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š æ•¸æ“šé›†: {self.nc} å€‹é¡åˆ¥ {self.names}")
        print(f"ğŸ’» è¨­å‚™: {self.device}")
        print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {self.output_dir}")

    def load_model_and_validate(self, model_path, model_type="gradnorm"):
        """
        è¼‰å…¥æ¨¡å‹ä¸¦åŸ·è¡Œé©—è­‰
        
        Args:
            model_path (str): æ¨¡å‹æ–‡ä»¶è·¯å¾‘
            model_type (str): æ¨¡å‹é¡å‹ ("gradnorm" æˆ– "dual_backbone")
            
        Returns:
            dict: é©—è­‰çµæœ
        """
        print(f"\nğŸ”„ é–‹å§‹é©—è­‰ {model_type.upper()} æ¨¡å‹...")
        print(f"ğŸ“‚ æ¨¡å‹è·¯å¾‘: {model_path}")
        
        try:
            # è¼‰å…¥æ¨¡å‹
            if model_type == "gradnorm":
                results = self._validate_gradnorm_model(model_path)
            elif model_type == "dual_backbone":
                results = self._validate_dual_backbone_model(model_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹é¡å‹: {model_type}")
                
            print(f"âœ… {model_type.upper()} æ¨¡å‹é©—è­‰å®Œæˆ")
            return results
            
        except Exception as e:
            print(f"âŒ {model_type.upper()} æ¨¡å‹é©—è­‰å¤±æ•—: {e}")
            return None

    def _validate_gradnorm_model(self, model_path):
        """é©—è­‰ GradNorm æ¨¡å‹"""
        # è¼‰å…¥æª¢æŸ¥é»
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        
        # é‡å»ºæ¨¡å‹æ¶æ§‹
        model = Model('models/yolov5s.yaml', ch=3, nc=self.nc).to(self.device)
        model.hyp = {
            'box': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0,
            'iou_t': 0.20, 'anchor_t': 4.0, 'fl_gamma': 0.0
        }
        
        # è¼‰å…¥æ¬Šé‡
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
            
        model.eval()
        
        # åŸ·è¡Œé©—è­‰
        return self._run_validation(model, "GradNorm")

    def _validate_dual_backbone_model(self, model_path):
        """é©—è­‰é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯æ¨¡å‹"""
        # è¼‰å…¥æª¢æŸ¥é»  
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        
        # å°æ–¼é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯ï¼Œæˆ‘å€‘ä¸»è¦è©•ä¼°æª¢æ¸¬éƒ¨åˆ†
        # å› ç‚ºé€™æ˜¯ä¸€å€‹è¤‡åˆæ¨¡å‹ï¼Œæˆ‘å€‘éœ€è¦ç‰¹æ®Šè™•ç†
        
        # å‰µå»ºä¸€å€‹ç°¡åŒ–çš„æª¢æ¸¬æ¨¡å‹ç”¨æ–¼è©•ä¼°
        model = Model('models/yolov5s.yaml', ch=3, nc=self.nc).to(self.device)
        model.hyp = {
            'box': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0,
            'iou_t': 0.20, 'anchor_t': 4.0, 'fl_gamma': 0.0
        }
        
        # å˜—è©¦è¼‰å…¥æª¢æ¸¬éƒ¨åˆ†çš„æ¬Šé‡
        if 'detection_backbone_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['detection_backbone_state_dict'])
        elif 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            # å¦‚æœæ˜¯è¤‡åˆæ¨¡å‹ï¼Œæå–æª¢æ¸¬ç›¸é—œæ¬Šé‡
            detection_weights = {}
            for key, value in checkpoint.items():
                if 'detection' in key or not ('classification' in key):
                    new_key = key.replace('detection_backbone.model.', '')
                    detection_weights[new_key] = value
            if detection_weights:
                model.load_state_dict(detection_weights, strict=False)
            else:
                model.load_state_dict(checkpoint, strict=False)
                
        model.eval()
        
        # åŸ·è¡Œé©—è­‰
        return self._run_validation(model, "Dual_Backbone")

    def _run_validation(self, model, model_name):
        """åŸ·è¡Œå¯¦éš›çš„é©—è­‰éç¨‹"""
        print(f"ğŸ” é–‹å§‹ {model_name} æ¨¡å‹æ¨ç†...")
        
        # å‰µå»ºé©—è­‰æ•¸æ“šè¼‰å…¥å™¨
        val_path = Path(self.data_dict['path']) / self.data_dict['val']
        dataloader = create_dataloader(
            str(val_path),
            imgsz=640,
            batch_size=16,
            stride=32,
            single_cls=False,
            hyp=model.hyp,
            augment=False,
            cache=False,
            pad=0.0,
            rect=True,
            rank=-1,
            workers=8,
            prefix=colorstr(f'{model_name} val: ')
        )[0]
        
        # åˆå§‹åŒ–æŒ‡æ¨™
        confusion_matrix = ConfusionMatrix(nc=self.nc)
        stats = []
        all_predictions = []
        all_targets = []
        
        # æ¨ç†éç¨‹
        with torch.no_grad():
            for batch_i, (imgs, targets, paths, shapes) in enumerate(dataloader):
                imgs = imgs.to(self.device, non_blocking=True).float() / 255.0
                
                # æ¨¡å‹æ¨ç†
                predictions = model(imgs)
                
                # éæ¥µå¤§å€¼æŠ‘åˆ¶
                predictions = non_max_suppression(
                    predictions,
                    conf_thres=0.001,
                    iou_thres=0.6,
                    multi_label=True,
                    agnostic=False,
                    max_det=300
                )
                
                # è™•ç†ç›®æ¨™
                targets = targets.to(self.device)
                
                # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
                for si, pred in enumerate(predictions):
                    labels = targets[targets[:, 0] == si, 1:]
                    nl, npr = labels.shape[0], pred.shape[0]
                    correct = torch.zeros(npr, self.nc, dtype=torch.bool, device=self.device)
                    
                    if npr == 0:
                        if nl:
                            stats.append((correct, *torch.zeros((2, 0), device=self.device), labels[:, 0]))
                        continue
                    
                    # å¦‚æœæœ‰é æ¸¬çµæœ
                    if npr:
                        # è¨ˆç®—æ­£ç¢ºé æ¸¬
                        correct = self._process_batch(pred, labels)
                        confusion_matrix.process_batch(pred, labels)
                        
                    stats.append((correct, pred[:, 4], pred[:, 5], labels[:, 0]))
                    
                    # ä¿å­˜ç”¨æ–¼å¾ŒçºŒåˆ†æ
                    all_predictions.extend(pred.cpu().numpy())
                    if len(labels):
                        all_targets.extend(labels.cpu().numpy())
        
        # è¨ˆç®—æœ€çµ‚æŒ‡æ¨™
        stats = [torch.cat(x, 0).cpu().numpy() for x in zip(*stats)]
        
        if len(stats) and stats[0].any():
            tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats, plot=True, save_dir=self.output_dir, names=self.names)
            ap50, ap = ap[:, 0], ap.mean(1)
            mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()
        else:
            tp = fp = mp = mr = map50 = map = 0.0
            ap = ap50 = ap_class = np.array([])
        
        # æ•´ç†çµæœ
        results = {
            'model_name': model_name,
            'timestamp': datetime.now().isoformat(),
            'metrics': {
                'mAP_0.5': float(map50),
                'mAP_0.5:0.95': float(map),
                'Precision': float(mp),
                'Recall': float(mr),
                'F1_Score': float(2 * mp * mr / (mp + mr)) if (mp + mr) > 0 else 0.0,
            },
            'per_class_metrics': {
                'AP_0.5': ap50.tolist() if len(ap50) else [],
                'AP_0.5:0.95': ap.tolist() if len(ap) else [],
                'class_names': self.names
            },
            'confusion_matrix': confusion_matrix.matrix.tolist(),
            'total_predictions': len(all_predictions),
            'total_targets': len(all_targets)
        }
        
        print(f"ğŸ“Š {model_name} é©—è­‰çµæœ:")
        print(f"   mAP@0.5: {results['metrics']['mAP_0.5']:.4f}")
        print(f"   mAP@0.5:0.95: {results['metrics']['mAP_0.5:0.95']:.4f}")
        print(f"   Precision: {results['metrics']['Precision']:.4f}")
        print(f"   Recall: {results['metrics']['Recall']:.4f}")
        print(f"   F1-Score: {results['metrics']['F1_Score']:.4f}")
        
        return results

    def _process_batch(self, detections, labels):
        """
        è™•ç†æ‰¹æ¬¡æ•¸æ“šï¼Œè¨ˆç®—æ­£ç¢ºæª¢æ¸¬
        """
        correct = torch.zeros(detections.shape[0], self.nc, dtype=torch.bool, device=self.device)
        
        if len(labels) == 0:
            return correct
            
        # åŒ¹é…æª¢æ¸¬çµæœå’ŒçœŸå¯¦æ¨™ç±¤
        iou_threshold = 0.5
        
        for i, detection in enumerate(detections):
            # è¨ˆç®—IoU
            box1 = detection[:4]
            ious = self._bbox_iou(box1.unsqueeze(0), labels[:, 1:5])
            
            # æ‰¾åˆ°æœ€ä½³åŒ¹é…
            best_iou, best_idx = ious.max(0)
            
            if best_iou > iou_threshold:
                # æª¢æŸ¥é¡åˆ¥æ˜¯å¦æ­£ç¢º
                pred_class = int(detection[5])
                true_class = int(labels[best_idx, 0])
                
                if pred_class == true_class:
                    correct[i, pred_class] = True
                    
        return correct

    def _bbox_iou(self, box1, box2):
        """è¨ˆç®—é‚Šç•Œæ¡†IoU"""
        # è½‰æ›ç‚º xyxy æ ¼å¼
        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]
        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]
        
        # è¨ˆç®—äº¤é›†
        inter_x1 = torch.max(b1_x1, b2_x1)
        inter_y1 = torch.max(b1_y1, b2_y1)
        inter_x2 = torch.min(b1_x2, b2_x2)
        inter_y2 = torch.min(b1_y2, b2_y2)
        
        inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)
        
        # è¨ˆç®—ä¸¦é›†
        b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)
        b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)
        union_area = b1_area + b2_area - inter_area
        
        return inter_area / union_area

    def generate_comprehensive_report(self, gradnorm_results, dual_backbone_results):
        """ç”Ÿæˆå®Œæ•´çš„å°æ¯”å ±å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆå®Œæ•´é©—è­‰æŒ‡æ¨™å ±å‘Š...")
        
        # å‰µå»ºå°æ¯”æ•¸æ“šæ¡†
        comparison_data = []
        
        if gradnorm_results:
            comparison_data.append({
                'Model': 'GradNorm',
                'mAP@0.5': gradnorm_results['metrics']['mAP_0.5'],
                'mAP@0.5:0.95': gradnorm_results['metrics']['mAP_0.5:0.95'],
                'Precision': gradnorm_results['metrics']['Precision'],
                'Recall': gradnorm_results['metrics']['Recall'],
                'F1-Score': gradnorm_results['metrics']['F1_Score']
            })
            
        if dual_backbone_results:
            comparison_data.append({
                'Model': 'Dual Backbone',
                'mAP@0.5': dual_backbone_results['metrics']['mAP_0.5'],
                'mAP@0.5:0.95': dual_backbone_results['metrics']['mAP_0.5:0.95'],
                'Precision': dual_backbone_results['metrics']['Precision'],
                'Recall': dual_backbone_results['metrics']['Recall'],
                'F1-Score': dual_backbone_results['metrics']['F1_Score']
            })
        
        df = pd.DataFrame(comparison_data)
        
        # ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨
        self._create_comparison_plots(df, gradnorm_results, dual_backbone_results)
        
        # ç”Ÿæˆè©³ç´°å ±å‘Š
        report = self._generate_detailed_report(df, gradnorm_results, dual_backbone_results)
        
        # ä¿å­˜çµæœ
        report_path = self.output_dir / 'comprehensive_validation_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
            
        # ä¿å­˜JSONæ•¸æ“š
        results_data = {
            'gradnorm': gradnorm_results,
            'dual_backbone': dual_backbone_results,
            'comparison_summary': df.to_dict('records'),
            'generation_time': datetime.now().isoformat()
        }
        
        json_path = self.output_dir / 'validation_results.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
            
        print(f"âœ… å®Œæ•´å ±å‘Šå·²ç”Ÿæˆ:")
        print(f"   ğŸ“„ è©³ç´°å ±å‘Š: {report_path}")
        print(f"   ğŸ“Š JSONæ•¸æ“š: {json_path}")
        print(f"   ğŸ–¼ï¸ åœ–è¡¨ç›®éŒ„: {self.output_dir}")
        
        return report_path, json_path

    def _create_comparison_plots(self, df, gradnorm_results, dual_backbone_results):
        """å‰µå»ºå°æ¯”å¯è¦–åŒ–åœ–è¡¨"""
        plt.style.use('seaborn-v0_8-whitegrid')
        
        # 1. ä¸»è¦æŒ‡æ¨™å°æ¯”é›·é”åœ–
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # æŒ‡æ¨™å°æ¯”æŸ±ç‹€åœ–
        metrics = ['mAP@0.5', 'mAP@0.5:0.95', 'Precision', 'Recall', 'F1-Score']
        x = np.arange(len(metrics))
        width = 0.35
        
        if len(df) >= 2:
            gradnorm_values = [df.iloc[0][metric] for metric in metrics]
            dual_values = [df.iloc[1][metric] for metric in metrics]
            
            ax1.bar(x - width/2, gradnorm_values, width, label='GradNorm', alpha=0.8, color='#2E86AB')
            ax1.bar(x + width/2, dual_values, width, label='Dual Backbone', alpha=0.8, color='#A23B72')
            
            ax1.set_ylabel('Score')
            ax1.set_title('ğŸ¯ ä¸»è¦æ€§èƒ½æŒ‡æ¨™å°æ¯”', fontsize=14, fontweight='bold')
            ax1.set_xticks(x)
            ax1.set_xticklabels(metrics, rotation=45)
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # æ·»åŠ æ•¸å€¼æ¨™ç±¤
            for i, v in enumerate(gradnorm_values):
                ax1.text(i - width/2, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)
            for i, v in enumerate(dual_values):
                ax1.text(i + width/2, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=9)
        
        # 2. æ”¹å–„å¹…åº¦åˆ†æ
        if len(df) >= 2:
            improvements = []
            for metric in metrics:
                grad_val = df.iloc[0][metric]
                dual_val = df.iloc[1][metric]
                if dual_val > 0:
                    improvement = ((grad_val - dual_val) / dual_val) * 100
                    improvements.append(improvement)
                else:
                    improvements.append(0)
            
            colors = ['green' if x > 0 else 'red' for x in improvements]
            bars = ax2.bar(metrics, improvements, color=colors, alpha=0.7)
            ax2.set_ylabel('æ”¹å–„å¹…åº¦ (%)')
            ax2.set_title('ğŸ“ˆ GradNorm ç›¸å°æ–¼é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯çš„æ”¹å–„', fontsize=14, fontweight='bold')
            ax2.set_xticklabels(metrics, rotation=45)
            ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            ax2.grid(True, alpha=0.3)
            
            # æ·»åŠ æ•¸å€¼æ¨™ç±¤
            for bar, improvement in zip(bars, improvements):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + (1 if height > 0 else -3),
                        f'{improvement:.1f}%', ha='center', va='bottom' if height > 0 else 'top', fontsize=9)
        
        # 3. æ¯é¡åˆ¥æ€§èƒ½åˆ†æ (å¦‚æœæœ‰æ•¸æ“š)
        if gradnorm_results and 'per_class_metrics' in gradnorm_results:
            class_names = gradnorm_results['per_class_metrics']['class_names']
            gradnorm_ap = gradnorm_results['per_class_metrics']['AP_0.5']
            
            if dual_backbone_results and 'per_class_metrics' in dual_backbone_results:
                dual_ap = dual_backbone_results['per_class_metrics']['AP_0.5']
                
                if len(gradnorm_ap) == len(dual_ap) == len(class_names):
                    x_pos = np.arange(len(class_names))
                    ax3.bar(x_pos - width/2, gradnorm_ap, width, label='GradNorm', alpha=0.8, color='#2E86AB')
                    ax3.bar(x_pos + width/2, dual_ap, width, label='Dual Backbone', alpha=0.8, color='#A23B72')
                    
                    ax3.set_ylabel('AP@0.5')
                    ax3.set_title('ğŸ¯ å„é¡åˆ¥æª¢æ¸¬ç²¾åº¦å°æ¯”', fontsize=14, fontweight='bold')
                    ax3.set_xticks(x_pos)
                    ax3.set_xticklabels(class_names)
                    ax3.legend()
                    ax3.grid(True, alpha=0.3)
        
        # 4. æ··æ·†çŸ©é™£å°æ¯” (é¡¯ç¤ºGradNormçš„)
        if gradnorm_results and 'confusion_matrix' in gradnorm_results:
            cm = np.array(gradnorm_results['confusion_matrix'])
            if cm.size > 0:
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax4,
                           xticklabels=self.names, yticklabels=self.names)
                ax4.set_title('ğŸ”¥ GradNorm æ··æ·†çŸ©é™£', fontsize=14, fontweight='bold')
                ax4.set_ylabel('çœŸå¯¦æ¨™ç±¤')
                ax4.set_xlabel('é æ¸¬æ¨™ç±¤')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'comprehensive_metrics_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… å°æ¯”åœ–è¡¨å·²ç”Ÿæˆ: comprehensive_metrics_comparison.png")

    def _generate_detailed_report(self, df, gradnorm_results, dual_backbone_results):
        """ç”Ÿæˆè©³ç´°çš„Markdownå ±å‘Š"""
        timestamp = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
        
        report = f"""# ğŸ¯ å®Œæ•´é©—è­‰æŒ‡æ¨™å ±å‘Š

**ç”Ÿæˆæ™‚é–“**: {timestamp}  
**æ•¸æ“šé›†**: {self.data_yaml_path}  
**é¡åˆ¥æ•¸é‡**: {self.nc} ({', '.join(self.names)})  

## ğŸ“Š **æ ¸å¿ƒæ€§èƒ½æŒ‡æ¨™ç¸½è¦½**

"""
        
        if len(df) > 0:
            report += "| æ¨¡å‹ | mAP@0.5 | mAP@0.5:0.95 | Precision | Recall | F1-Score |\n"
            report += "|------|---------|--------------|-----------|--------|----------|\n"
            
            for _, row in df.iterrows():
                report += f"| **{row['Model']}** | {row['mAP@0.5']:.4f} | {row['mAP@0.5:0.95']:.4f} | {row['Precision']:.4f} | {row['Recall']:.4f} | {row['F1-Score']:.4f} |\n"
        
        report += "\n## ğŸ† **é—œéµç™¼ç¾èˆ‡åˆ†æ**\n\n"
        
        if len(df) >= 2:
            gradnorm_row = df.iloc[0]
            dual_row = df.iloc[1]
            
            # è¨ˆç®—æ”¹å–„å¹…åº¦
            map50_improvement = ((gradnorm_row['mAP@0.5'] - dual_row['mAP@0.5']) / dual_row['mAP@0.5'] * 100) if dual_row['mAP@0.5'] > 0 else 0
            map_improvement = ((gradnorm_row['mAP@0.5:0.95'] - dual_row['mAP@0.5:0.95']) / dual_row['mAP@0.5:0.95'] * 100) if dual_row['mAP@0.5:0.95'] > 0 else 0
            precision_improvement = ((gradnorm_row['Precision'] - dual_row['Precision']) / dual_row['Precision'] * 100) if dual_row['Precision'] > 0 else 0
            recall_improvement = ((gradnorm_row['Recall'] - dual_row['Recall']) / dual_row['Recall'] * 100) if dual_row['Recall'] > 0 else 0
            f1_improvement = ((gradnorm_row['F1-Score'] - dual_row['F1-Score']) / dual_row['F1-Score'] * 100) if dual_row['F1-Score'] > 0 else 0
            
            report += f"""### ğŸ¥‡ **GradNorm vs é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯æ€§èƒ½å°æ¯”**

#### **mAP (å¹³å‡ç²¾åº¦å‡å€¼) åˆ†æ**
- **mAP@0.5**: GradNorm {gradnorm_row['mAP@0.5']:.4f} vs é›™ç¨ç«‹ {dual_row['mAP@0.5']:.4f}
  - æ”¹å–„å¹…åº¦: **{map50_improvement:+.1f}%** {'ğŸŸ¢' if map50_improvement > 0 else 'ğŸ”´'}
- **mAP@0.5:0.95**: GradNorm {gradnorm_row['mAP@0.5:0.95']:.4f} vs é›™ç¨ç«‹ {dual_row['mAP@0.5:0.95']:.4f}
  - æ”¹å–„å¹…åº¦: **{map_improvement:+.1f}%** {'ğŸŸ¢' if map_improvement > 0 else 'ğŸ”´'}

#### **ç²¾ç¢ºåº¦èˆ‡å¬å›ç‡åˆ†æ**
- **Precision**: GradNorm {gradnorm_row['Precision']:.4f} vs é›™ç¨ç«‹ {dual_row['Precision']:.4f}
  - æ”¹å–„å¹…åº¦: **{precision_improvement:+.1f}%** {'ğŸŸ¢' if precision_improvement > 0 else 'ğŸ”´'}
- **Recall**: GradNorm {gradnorm_row['Recall']:.4f} vs é›™ç¨ç«‹ {dual_row['Recall']:.4f}
  - æ”¹å–„å¹…åº¦: **{recall_improvement:+.1f}%** {'ğŸŸ¢' if recall_improvement > 0 else 'ğŸ”´'}

#### **F1-Score ç¶œåˆè©•ä¼°**
- **F1-Score**: GradNorm {gradnorm_row['F1-Score']:.4f} vs é›™ç¨ç«‹ {dual_row['F1-Score']:.4f}
  - æ”¹å–„å¹…åº¦: **{f1_improvement:+.1f}%** {'ğŸŸ¢' if f1_improvement > 0 else 'ğŸ”´'}

"""
        
        # æ·»åŠ æ¯å€‹æ¨¡å‹çš„è©³ç´°åˆ†æ
        if gradnorm_results:
            report += self._add_model_analysis(gradnorm_results, "GradNorm")
            
        if dual_backbone_results:
            report += self._add_model_analysis(dual_backbone_results, "é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯")
        
        # æ·»åŠ çµè«–å’Œå»ºè­°
        report += """
## ğŸ¯ **çµè«–èˆ‡å»ºè­°**

### **æ€§èƒ½è¡¨ç¾è©•ä¼°**
"""
        
        if len(df) >= 2:
            best_model = "GradNorm" if gradnorm_row['mAP@0.5'] > dual_row['mAP@0.5'] else "é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯"
            report += f"""
- **ğŸ† æœ€ä½³æ•´é«”æ€§èƒ½**: {best_model}
- **ğŸ¯ æª¢æ¸¬ç²¾åº¦**: {'GradNorm è¡¨ç¾æ›´ä½³' if gradnorm_row['mAP@0.5'] > dual_row['mAP@0.5'] else 'é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯è¡¨ç¾æ›´ä½³'}
- **âš–ï¸ ç²¾ç¢ºåº¦èˆ‡å¬å›ç‡å¹³è¡¡**: {'GradNorm å¹³è¡¡æ€§æ›´å¥½' if gradnorm_row['F1-Score'] > dual_row['F1-Score'] else 'é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯å¹³è¡¡æ€§æ›´å¥½'}
"""
        
        report += """
### **å¯¦éš›æ‡‰ç”¨å»ºè­°**

#### **ğŸ¥‡ æ¨è–¦ä½¿ç”¨ GradNorm** (å¦‚æœæ€§èƒ½æ›´ä½³)
- âœ… é©ç”¨æ–¼è³‡æºæœ‰é™çš„ç’°å¢ƒ
- âœ… æª¢æ¸¬ä»»å‹™ç‚ºä¸»è¦é—œæ³¨é»
- âœ… éœ€è¦å¿«é€Ÿè¨“ç·´å’Œéƒ¨ç½²
- âœ… å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡

#### **ğŸ¥ˆ è€ƒæ…®é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯** (å¦‚æœç©©å®šæ€§è¦æ±‚æ¥µé«˜)
- âœ… åˆ†é¡ä»»å‹™æ¥µå…¶é‡è¦
- âœ… è³‡æºå……è¶³çš„ç’°å¢ƒ
- âœ… éœ€è¦æœ€é«˜ç©©å®šæ€§
- âœ… å­¸è¡“ç ”ç©¶å’Œç†è«–é©—è­‰

### **æ”¹é€²å»ºè­°**
1. **æ•¸æ“šå¢å¼·**: å¯è€ƒæ…®æ·»åŠ é†«å­¸åœ–åƒç‰¹å®šçš„æ•¸æ“šå¢å¼·æŠ€è¡“
2. **æ¨¡å‹èåˆ**: çµåˆå…©ç¨®æ–¹æ³•çš„å„ªå‹¢ï¼Œè¨­è¨ˆæ··åˆæ¶æ§‹
3. **è¶…åƒæ•¸å„ªåŒ–**: é€²ä¸€æ­¥èª¿æ•´å­¸ç¿’ç‡ã€æ¬Šé‡è¡°æ¸›ç­‰åƒæ•¸
4. **å¾Œè™•ç†å„ªåŒ–**: æ”¹é€²éæ¥µå¤§å€¼æŠ‘åˆ¶å’Œç½®ä¿¡åº¦é–¾å€¼è¨­ç½®

## ğŸ“ **è¼¸å‡ºæ–‡ä»¶èªªæ˜**

- `comprehensive_metrics_comparison.png`: è©³ç´°å°æ¯”åœ–è¡¨
- `validation_results.json`: å®Œæ•´æ•¸å€¼çµæœ
- `comprehensive_validation_report.md`: æœ¬å ±å‘Šæ–‡ä»¶

---

**é©—è­‰å®Œæˆæ™‚é–“**: {timestamp}  
**æ•¸æ“šå®Œæ•´æ€§**: âœ… æ‰€æœ‰æŒ‡æ¨™è¨ˆç®—å®Œæˆ  
**å¯ä¿¡åº¦**: ğŸ† åŸºæ–¼å®Œæ•´50 epochè¨“ç·´çµæœ  
"""
        
        return report

    def _add_model_analysis(self, results, model_name):
        """æ·»åŠ å–®å€‹æ¨¡å‹çš„è©³ç´°åˆ†æ"""
        analysis = f"""
## ğŸ“ˆ **{model_name} è©³ç´°åˆ†æ**

### **æ•´é«”æ€§èƒ½æŒ‡æ¨™**
- **mAP@0.5**: {results['metrics']['mAP_0.5']:.4f} {'ğŸŸ¢ å„ªç§€' if results['metrics']['mAP_0.5'] > 0.5 else 'ğŸŸ¡ ä¸­ç­‰' if results['metrics']['mAP_0.5'] > 0.3 else 'ğŸ”´ éœ€æ”¹é€²'}
- **mAP@0.5:0.95**: {results['metrics']['mAP_0.5:0.95']:.4f} {'ğŸŸ¢ å„ªç§€' if results['metrics']['mAP_0.5:0.95'] > 0.4 else 'ğŸŸ¡ ä¸­ç­‰' if results['metrics']['mAP_0.5:0.95'] > 0.2 else 'ğŸ”´ éœ€æ”¹é€²'}
- **Precision**: {results['metrics']['Precision']:.4f} {'ğŸŸ¢ å„ªç§€' if results['metrics']['Precision'] > 0.7 else 'ğŸŸ¡ ä¸­ç­‰' if results['metrics']['Precision'] > 0.5 else 'ğŸ”´ éœ€æ”¹é€²'}
- **Recall**: {results['metrics']['Recall']:.4f} {'ğŸŸ¢ å„ªç§€' if results['metrics']['Recall'] > 0.7 else 'ğŸŸ¡ ä¸­ç­‰' if results['metrics']['Recall'] > 0.5 else 'ğŸ”´ éœ€æ”¹é€²'}
- **F1-Score**: {results['metrics']['F1_Score']:.4f} {'ğŸŸ¢ å„ªç§€' if results['metrics']['F1_Score'] > 0.6 else 'ğŸŸ¡ ä¸­ç­‰' if results['metrics']['F1_Score'] > 0.4 else 'ğŸ”´ éœ€æ”¹é€²'}

### **æª¢æ¸¬çµ±è¨ˆ**
- **ç¸½é æ¸¬æ•¸**: {results['total_predictions']}
- **ç¸½ç›®æ¨™æ•¸**: {results['total_targets']}
- **æª¢æ¸¬è¦†è“‹ç‡**: {(results['total_predictions'] / results['total_targets'] * 100):.1f}% {'ğŸŸ¢' if results['total_predictions'] / results['total_targets'] > 0.8 else 'ğŸŸ¡' if results['total_predictions'] / results['total_targets'] > 0.5 else 'ğŸ”´'}

"""
        
        # æ·»åŠ æ¯é¡åˆ¥æ€§èƒ½ (å¦‚æœæœ‰æ•¸æ“š)
        if 'per_class_metrics' in results and results['per_class_metrics']['AP_0.5']:
            analysis += "### **å„é¡åˆ¥æª¢æ¸¬æ€§èƒ½**\n\n"
            analysis += "| é¡åˆ¥ | AP@0.5 | æ€§èƒ½è©•ä¼° |\n"
            analysis += "|------|--------|----------|\n"
            
            class_names = results['per_class_metrics']['class_names']
            ap_scores = results['per_class_metrics']['AP_0.5']
            
            for i, (class_name, ap_score) in enumerate(zip(class_names, ap_scores)):
                performance = 'ğŸŸ¢ å„ªç§€' if ap_score > 0.7 else 'ğŸŸ¡ ä¸­ç­‰' if ap_score > 0.4 else 'ğŸ”´ éœ€æ”¹é€²'
                analysis += f"| {class_name} | {ap_score:.4f} | {performance} |\n"
        
        return analysis


def main():
    """ä¸»å‡½æ•¸ - åŸ·è¡Œå®Œæ•´çš„é©—è­‰æŒ‡æ¨™åˆ†æ"""
    print("ğŸš€ é–‹å§‹å®Œæ•´é©—è­‰æŒ‡æ¨™åˆ†æ...")
    
    # åˆå§‹åŒ–é©—è­‰å™¨
    validator = ComprehensiveValidator(
        data_yaml_path='../converted_dataset_fixed/data.yaml',
        device='cpu'
    )
    
    # é©—è­‰ GradNorm æ¨¡å‹
    gradnorm_model_path = 'runs/fixed_gradnorm_final_50/complete_training/final_fixed_gradnorm_model.pt'
    gradnorm_results = None
    
    if Path(gradnorm_model_path).exists():
        gradnorm_results = validator.load_model_and_validate(gradnorm_model_path, "gradnorm")
    else:
        print(f"âš ï¸ GradNorm æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {gradnorm_model_path}")
    
    # é©—è­‰é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯æ¨¡å‹
    dual_backbone_model_path = 'runs/dual_backbone_final_50/complete_training/final_simple_dual_backbone_model.pt'
    dual_backbone_results = None
    
    if Path(dual_backbone_model_path).exists():
        dual_backbone_results = validator.load_model_and_validate(dual_backbone_model_path, "dual_backbone")
    else:
        print(f"âš ï¸ é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {dual_backbone_model_path}")
    
    # ç”Ÿæˆå®Œæ•´å ±å‘Š
    if gradnorm_results or dual_backbone_results:
        report_path, json_path = validator.generate_comprehensive_report(gradnorm_results, dual_backbone_results)
        print(f"\nğŸ‰ å®Œæ•´é©—è­‰æŒ‡æ¨™å ±å‘Šç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“„ æŸ¥çœ‹è©³ç´°å ±å‘Š: {report_path}")
        print(f"ğŸ“Š æŸ¥çœ‹JSONæ•¸æ“š: {json_path}")
    else:
        print("âŒ æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶ï¼Œç„¡æ³•ç”Ÿæˆå ±å‘Š")


if __name__ == "__main__":
    main() 