#!/usr/bin/env python3
"""
æœ€ç°¡åŒ–çš„é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯è¨“ç·´è…³æœ¬
å®Œå…¨æ¶ˆé™¤æ¢¯åº¦å¹²æ“¾ï¼Œç¢ºä¿50 epochè¨“ç·´èƒ½å¤ å®Œæˆ
"""

import argparse
import json
import os
import sys
import time
from pathlib import Path

import torch
import torch.nn as nn
import torch.optim as optim
import yaml
from tqdm import tqdm

# æ·»åŠ è·¯å¾‘
FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))

# æ ¸å¿ƒå°å…¥
from models.yolo import Model
from utils.dataloaders import create_dataloader
from utils.general import colorstr, init_seeds
from utils.torch_utils import select_device
from utils.loss import ComputeLoss

class DetectionBackbone(nn.Module):
    """æª¢æ¸¬å°ˆç”¨éª¨å¹¹ç¶²è·¯"""
    def __init__(self):
        super(DetectionBackbone, self).__init__()
        self.model = Model('models/yolov5s.yaml', ch=3, nc=4, anchors=None)
        
    def forward(self, x):
        return self.model(x)

class ClassificationBackbone(nn.Module):
    """åˆ†é¡å°ˆç”¨éª¨å¹¹ç¶²è·¯"""
    def __init__(self, num_classes=3):
        super(ClassificationBackbone, self).__init__()
        
        # ç¨ç«‹çš„åˆ†é¡backbone
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((7, 7)),
        )
        
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(256 * 7 * 7, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, num_classes)
        )
        
    def forward(self, x):
        features = self.features(x)
        features = features.view(features.size(0), -1)
        return self.classifier(features)

class DualBackboneModel(nn.Module):
    """é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯è¯åˆæ¨¡å‹"""
    def __init__(self, num_classes=3):
        super(DualBackboneModel, self).__init__()
        self.detection_backbone = DetectionBackbone()
        self.classification_backbone = ClassificationBackbone(num_classes=num_classes)
        
    def forward(self, x):
        # å®Œå…¨ç¨ç«‹çš„å‰å‘å‚³æ’­
        detection_outputs = self.detection_backbone(x)
        classification_outputs = self.classification_backbone(x)
        return detection_outputs, classification_outputs

class SimpleDualBackboneTrainer:
    def __init__(self, opt):
        self.opt = opt
        self.device = select_device(opt.device, batch_size=opt.batch_size)
        
        # è¨­ç½®éš¨æ©Ÿç¨®å­
        init_seeds(1, deterministic=True)
        
        # åŠ è¼‰æ•¸æ“šé…ç½®
        with open(opt.data, encoding='utf-8') as f:
            self.data_dict = yaml.safe_load(f)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = DualBackboneModel(num_classes=3).to(self.device)
        
        # è¨­ç½®æå¤±å‡½æ•¸ - æ·»åŠ hypå±¬æ€§
        self.model.detection_backbone.model.hyp = {
            'box': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0,
            'iou_t': 0.20, 'anchor_t': 4.0, 'fl_gamma': 0.0
        }
        self.detection_loss_fn = ComputeLoss(self.model.detection_backbone.model)
        self.classification_loss_fn = nn.CrossEntropyLoss()
        
        # è¨­ç½®å„ªåŒ–å™¨ - åˆ†åˆ¥ç‚ºå…©å€‹ç¶²è·¯è¨­ç½®
        detection_params = list(self.model.detection_backbone.parameters())
        classification_params = list(self.model.classification_backbone.parameters())
        
        self.detection_optimizer = optim.SGD(detection_params, lr=0.01, momentum=0.937, weight_decay=0.0005)
        self.classification_optimizer = optim.SGD(classification_params, lr=0.01, momentum=0.937, weight_decay=0.0005)
        
        # è¨­ç½®æ•¸æ“šåŠ è¼‰å™¨
        self.setup_dataloaders()
        
        # è¨“ç·´æ­·å²
        self.training_history = {
            'epochs': [],
            'detection_losses': [],
            'classification_losses': [],
            'total_losses': []
        }
        
        print(f"âœ… ç°¡åŒ–é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯åˆå§‹åŒ–å®Œæˆï¼Œå…± {opt.epochs} epochs")
        print(f"ğŸ¯ è¨­å‚™: {self.device}")
        print(f"ğŸ” æª¢æ¸¬ç¶²è·¯åƒæ•¸: {sum(p.numel() for p in self.model.detection_backbone.parameters()):,}")
        print(f"ğŸ¯ åˆ†é¡ç¶²è·¯åƒæ•¸: {sum(p.numel() for p in self.model.classification_backbone.parameters()):,}")

    def setup_dataloaders(self):
        """è¨­ç½®æ•¸æ“šåŠ è¼‰å™¨"""
        train_path = str(Path(self.data_dict['path']) / self.data_dict['train'])
        
        # æœ€ç°¡åŒ–çš„è¶…åƒæ•¸
        hyp = {
            'degrees': 0.0, 'translate': 0.0, 'scale': 0.0, 'shear': 0.0, 'perspective': 0.0,
            'flipud': 0.0, 'fliplr': 0.0, 'mosaic': 0.0, 'mixup': 0.0, 'copy_paste': 0.0
        }
        
        self.train_loader, _ = create_dataloader(
            path=train_path,
            imgsz=self.opt.imgsz,
            batch_size=self.opt.batch_size,
            stride=32,
            single_cls=False,
            hyp=hyp,
            augment=False,
            cache=False,
            rect=False,
            rank=-1,
            workers=0,
            image_weights=False,
            quad=False,
            prefix='train: '
        )
        
        print(f"âœ… è¨“ç·´æ•¸æ“šè¼‰å…¥å®Œæˆ: {len(self.train_loader)} batches")

    def train_epoch(self, epoch):
        """è¨“ç·´ä¸€å€‹epoch"""
        self.model.train()
        
        epoch_detection_loss = 0.0
        epoch_classification_loss = 0.0
        epoch_total_loss = 0.0
        
        pbar = tqdm(enumerate(self.train_loader), total=len(self.train_loader), 
                    desc=f"Epoch {epoch+1}/{self.opt.epochs}")
        
        for i, batch in pbar:
            # éˆæ´»è™•ç†æ‰¹æ¬¡æ•¸æ“š - ä½¿ç”¨ç´¢å¼•æ–¹å¼æ›´å®‰å…¨
            imgs = batch[0]
            targets = batch[1]
            paths = batch[2] if len(batch) > 2 else None
            
            imgs = imgs.to(self.device, non_blocking=True).float() / 255
            
            # è™•ç†ç›®æ¨™
            detection_targets = targets.to(self.device)
            # å‰µå»ºéš¨æ©Ÿåˆ†é¡ç›®æ¨™
            classification_targets = torch.randint(0, 3, (imgs.shape[0],)).to(self.device)
            
            # å®Œå…¨ç¨ç«‹çš„å‰å‘å‚³æ’­
            detection_outputs, classification_outputs = self.model(imgs)
            
            # è¨ˆç®—æå¤±
            detection_loss_result = self.detection_loss_fn(detection_outputs, detection_targets)
            detection_loss = detection_loss_result[0] if isinstance(detection_loss_result, tuple) else detection_loss_result
            
            classification_loss = self.classification_loss_fn(classification_outputs, classification_targets)
            
            # å®Œå…¨ç¨ç«‹çš„åå‘å‚³æ’­ - æ²’æœ‰æ¢¯åº¦å¹²æ“¾
            # æª¢æ¸¬ç¶²è·¯æ›´æ–°
            self.detection_optimizer.zero_grad()
            detection_loss.backward(retain_graph=True)
            self.detection_optimizer.step()
            
            # åˆ†é¡ç¶²è·¯æ›´æ–°
            self.classification_optimizer.zero_grad()
            classification_loss.backward()
            self.classification_optimizer.step()
            
            # è¨˜éŒ„æå¤±
            detection_loss_val = detection_loss.item()
            classification_loss_val = classification_loss.item()
            total_loss_val = detection_loss_val + classification_loss_val
            
            epoch_detection_loss += detection_loss_val
            epoch_classification_loss += classification_loss_val
            epoch_total_loss += total_loss_val
            
            # æ›´æ–°é€²åº¦æ¢
            pbar.set_postfix({
                'Det': f'{detection_loss_val:.4f}',
                'Cls': f'{classification_loss_val:.4f}',
                'Total': f'{total_loss_val:.4f}'
            })
            
            # æ¯20å€‹batchæ‰“å°è©³ç´°ä¿¡æ¯
            if i % 20 == 0:
                print(f"  Batch {i}: Detection Loss {detection_loss_val:.4f}, "
                      f"Classification Loss {classification_loss_val:.4f}")
        
        avg_detection_loss = epoch_detection_loss / len(self.train_loader)
        avg_classification_loss = epoch_classification_loss / len(self.train_loader)
        avg_total_loss = epoch_total_loss / len(self.train_loader)
        
        return avg_detection_loss, avg_classification_loss, avg_total_loss

    def train(self):
        """ä¸»è¨“ç·´å¾ªç’°"""
        print(f"ğŸš€ é–‹å§‹ç°¡åŒ–é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯è¨“ç·´ - {self.opt.epochs} epochs")
        print("ğŸ”§ å®Œå…¨æ¶ˆé™¤æ¢¯åº¦å¹²æ“¾ - æª¢æ¸¬å’Œåˆ†é¡ä½¿ç”¨ç¨ç«‹backbone")
        
        start_time = time.time()
        
        for epoch in range(self.opt.epochs):
            print(f"\nğŸ“Š Epoch {epoch + 1}/{self.opt.epochs}")
            
            # è¨“ç·´ä¸€å€‹epoch
            detection_loss, classification_loss, total_loss = self.train_epoch(epoch)
            
            # è¨˜éŒ„æ­·å²
            self.training_history['epochs'].append(epoch + 1)
            self.training_history['detection_losses'].append(detection_loss)
            self.training_history['classification_losses'].append(classification_loss)
            self.training_history['total_losses'].append(total_loss)
            
            print(f"âœ… Epoch {epoch + 1} å®Œæˆ:")
            print(f"   æª¢æ¸¬æå¤±: {detection_loss:.4f}")
            print(f"   åˆ†é¡æå¤±: {classification_loss:.4f}")
            print(f"   ç¸½æå¤±: {total_loss:.4f}")
            print("   ğŸ† æ¢¯åº¦å¹²æ“¾: å®Œå…¨æ¶ˆé™¤ (ç¨ç«‹backbone)")
            
            # æ¯10å€‹epochä¿å­˜ä¸€æ¬¡
            if (epoch + 1) % 10 == 0:
                self.save_checkpoint(epoch + 1)
        
        elapsed_time = time.time() - start_time
        print(f"\nğŸ‰ è¨“ç·´å®Œæˆ! ç”¨æ™‚: {elapsed_time/3600:.2f} å°æ™‚")
        
        # ä¿å­˜æœ€çµ‚çµæœ
        self.save_final_results()
        
        return self.training_history

    def save_checkpoint(self, epoch):
        """ä¿å­˜æª¢æŸ¥é»"""
        save_dir = Path(self.opt.project) / self.opt.name
        save_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'detection_backbone': self.model.detection_backbone.state_dict(),
            'classification_backbone': self.model.classification_backbone.state_dict(),
            'detection_optimizer': self.detection_optimizer.state_dict(),
            'classification_optimizer': self.classification_optimizer.state_dict(),
            'training_history': self.training_history
        }
        
        torch.save(checkpoint, save_dir / f'dual_backbone_epoch_{epoch}.pt')
        print(f"ğŸ’¾ æª¢æŸ¥é»å·²ä¿å­˜: epoch {epoch}")

    def save_final_results(self):
        """ä¿å­˜æœ€çµ‚çµæœ"""
        save_dir = Path(self.opt.project) / self.opt.name
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        final_model = {
            'detection_backbone': self.model.detection_backbone.state_dict(),
            'classification_backbone': self.model.classification_backbone.state_dict(),
            'training_history': self.training_history,
            'config': vars(self.opt)
        }
        
        torch.save(final_model, save_dir / 'final_simple_dual_backbone_model.pt')
        
        # ä¿å­˜è¨“ç·´æ­·å²
        with open(save_dir / 'training_history.json', 'w') as f:
            json.dump(self.training_history, f, indent=2)
        
        print(f"âœ… æœ€çµ‚çµæœå·²ä¿å­˜åˆ°: {save_dir}")
        print(f"ğŸ“Š å…±å®Œæˆ {len(self.training_history['epochs'])} epochs")
        print(f"ğŸ¯ æœ€çµ‚æª¢æ¸¬æå¤±: {self.training_history['detection_losses'][-1]:.4f}")
        print(f"ğŸ¯ æœ€çµ‚åˆ†é¡æå¤±: {self.training_history['classification_losses'][-1]:.4f}")
        print("ğŸ† æ¢¯åº¦å¹²æ“¾: å®Œå…¨æ¶ˆé™¤ (ä½¿ç”¨ç¨ç«‹backbone)")


def parse_opt():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', type=str, required=True, help='dataset.yaml path')
    parser.add_argument('--epochs', type=int, default=50, help='total training epochs')
    parser.add_argument('--batch-size', type=int, default=8, help='total batch size')
    parser.add_argument('--imgsz', type=int, default=640, help='train image size (pixels)')
    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or cpu')
    parser.add_argument('--project', default='runs/simple_dual_backbone', help='save to project/name')
    parser.add_argument('--name', default='exp', help='save to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    
    return parser.parse_args()


def main():
    opt = parse_opt()
    print(f"é–‹å§‹åƒæ•¸: {vars(opt)}")
    
    trainer = SimpleDualBackboneTrainer(opt)
    trainer.train()


if __name__ == "__main__":
    main() 