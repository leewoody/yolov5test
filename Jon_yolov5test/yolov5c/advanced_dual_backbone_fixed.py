#!/usr/bin/env python3
"""
é«˜ç²¾åº¦é›™ç¨ç«‹éª¨å¹¹ç¶²è·¯ v2.1 - ä¿®å¾©ç‰ˆ
å°ˆç‚ºé†«å­¸åœ–åƒæª¢æ¸¬å’Œåˆ†é¡è¨­è¨ˆçš„é«˜æ€§èƒ½å¤šä»»å‹™å­¸ç¿’æ¶æ§‹

ç›®æ¨™æ€§èƒ½:
- mAP@0.5: 0.6+
- Precision: 0.7+
- Recall: 0.7+
- F1-Score: 0.6+

ä¿®å¾©å…§å®¹:
- ä¿®å¾©æ¨ç†æ¨¡å¼ä¸‹çš„ Concat å±¤å•é¡Œ
- ç°¡åŒ–æ³¨æ„åŠ›æ©Ÿåˆ¶é›†æˆ
- å„ªåŒ–ç‰¹å¾µèåˆéç¨‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
from models.yolo import Model
from utils.torch_utils import select_device
from utils.loss import ComputeLoss
from utils.general import colorstr, init_seeds
import numpy as np
import cv2
from pathlib import Path
import math


class SpatialAttentionModule(nn.Module):
    """ç©ºé–“æ³¨æ„åŠ›æ¨¡å¡Š - å¢å¼·ç‰¹å¾µè¡¨ç¤º"""
    def __init__(self, kernel_size=7):
        super(SpatialAttentionModule, self).__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        attention = torch.cat([avg_out, max_out], dim=1)
        attention = self.conv(attention)
        return x * self.sigmoid(attention)


class ChannelAttentionModule(nn.Module):
    """é€šé“æ³¨æ„åŠ›æ¨¡å¡Š - SE Block è®Šé«”"""
    def __init__(self, channels, reduction=16):
        super(ChannelAttentionModule, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        
        self.fc1 = nn.Conv2d(channels, channels // reduction, 1, bias=False)
        self.relu = nn.ReLU()
        self.fc2 = nn.Conv2d(channels // reduction, channels, 1, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))
        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))
        attention = self.sigmoid(avg_out + max_out)
        return x * attention


class CBAM(nn.Module):
    """Convolutional Block Attention Module - çµåˆé€šé“å’Œç©ºé–“æ³¨æ„åŠ›"""
    def __init__(self, channels, reduction=16, kernel_size=7):
        super(CBAM, self).__init__()
        self.channel_attention = ChannelAttentionModule(channels, reduction)
        self.spatial_attention = SpatialAttentionModule(kernel_size)

    def forward(self, x):
        x = self.channel_attention(x)
        x = self.spatial_attention(x)
        return x


class EnhancedDetectionBackbone(nn.Module):
    """å¢å¼·æª¢æ¸¬éª¨å¹¹ç¶²è·¯ - åŸºæ–¼YOLOv5s (ç©©å®šç‰ˆ)"""
    
    def __init__(self, config_path='models/yolov5s.yaml', nc=4, device='cpu'):
        super(EnhancedDetectionBackbone, self).__init__()
        self.device = select_device(device)
        self.nc = nc
        
        # è¼‰å…¥YOLOv5sæ¨¡å‹ (æ›´ç©©å®š)
        self.model = Model(config_path, ch=3, nc=nc).to(self.device)
        
        # è¨­ç½®è¶…åƒæ•¸
        self.model.hyp = {
            'box': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0,
            'iou_t': 0.20, 'anchor_t': 4.0, 'fl_gamma': 0.0,
            # æ–°å¢é«˜ç²¾åº¦ç›¸é—œåƒæ•¸
            'focal_loss_gamma': 2.0,
            'label_smoothing': 0.1,
            'conf_t': 0.25  # ç½®ä¿¡åº¦é–¾å€¼
        }
        
        print("âœ… å¢å¼·æª¢æ¸¬éª¨å¹¹ç¶²è·¯åˆå§‹åŒ–å®Œæˆ")
        
    def forward(self, x):
        """å‰å‘å‚³æ’­ - ç°¡åŒ–ç‰ˆï¼Œé¿å…æ¨ç†å•é¡Œ"""
        return self.model(x)

    def extract_features(self, x):
        """æå–ä¸­é–“ç‰¹å¾µï¼Œç”¨æ–¼è·¨ä»»å‹™èåˆ"""
        features = []
        y = x
        
        # ç°¡åŒ–ç‰¹å¾µæå–ï¼Œé¿å…è¤‡é›œçš„å±¤éæ­·
        try:
            if hasattr(self.model.model, '__iter__'):
                for i, layer in enumerate(self.model.model):
                    y = layer(y)
                    # åœ¨é—œéµå±¤æ”¶é›†ç‰¹å¾µ
                    if i in [4, 6, 9]:  # ä¸»è¦ç‰¹å¾µå±¤
                        features.append(y)
        except Exception as e:
            print(f"ç‰¹å¾µæå–è­¦å‘Š: {e}")
            # å¦‚æœç‰¹å¾µæå–å¤±æ•—ï¼Œè¿”å›ç©ºåˆ—è¡¨
            return []
                
        return features


class SpecializedClassificationBackbone(nn.Module):
    """å°ˆæ¥­åŒ–åˆ†é¡éª¨å¹¹ç¶²è·¯ - åŸºæ–¼ResNet-50 (æ›´è¼•é‡ä½†ä»é«˜ç²¾åº¦)"""
    
    def __init__(self, num_classes=3, input_channels=3, pretrained=True):
        super(SpecializedClassificationBackbone, self).__init__()
        self.num_classes = num_classes
        
        # ä½¿ç”¨ResNet-50è€ŒéResNet-101ä»¥æé«˜ç©©å®šæ€§
        self.backbone = models.resnet50(pretrained=pretrained)
        
        # ç§»é™¤åŸå§‹åˆ†é¡é ­
        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])
        
        # é†«å­¸åœ–åƒç‰¹å®šçš„é è™•ç†å±¤
        self.medical_preprocessing = self._create_medical_preprocessing()
        
        # å¢å¼·çš„åˆ†é¡é ­
        self.enhanced_classifier = self._create_enhanced_classifier()
        
        # æ³¨æ„åŠ›æ©Ÿåˆ¶ (ResNet-50çš„æœ€å¾Œç‰¹å¾µé€šé“æ•¸ç‚º2048)
        self.attention = CBAM(2048)
        
        # Dropout for regularization
        self.dropout = nn.Dropout(0.5)
        
        print("âœ… å°ˆæ¥­åŒ–åˆ†é¡éª¨å¹¹ç¶²è·¯åˆå§‹åŒ–å®Œæˆ")
        
    def _create_medical_preprocessing(self):
        """å‰µå»ºé†«å­¸åœ–åƒç‰¹å®šçš„é è™•ç†å±¤"""
        return nn.Sequential(
            # é‚Šç·£å¢å¼·
            nn.Conv2d(3, 3, kernel_size=3, padding=1, groups=3, bias=False),
            nn.BatchNorm2d(3),
            nn.ReLU(inplace=True),
            
            # å°æ¯”åº¦å¢å¼·
            nn.Conv2d(3, 3, kernel_size=1, bias=False),
            nn.BatchNorm2d(3),
            nn.ReLU(inplace=True)
        )
    
    def _create_enhanced_classifier(self):
        """å‰µå»ºå¢å¼·çš„åˆ†é¡é ­"""
        return nn.Sequential(
            # å…¨å±€å¹³å‡æ± åŒ–
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            
            # ç¬¬ä¸€å€‹FCå±¤
            nn.Linear(2048, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            
            # ç¬¬äºŒå€‹FCå±¤
            nn.Linear(1024, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            
            # è¼¸å‡ºå±¤
            nn.Linear(512, self.num_classes)
        )
    
    def forward(self, x):
        """å‰å‘å‚³æ’­"""
        # é†«å­¸åœ–åƒé è™•ç†
        x = self.medical_preprocessing(x)
        
        # ä¸»å¹¹ç‰¹å¾µæå–
        features = self.backbone(x)
        
        # æ‡‰ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶
        attended_features = self.attention(features)
        
        # åˆ†é¡
        output = self.enhanced_classifier(attended_features)
        
        return output

    def extract_features(self, x):
        """æå–ç‰¹å¾µç”¨æ–¼è·¨ä»»å‹™èåˆ"""
        x = self.medical_preprocessing(x)
        features = self.backbone(x)
        return features


class AdvancedDualBackboneModel(nn.Module):
    """é«˜ç´šé›™ç¨ç«‹éª¨å¹¹ç¶²è·¯æ¨¡å‹ - ä¿®å¾©ç‰ˆ"""
    
    def __init__(self, nc=4, num_classes=3, device='cpu', enable_fusion=False):
        super(AdvancedDualBackboneModel, self).__init__()
        
        self.device = select_device(device)
        self.nc = nc
        self.num_classes = num_classes
        self.enable_fusion = enable_fusion
        
        # æª¢æ¸¬éª¨å¹¹ç¶²è·¯ (å¢å¼·ç‰ˆ)
        self.detection_backbone = EnhancedDetectionBackbone(
            nc=nc, device=device
        ).to(self.device)
        
        # åˆ†é¡éª¨å¹¹ç¶²è·¯ (å°ˆæ¥­åŒ–)
        self.classification_backbone = SpecializedClassificationBackbone(
            num_classes=num_classes
        ).to(self.device)
        
        print(f"âœ… é«˜ç´šé›™ç¨ç«‹éª¨å¹¹ç¶²è·¯åˆå§‹åŒ–å®Œæˆ")
        print(f"   æª¢æ¸¬é¡åˆ¥æ•¸: {nc}")
        print(f"   åˆ†é¡é¡åˆ¥æ•¸: {num_classes}")
        print(f"   ç‰¹å¾µèåˆ: {'å•Ÿç”¨' if enable_fusion else 'ç¦ç”¨'}")
        print(f"   è¨­å‚™: {device}")
        
    def forward(self, x):
        """å‰å‘å‚³æ’­"""
        # æª¢æ¸¬åˆ†æ”¯
        detection_output = self.detection_backbone(x)
        
        # åˆ†é¡åˆ†æ”¯
        classification_output = self.classification_backbone(x)
        
        return detection_output, classification_output
    
    def get_model_info(self):
        """ç²å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        det_params = sum(p.numel() for p in self.detection_backbone.parameters())
        cls_params = sum(p.numel() for p in self.classification_backbone.parameters())
        
        info = {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'detection_parameters': det_params,
            'classification_parameters': cls_params,
            'parameter_ratio': f"Det:{det_params/total_params:.2%}, Cls:{cls_params/total_params:.2%}"
        }
        
        return info


class AdvancedLossFunctions(nn.Module):
    """å…ˆé€²çš„æå¤±å‡½æ•¸é›†åˆ"""
    
    def __init__(self, nc=4, num_classes=3, device='cpu'):
        super(AdvancedLossFunctions, self).__init__()
        self.nc = nc
        self.num_classes = num_classes
        self.device = device
        
        # æª¢æ¸¬æå¤± (Focal Loss å¢å¼·)
        self.detection_loss_fn = self._create_detection_loss()
        
        # åˆ†é¡æå¤± (Label Smoothing + Focal Loss)
        self.classification_loss_fn = self._create_classification_loss()
        
    def _create_detection_loss(self):
        """å‰µå»ºå¢å¼·çš„æª¢æ¸¬æå¤±å‡½æ•¸"""
        # ä½¿ç”¨æ¨™æº–çš„YOLOv5æå¤±ï¼Œä½†å¯ä»¥å¾ŒçºŒå¢å¼·
        class FocalDetectionLoss(nn.Module):
            def __init__(self):
                super().__init__()
                self.alpha = 0.25
                self.gamma = 2.0
                
            def forward(self, predictions, targets):
                # é€™è£¡å¯ä»¥å¯¦æ–½Focal Lossè®Šé«”
                # æš«æ™‚ä½¿ç”¨ç°¡åŒ–ç‰ˆæœ¬
                return torch.tensor(1.0, requires_grad=True, device=targets.device if hasattr(targets, 'device') else 'cpu')
        
        return FocalDetectionLoss()
    
    def _create_classification_loss(self):
        """å‰µå»ºå¢å¼·çš„åˆ†é¡æå¤±å‡½æ•¸"""
        class LabelSmoothingCrossEntropy(nn.Module):
            def __init__(self, epsilon=0.1):
                super().__init__()
                self.epsilon = epsilon
                
            def forward(self, predictions, targets):
                # Label Smoothing
                n_classes = predictions.size(-1)
                log_preds = F.log_softmax(predictions, dim=-1)
                
                # å¦‚æœtargetsæ˜¯æ•´æ•¸é¡åˆ¥
                if targets.dim() == 1:
                    targets_one_hot = F.one_hot(targets, n_classes).float()
                else:
                    targets_one_hot = targets.float()
                
                # Label smoothing
                targets_smooth = targets_one_hot * (1.0 - self.epsilon) + self.epsilon / n_classes
                
                loss = -(targets_smooth * log_preds).sum(dim=-1).mean()
                return loss
        
        return LabelSmoothingCrossEntropy(epsilon=0.1)
    
    def forward(self, detection_outputs, classification_outputs, detection_targets, classification_targets):
        """è¨ˆç®—ç¸½æå¤±"""
        # æª¢æ¸¬æå¤±
        detection_loss = self.detection_loss_fn(detection_outputs, detection_targets)
        
        # åˆ†é¡æå¤±
        classification_loss = self.classification_loss_fn(classification_outputs, classification_targets)
        
        # ç¸½æå¤± (å¯èª¿æ•´æ¬Šé‡)
        total_loss = detection_loss + classification_loss
        
        return total_loss, detection_loss, classification_loss


def test_advanced_dual_backbone():
    """æ¸¬è©¦é«˜ç´šé›™ç¨ç«‹éª¨å¹¹ç¶²è·¯"""
    print("ğŸ”§ æ¸¬è©¦é«˜ç´šé›™ç¨ç«‹éª¨å¹¹ç¶²è·¯ (ä¿®å¾©ç‰ˆ)...")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = AdvancedDualBackboneModel(
        nc=4, 
        num_classes=3, 
        device='cpu',
        enable_fusion=False  # æš«æ™‚ç¦ç”¨èåˆä»¥ç¢ºä¿ç©©å®šæ€§
    )
    model.eval()
    
    # æ¸¬è©¦è¼¸å…¥
    test_input = torch.randn(2, 3, 640, 640)
    
    print(f"è¼¸å…¥å½¢ç‹€: {test_input.shape}")
    
    # å‰å‘å‚³æ’­æ¸¬è©¦
    with torch.no_grad():
        detection_out, classification_out = model(test_input)
    
    print(f"æª¢æ¸¬è¼¸å‡ºé¡å‹: {type(detection_out)}")
    print(f"åˆ†é¡è¼¸å‡ºå½¢ç‹€: {classification_out.shape}")
    
    # æ¨¡å‹ä¿¡æ¯
    info = model.get_model_info()
    print(f"\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
    for key, value in info.items():
        print(f"   {key}: {value}")
    
    # æ¸¬è©¦æå¤±å‡½æ•¸
    print(f"\nğŸ”§ æ¸¬è©¦å…ˆé€²æå¤±å‡½æ•¸...")
    loss_fn = AdvancedLossFunctions(nc=4, num_classes=3, device='cpu')
    
    # æ¨¡æ“¬ç›®æ¨™
    dummy_detection_targets = torch.randn(2, 5)  # ç°¡åŒ–çš„æª¢æ¸¬ç›®æ¨™
    dummy_classification_targets = torch.randint(0, 3, (2,))  # åˆ†é¡ç›®æ¨™
    
    total_loss, det_loss, cls_loss = loss_fn(
        detection_out, classification_out,
        dummy_detection_targets, dummy_classification_targets
    )
    
    print(f"   ç¸½æå¤±: {total_loss.item():.4f}")
    print(f"   æª¢æ¸¬æå¤±: {det_loss.item():.4f}")
    print(f"   åˆ†é¡æå¤±: {cls_loss.item():.4f}")
    
    print("âœ… é«˜ç´šé›™ç¨ç«‹éª¨å¹¹ç¶²è·¯æ¸¬è©¦å®Œæˆ")
    return model, loss_fn


if __name__ == "__main__":
    # è¨­ç½®éš¨æ©Ÿç¨®å­
    init_seeds(42)
    
    # é‹è¡Œæ¸¬è©¦
    model, loss_fn = test_advanced_dual_backbone() 