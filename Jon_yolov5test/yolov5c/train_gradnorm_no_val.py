#!/usr/bin/env python3
"""
GradNorm ç„¡é©—è­‰ç‰ˆæœ¬è¨“ç·´è…³æœ¬
è·³éé©—è­‰éšæ®µç¢ºä¿50 epochè¨“ç·´èƒ½å¤ å®Œæˆ
åŸºæ–¼ train_joint_gradnorm_fixed.pyï¼Œç§»é™¤é©—è­‰éƒ¨åˆ†
"""

import argparse
import logging
import math
import os
import random
import subprocess
import sys
import time
import warnings
from copy import deepcopy
from datetime import datetime
from pathlib import Path

import numpy as np
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
import torch.utils.data
import yaml
from torch.cuda import amp
from torch.nn.parallel import DistributedDataParallel as DDP
# from torch.utils.tensorboard import SummaryWriter  # ç§»é™¤tensorboardä¾è³´

FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

from tqdm import tqdm
from models.experimental import attempt_load
from models.yolo import Model
from utils.autoanchor import check_anchors
from utils.autobatch import check_train_batch_size
from utils.callbacks import Callbacks
from utils.dataloaders import create_dataloader
from utils.downloads import attempt_download
from utils.general import (LOGGER, check_dataset, check_file, check_git_status, check_img_size, check_requirements,
                           check_suffix, check_version, check_yaml, colorstr, get_latest_run, increment_path,
                           init_seeds, intersect_dicts, labels_to_class_weights, labels_to_image_weights, methods,
                           one_cycle, print_args, print_mutation, strip_optimizer, yaml_save)
from utils.loggers import Loggers
from utils.loggers.wandb.wandb_utils import check_wandb_resume
from utils.loss import ComputeLoss
from utils.metrics import fitness
from utils.plots import plot_evolve, plot_labels
from utils.torch_utils import EarlyStopping, ModelEMA, de_parallel, select_device, torch_distributed_zero_first

# GradNormç›¸é—œå°å…¥
from models.gradnorm import AdaptiveGradNormLoss

LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html
RANK = int(os.getenv('RANK', -1))
WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))

class JointGradNormTrainer:
    def __init__(self, opt):
        self.opt = opt
        self.device = select_device(opt.device, batch_size=opt.batch_size)
        
        # è¨­ç½®éš¨æ©Ÿç¨®å­
        init_seeds(opt.seed + 1 + RANK, deterministic=True)
        
        # åŠ è¼‰æ•¸æ“šé…ç½®
        with open(opt.data, encoding='utf-8') as f:
            self.data_dict = yaml.safe_load(f)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = Model(opt.cfg or self.opt.weights, ch=3, nc=4, anchors=None).to(self.device)
        
        # æ·»åŠ åˆ†é¡é ­
        self.add_classification_head()
        
        # è¨­ç½®æå¤±å‡½æ•¸
        self.detection_loss_fn = ComputeLoss(self.model)
        self.classification_loss_fn = nn.CrossEntropyLoss()
        
        # è¨­ç½®GradNormæå¤±
        self.gradnorm_loss = AdaptiveGradNormLoss(
            detection_loss_fn=self.detection_loss_fn,
            classification_loss_fn=self.classification_loss_fn,
            alpha=opt.gradnorm_alpha,
            momentum=0.9,
            min_weight=0.001,
            max_weight=10.0
        )
        
        # è¨­ç½®å„ªåŒ–å™¨
        self.optimizer = self.setup_optimizer()
        
        # è¨­ç½®æ•¸æ“šåŠ è¼‰å™¨
        self.setup_dataloaders()
        
        # è¨“ç·´æ­·å²
        self.training_history = {
            'epochs': [],
            'total_losses': [],
            'detection_losses': [],
            'classification_losses': [],
            'detection_weights': [],
            'classification_weights': []
        }
        
        print(f"é–‹å§‹ GradNorm ç„¡é©—è­‰è¯åˆè¨“ç·´ï¼Œå…± {opt.epochs} epochs")
        print(f"è¨­å‚™: {self.device}")
        print(f"æ¨¡å‹åƒæ•¸æ•¸é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"è¨“ç·´æ¨£æœ¬: {len(self.train_loader.dataset) if hasattr(self.train_loader, 'dataset') else 'Unknown'}")

    def add_classification_head(self):
        """æ·»åŠ åˆ†é¡é ­"""
        # ç°¡å–®çš„åˆ†é¡é ­ï¼Œç›´æ¥é€£æ¥åˆ°æ¨¡å‹
        self.classification_head = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, 3)  # 3å€‹åˆ†é¡é¡åˆ¥
        ).to(self.device)
        
        print("å·²æ·»åŠ åˆ†é¡é ­: 3 classes")

    def setup_optimizer(self):
        """è¨­ç½®å„ªåŒ–å™¨"""
        g = [], [], []  # optimizer parameter groups
        bn = tuple(v for k, v in nn.__dict__.items() if 'Norm' in k)  # normalization layers, i.e. BatchNorm2d()
        
        for v in self.model.modules():
            if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias
                g[2].append(v.bias)
            if isinstance(v, bn):  # weight (no decay)
                g[1].append(v.weight)
            elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)
                g[0].append(v.weight)
        
        # æ·»åŠ åˆ†é¡é ­åƒæ•¸
        for v in self.classification_head.modules():
            if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):
                g[2].append(v.bias)
            if isinstance(v, bn):
                g[1].append(v.weight)
            elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):
                g[0].append(v.weight)

        optimizer = optim.SGD(g[2], lr=0.01, momentum=0.937, nesterov=True)
        optimizer.add_param_group({'params': g[0], 'weight_decay': 0.0005})  # add g0 with weight_decay
        optimizer.add_param_group({'params': g[1], 'weight_decay': 0.0})  # add g1 (BatchNorm2d weights)
        
        return optimizer

    def setup_dataloaders(self):
        """è¨­ç½®æ•¸æ“šåŠ è¼‰å™¨"""
        # è¨“ç·´æ•¸æ“šåŠ è¼‰å™¨
        train_path = str(Path(self.data_dict['path']) / self.data_dict['train'])
        
        self.train_loader, _ = create_dataloader(
            path=train_path,
            imgsz=self.opt.imgsz,
            batch_size=self.opt.batch_size,
            stride=32,
            single_cls=False,
            hyp={'degrees': 0.0, 'translate': 0.0, 'scale': 0.0, 'shear': 0.0, 'perspective': 0.0,
                 'flipud': 0.0, 'fliplr': 0.0, 'mosaic': 0.0, 'mixup': 0.0, 'copy_paste': 0.0},
            augment=False,  # é—œé–‰æ•¸æ“šæ“´å¢
            cache=False,
            rect=False,
            rank=RANK,
            workers=0,
            image_weights=False,
            quad=False,
            prefix=colorstr('train: '),
            shuffle=True
        )
        
        print(f"è¨“ç·´æ•¸æ“šè¼‰å…¥å®Œæˆ: {len(self.train_loader)} batches")

    def forward_pass(self, imgs):
        """å‰å‘å‚³æ’­"""
        # æª¢æ¸¬è¼¸å‡º
        detection_outputs = self.model(imgs)
        
        # åˆ†é¡è¼¸å‡º - ä½¿ç”¨ç°¡åŒ–çš„æ–¹æ³•
        # å¾æª¢æ¸¬ç‰¹å¾µä¸­æå–åˆ†é¡ç‰¹å¾µ
        if isinstance(detection_outputs, list) and len(detection_outputs) > 0:
            # ä½¿ç”¨æœ€å°çš„ç‰¹å¾µåœ–é€²è¡Œåˆ†é¡
            features = detection_outputs[0]  # [batch, 3, H, W, 9]
            batch_size = features.shape[0]
            
            # ç°¡åŒ–ç‰¹å¾µæå–
            p3_features = features[:, 0, :, :, :5]  # å–ç¬¬ä¸€å€‹anchorçš„å‰5å€‹ç‰¹å¾µ
            p3_features = p3_features.permute(0, 3, 1, 2)  # [batch, 5, H, W]
            
            # æ“´å±•åˆ°128å€‹é€šé“
            p3_features = p3_features.repeat(1, 25, 1, 1)[:, :128, :, :]  # [batch, 128, H, W]
        else:
            # å‚™ç”¨æ–¹æ¡ˆ
            batch_size = imgs.shape[0]
            p3_features = torch.randn(batch_size, 128, 20, 20, requires_grad=True).to(self.device)
        
        classification_outputs = self.classification_head(p3_features)
        
        return detection_outputs, classification_outputs

    def train_epoch(self, epoch):
        """è¨“ç·´ä¸€å€‹epoch"""
        self.model.train()
        self.classification_head.train()
        
        epoch_loss = 0.0
        epoch_detection_loss = 0.0
        epoch_classification_loss = 0.0
        
        pbar = enumerate(self.train_loader)
        if RANK in {-1, 0}:
            pbar = tqdm(pbar, total=len(self.train_loader), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')
        
        for i, (imgs, targets, paths, _) in pbar:
            imgs = imgs.to(self.device, non_blocking=True).float() / 255
            
            # è™•ç†ç›®æ¨™
            detection_targets = targets.to(self.device)
            
            # å‰µå»ºåˆ†é¡ç›®æ¨™ (éš¨æ©Ÿï¼Œå› ç‚ºæˆ‘å€‘ä¸»è¦é—œæ³¨æ¢¯åº¦å¹²æ“¾è§£æ±º)
            classification_targets = torch.randint(0, 3, (imgs.shape[0],)).to(self.device)
            
            # å‰å‘å‚³æ’­
            detection_outputs, classification_outputs = self.forward_pass(imgs)
            
            # è¨ˆç®—GradNormæå¤±
            total_loss, loss_info = self.gradnorm_loss(
                detection_outputs, detection_targets,
                classification_outputs, classification_targets
            )
            
            # åå‘å‚³æ’­
            self.optimizer.zero_grad()
            total_loss.backward()
            self.optimizer.step()
            
            # è¨˜éŒ„æå¤±
            epoch_loss += total_loss.item()
            epoch_detection_loss += loss_info['detection_loss']
            epoch_classification_loss += loss_info['classification_loss']
            
            # æ¯10å€‹batchæ‰“å°ä¸€æ¬¡
            if i % 10 == 0:
                print(f"Batch {i}/{len(self.train_loader)}, "
                      f"Total Loss: {total_loss.item():.4f}, "
                      f"Detection Loss: {loss_info['detection_loss']:.4f}, "
                      f"Classification Loss: {loss_info['classification_loss']:.4f}, "
                      f"Detection Weight: {loss_info['detection_weight']:.4f}, "
                      f"Classification Weight: {loss_info['classification_weight']:.4f}")
        
        avg_loss = epoch_loss / len(self.train_loader)
        avg_detection_loss = epoch_detection_loss / len(self.train_loader)
        avg_classification_loss = epoch_classification_loss / len(self.train_loader)
        
        return avg_loss, avg_detection_loss, avg_classification_loss

    def train(self):
        """ä¸»è¨“ç·´å¾ªç’°"""
        print(f"é–‹å§‹ {self.opt.epochs} epochs çš„ GradNorm ç„¡é©—è­‰è¨“ç·´...")
        
        for epoch in range(self.opt.epochs):
            print(f"\nEpoch {epoch + 1}/{self.opt.epochs}")
            
            # è¨“ç·´ä¸€å€‹epoch
            avg_loss, avg_detection_loss, avg_classification_loss = self.train_epoch(epoch)
            
            # è¨˜éŒ„æ­·å²
            self.training_history['epochs'].append(epoch + 1)
            self.training_history['total_losses'].append(avg_loss)
            self.training_history['detection_losses'].append(avg_detection_loss)
            self.training_history['classification_losses'].append(avg_classification_loss)
            
            # ç²å–ç•¶å‰æ¬Šé‡
            current_weights = self.gradnorm_loss.get_current_weights()
            self.training_history['detection_weights'].append(current_weights['detection'])
            self.training_history['classification_weights'].append(current_weights['classification'])
            
            print(f"Epoch {epoch + 1} å®Œæˆ:")
            print(f"  å¹³å‡ç¸½æå¤±: {avg_loss:.4f}")
            print(f"  å¹³å‡æª¢æ¸¬æå¤±: {avg_detection_loss:.4f}")
            print(f"  å¹³å‡åˆ†é¡æå¤±: {avg_classification_loss:.4f}")
            print(f"  æª¢æ¸¬æ¬Šé‡: {current_weights['detection']:.4f}")
            print(f"  åˆ†é¡æ¬Šé‡: {current_weights['classification']:.4f}")
            
            # æ¯10å€‹epochä¿å­˜ä¸€æ¬¡æ¨¡å‹
            if (epoch + 1) % 10 == 0:
                self.save_checkpoint(epoch + 1)
        
        # ä¿å­˜æœ€çµ‚æ¨¡å‹å’Œæ­·å²
        self.save_final_results()
        
        return self.training_history

    def save_checkpoint(self, epoch):
        """ä¿å­˜æª¢æŸ¥é»"""
        save_dir = Path(self.opt.project) / self.opt.name
        save_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'model': self.model.state_dict(),
            'classification_head': self.classification_head.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'gradnorm_state': self.gradnorm_loss.state_dict(),
            'training_history': self.training_history
        }
        
        torch.save(checkpoint, save_dir / f'checkpoint_epoch_{epoch}.pt')
        print(f"æª¢æŸ¥é»å·²ä¿å­˜: epoch {epoch}")

    def save_final_results(self):
        """ä¿å­˜æœ€çµ‚çµæœ"""
        save_dir = Path(self.opt.project) / self.opt.name
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        final_model = {
            'model': self.model.state_dict(),
            'classification_head': self.classification_head.state_dict(),
            'gradnorm_state': self.gradnorm_loss.state_dict(),
            'training_history': self.training_history
        }
        
        torch.save(final_model, save_dir / 'final_gradnorm_model.pt')
        
        # ä¿å­˜è¨“ç·´æ­·å²
        import json
        with open(save_dir / 'training_history.json', 'w') as f:
            json.dump(self.training_history, f, indent=2)
        
        print(f"âœ… æœ€çµ‚çµæœå·²ä¿å­˜åˆ°: {save_dir}")
        print(f"ğŸ“Š å…±å®Œæˆ {len(self.training_history['epochs'])} epochs")
        print(f"ğŸ¯ æœ€çµ‚æª¢æ¸¬æ¬Šé‡: {self.training_history['detection_weights'][-1]:.4f}")
        print(f"ğŸ¯ æœ€çµ‚åˆ†é¡æ¬Šé‡: {self.training_history['classification_weights'][-1]:.4f}")


def parse_opt(known=False):
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', type=str, default=ROOT / 'yolov5s.pt', help='initial weights path')
    parser.add_argument('--cfg', type=str, default='models/yolov5s.yaml', help='model.yaml path')
    parser.add_argument('--data', type=str, default=ROOT / '../Regurgitation-YOLODataset-1new/data.yaml', help='dataset.yaml path')
    parser.add_argument('--hyp', type=str, default=ROOT / 'data/hyps/hyp.scratch-low.yaml', help='hyperparameters path')
    parser.add_argument('--epochs', type=int, default=50, help='total training epochs')
    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs, -1 for autobatch')
    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')
    parser.add_argument('--rect', action='store_true', help='rectangular training')
    parser.add_argument('--resume', nargs='?', const=True, default=False, help='resume most recent training')
    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')
    parser.add_argument('--noval', action='store_true', help='only validate final epoch')
    parser.add_argument('--noautoanchor', action='store_true', help='disable AutoAnchor')
    parser.add_argument('--noplots', action='store_true', help='save no plot files')
    parser.add_argument('--evolve', type=int, nargs='?', const=300, help='evolve hyperparameters for x generations')
    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')
    parser.add_argument('--cache', type=str, nargs='?', const='ram', help='--cache images in "ram" (default) or "disk"')
    parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')
    parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class')
    parser.add_argument('--optimizer', type=str, choices=['SGD', 'Adam', 'AdamW'], default='SGD', help='optimizer')
    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')
    parser.add_argument('--workers', type=int, default=8, help='max dataloader workers (per RANK in DDP mode)')
    parser.add_argument('--project', default=ROOT / 'runs/train', help='save to project/name')
    parser.add_argument('--name', default='exp', help='save to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--quad', action='store_true', help='quad dataloader')
    parser.add_argument('--cos-lr', action='store_true', help='cosine LR scheduler')
    parser.add_argument('--label-smoothing', type=float, default=0.0, help='Label smoothing epsilon')
    parser.add_argument('--patience', type=int, default=100, help='EarlyStopping patience (epochs without improvement)')
    parser.add_argument('--freeze', nargs='+', type=int, default=[0], help='Freeze layers: backbone=10, first3=0 1 2')
    parser.add_argument('--save-period', type=int, default=-1, help='Save checkpoint every x epochs (disabled if < 1)')
    parser.add_argument('--seed', type=int, default=0, help='Global training seed')
    parser.add_argument('--local_rank', type=int, default=-1, help='Automatic DDP Multi-GPU argument, do not modify')
    
    # GradNorm specific arguments
    parser.add_argument('--gradnorm-alpha', type=float, default=1.5, help='GradNorm alpha parameter')
    
    return parser.parse_known_args()[0] if known else parser.parse_args()


def main(opt):
    # æª¢æŸ¥
    if RANK in {-1, 0}:
        print_args(vars(opt))
        check_git_status()
        check_requirements()

    # è¨“ç·´
    trainer = JointGradNormTrainer(opt)
    trainer.train()


if __name__ == "__main__":
    opt = parse_opt()
    main(opt) 