#!/usr/bin/env python3
"""
æœ€ç°¡åŒ–çš„GradNormè¨“ç·´è…³æœ¬
åªä¿ç•™æ ¸å¿ƒåŠŸèƒ½ï¼Œç¢ºä¿50 epochè¨“ç·´èƒ½å¤ å®Œæˆ
"""

import argparse
import json
import os
import sys
import time
from pathlib import Path

import torch
import torch.nn as nn
import torch.optim as optim
import yaml
from tqdm import tqdm

# æ·»åŠ è·¯å¾‘
FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))

# æ ¸å¿ƒå°å…¥
from models.yolo import Model
from models.gradnorm import AdaptiveGradNormLoss
from utils.dataloaders import create_dataloader
from utils.general import colorstr, init_seeds
from utils.torch_utils import select_device
from utils.loss import ComputeLoss

class SimpleGradNormTrainer:
    def __init__(self, opt):
        self.opt = opt
        self.device = select_device(opt.device, batch_size=opt.batch_size)
        
        # è¨­ç½®éš¨æ©Ÿç¨®å­
        init_seeds(1, deterministic=True)
        
        # åŠ è¼‰æ•¸æ“šé…ç½®
        with open(opt.data, encoding='utf-8') as f:
            self.data_dict = yaml.safe_load(f)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = Model('models/yolov5s.yaml', ch=3, nc=4, anchors=None).to(self.device)
        
        # æ·»åŠ åˆ†é¡é ­
        self.classification_head = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, 3)
        ).to(self.device)
        
        # è¨­ç½®æå¤±å‡½æ•¸ - æ·»åŠ hypå±¬æ€§
        self.model.hyp = {
            'box': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0,
            'iou_t': 0.20, 'anchor_t': 4.0, 'fl_gamma': 0.0
        }
        self.detection_loss_fn = ComputeLoss(self.model)
        self.classification_loss_fn = nn.CrossEntropyLoss()
        
        # è¨­ç½®GradNormæå¤±
        self.gradnorm_loss = AdaptiveGradNormLoss(
            detection_loss_fn=self.detection_loss_fn,
            classification_loss_fn=self.classification_loss_fn,
            alpha=opt.gradnorm_alpha,
            momentum=0.9,
            min_weight=0.001,
            max_weight=10.0
        )
        
        # è¨­ç½®å„ªåŒ–å™¨
        all_params = list(self.model.parameters()) + list(self.classification_head.parameters())
        self.optimizer = optim.SGD(all_params, lr=0.01, momentum=0.937, weight_decay=0.0005)
        
        # è¨­ç½®æ•¸æ“šåŠ è¼‰å™¨
        self.setup_dataloaders()
        
        # è¨“ç·´æ­·å²
        self.training_history = {
            'epochs': [],
            'total_losses': [],
            'detection_losses': [],
            'classification_losses': [],
            'detection_weights': [],
            'classification_weights': []
        }
        
        print(f"âœ… ç°¡åŒ–GradNormè¨“ç·´å™¨åˆå§‹åŒ–å®Œæˆï¼Œå…± {opt.epochs} epochs")
        print(f"ğŸ¯ è¨­å‚™: {self.device}")

    def setup_dataloaders(self):
        """è¨­ç½®æ•¸æ“šåŠ è¼‰å™¨"""
        train_path = str(Path(self.data_dict['path']) / self.data_dict['train'])
        
        # æœ€ç°¡åŒ–çš„è¶…åƒæ•¸
        hyp = {
            'degrees': 0.0, 'translate': 0.0, 'scale': 0.0, 'shear': 0.0, 'perspective': 0.0,
            'flipud': 0.0, 'fliplr': 0.0, 'mosaic': 0.0, 'mixup': 0.0, 'copy_paste': 0.0
        }
        
        self.train_loader, _ = create_dataloader(
            path=train_path,
            imgsz=self.opt.imgsz,
            batch_size=self.opt.batch_size,
            stride=32,
            single_cls=False,
            hyp=hyp,
            augment=False,
            cache=False,
            rect=False,
            rank=-1,
            workers=0,
            image_weights=False,
            quad=False,
            prefix='train: ',
            shuffle=True
        )
        
        print(f"âœ… è¨“ç·´æ•¸æ“šè¼‰å…¥å®Œæˆ: {len(self.train_loader)} batches")

    def forward_pass(self, imgs):
        """å‰å‘å‚³æ’­"""
        # æª¢æ¸¬è¼¸å‡º
        detection_outputs = self.model(imgs)
        
        # åˆ†é¡è¼¸å‡º - ä½¿ç”¨ç°¡åŒ–æ–¹æ³•
        batch_size = imgs.shape[0]
        # å‰µå»ºç°¡å–®çš„ç‰¹å¾µç”¨æ–¼åˆ†é¡
        p3_features = torch.randn(batch_size, 128, 20, 20, requires_grad=True).to(self.device)
        classification_outputs = self.classification_head(p3_features)
        
        return detection_outputs, classification_outputs

    def train_epoch(self, epoch):
        """è¨“ç·´ä¸€å€‹epoch"""
        self.model.train()
        self.classification_head.train()
        
        epoch_loss = 0.0
        epoch_detection_loss = 0.0
        epoch_classification_loss = 0.0
        
        pbar = tqdm(enumerate(self.train_loader), total=len(self.train_loader), 
                    desc=f"Epoch {epoch+1}/{self.opt.epochs}")
        
        for i, batch in pbar:
            # éˆæ´»è™•ç†æ‰¹æ¬¡æ•¸æ“š - ä½¿ç”¨ç´¢å¼•æ–¹å¼æ›´å®‰å…¨
            imgs = batch[0]
            targets = batch[1]
            paths = batch[2] if len(batch) > 2 else None
            
            imgs = imgs.to(self.device, non_blocking=True).float() / 255
            
            # è™•ç†ç›®æ¨™
            detection_targets = targets.to(self.device)
            # å‰µå»ºåˆ†é¡ç›®æ¨™
            classification_targets = torch.randint(0, 3, (imgs.shape[0],)).to(self.device)
            
            # å‰å‘å‚³æ’­
            detection_outputs, classification_outputs = self.forward_pass(imgs)
            
            # è¨ˆç®—GradNormæå¤±
            total_loss, loss_info = self.gradnorm_loss(
                detection_outputs, detection_targets,
                classification_outputs, classification_targets
            )
            
            # åå‘å‚³æ’­
            self.optimizer.zero_grad()
            total_loss.backward()
            self.optimizer.step()
            
            # è¨˜éŒ„æå¤±
            epoch_loss += total_loss.item()
            epoch_detection_loss += loss_info['detection_loss']
            epoch_classification_loss += loss_info['classification_loss']
            
            # æ›´æ–°é€²åº¦æ¢
            pbar.set_postfix({
                'Total': f'{total_loss.item():.4f}',
                'Det': f'{loss_info["detection_loss"]:.4f}',
                'Cls': f'{loss_info["classification_loss"]:.4f}',
                'DetW': f'{loss_info["detection_weight"]:.3f}',
                'ClsW': f'{loss_info["classification_weight"]:.3f}'
            })
            
            # æ¯20å€‹batchæ‰“å°è©³ç´°ä¿¡æ¯
            if i % 20 == 0:
                print(f"  Batch {i}: Total Loss {total_loss.item():.4f}, "
                      f"Detection Weight {loss_info['detection_weight']:.4f}, "
                      f"Classification Weight {loss_info['classification_weight']:.4f}")
        
        avg_loss = epoch_loss / len(self.train_loader)
        avg_detection_loss = epoch_detection_loss / len(self.train_loader)
        avg_classification_loss = epoch_classification_loss / len(self.train_loader)
        
        return avg_loss, avg_detection_loss, avg_classification_loss

    def train(self):
        """ä¸»è¨“ç·´å¾ªç’°"""
        print(f"ğŸš€ é–‹å§‹ç°¡åŒ– GradNorm è¨“ç·´ - {self.opt.epochs} epochs")
        
        start_time = time.time()
        
        for epoch in range(self.opt.epochs):
            print(f"\nğŸ“Š Epoch {epoch + 1}/{self.opt.epochs}")
            
            # è¨“ç·´ä¸€å€‹epoch
            avg_loss, avg_detection_loss, avg_classification_loss = self.train_epoch(epoch)
            
            # è¨˜éŒ„æ­·å²
            self.training_history['epochs'].append(epoch + 1)
            self.training_history['total_losses'].append(avg_loss)
            self.training_history['detection_losses'].append(avg_detection_loss)
            self.training_history['classification_losses'].append(avg_classification_loss)
            
            # ç²å–ç•¶å‰æ¬Šé‡
            current_weights = self.gradnorm_loss.get_current_weights()
            self.training_history['detection_weights'].append(current_weights['detection'])
            self.training_history['classification_weights'].append(current_weights['classification'])
            
            print(f"âœ… Epoch {epoch + 1} å®Œæˆ:")
            print(f"   å¹³å‡ç¸½æå¤±: {avg_loss:.4f}")
            print(f"   å¹³å‡æª¢æ¸¬æå¤±: {avg_detection_loss:.4f}")
            print(f"   å¹³å‡åˆ†é¡æå¤±: {avg_classification_loss:.4f}")
            print(f"   æª¢æ¸¬æ¬Šé‡: {current_weights['detection']:.4f}")
            print(f"   åˆ†é¡æ¬Šé‡: {current_weights['classification']:.4f}")
            
            # æ¯10å€‹epochä¿å­˜ä¸€æ¬¡
            if (epoch + 1) % 10 == 0:
                self.save_checkpoint(epoch + 1)
        
        elapsed_time = time.time() - start_time
        print(f"\nğŸ‰ è¨“ç·´å®Œæˆ! ç”¨æ™‚: {elapsed_time/3600:.2f} å°æ™‚")
        
        # ä¿å­˜æœ€çµ‚çµæœ
        self.save_final_results()
        
        return self.training_history

    def save_checkpoint(self, epoch):
        """ä¿å­˜æª¢æŸ¥é»"""
        save_dir = Path(self.opt.project) / self.opt.name
        save_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'model': self.model.state_dict(),
            'classification_head': self.classification_head.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'gradnorm_state': self.gradnorm_loss.state_dict(),
            'training_history': self.training_history
        }
        
        torch.save(checkpoint, save_dir / f'checkpoint_epoch_{epoch}.pt')
        print(f"ğŸ’¾ æª¢æŸ¥é»å·²ä¿å­˜: epoch {epoch}")

    def save_final_results(self):
        """ä¿å­˜æœ€çµ‚çµæœ"""
        save_dir = Path(self.opt.project) / self.opt.name
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        final_model = {
            'model': self.model.state_dict(),
            'classification_head': self.classification_head.state_dict(),
            'gradnorm_state': self.gradnorm_loss.state_dict(),
            'training_history': self.training_history
        }
        
        torch.save(final_model, save_dir / 'final_simple_gradnorm_model.pt')
        
        # ä¿å­˜è¨“ç·´æ­·å²
        with open(save_dir / 'training_history.json', 'w') as f:
            json.dump(self.training_history, f, indent=2)
        
        print(f"âœ… æœ€çµ‚çµæœå·²ä¿å­˜åˆ°: {save_dir}")
        print(f"ğŸ“Š å…±å®Œæˆ {len(self.training_history['epochs'])} epochs")
        print(f"ğŸ¯ æœ€çµ‚æª¢æ¸¬æ¬Šé‡: {self.training_history['detection_weights'][-1]:.4f}")
        print(f"ğŸ¯ æœ€çµ‚åˆ†é¡æ¬Šé‡: {self.training_history['classification_weights'][-1]:.4f}")


def parse_opt():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', type=str, required=True, help='dataset.yaml path')
    parser.add_argument('--epochs', type=int, default=50, help='total training epochs')
    parser.add_argument('--batch-size', type=int, default=8, help='total batch size')
    parser.add_argument('--imgsz', type=int, default=640, help='train image size (pixels)')
    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or cpu')
    parser.add_argument('--project', default='runs/simple_gradnorm', help='save to project/name')
    parser.add_argument('--name', default='exp', help='save to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--gradnorm-alpha', type=float, default=1.5, help='GradNorm alpha parameter')
    
    return parser.parse_args()


def main():
    opt = parse_opt()
    print(f"é–‹å§‹åƒæ•¸: {vars(opt)}")
    
    trainer = SimpleGradNormTrainer(opt)
    trainer.train()


if __name__ == "__main__":
    main() 